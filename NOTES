                                                                                      ***GIT****
Stages of GIT
-------------
Working area : where we write the code
Staging area: where we track the code
Local Repository: where we store the code
Remote Repository: where we store the code in Remote Location (Github)

GIT COMMANDS
================

init
clone
add
status
commit
push
pull
checkout
reset
log
rm
branch
merge
rebase
stash

Instal gitbash or Launch RedHat EC2 instance and login

sudo -s

mkdir gitproject

cd gitproject

yum install git -y 

git -v

git status --> You get error, because this is not initialized yet

git init  --> initialize git , this is local repo. .git directory will be create which contain files/dir maintained by git. The .git directory contains metadata and the entire history of the project, 

git status


touch index.html 
--> it has created locally, now move from working directory to staging area

vi index.html
this is sample file

git status
--> this is not tracked, Not tracked files shows in red color, if you want to track file move to staging area

git add index.html 
--> command to move to staging area

git status
--> now file is in staging area, tracked files shows green color, and now move to local repository

git commit -m "my first commit" index.html
--> now file is in local repository

git status

what ever we do track, committing file etc will be in .git folder

--> now another example to create a file and add to staging and repo

touch hello.txt
git status
git add hello.txt
git commit -m "My second commit" hello.txt
git status


touch test.txt
git status
git add test.txt
git commit -m "My third commit" test.txt
git status

--> create mode 100644  means a normal file
--> 100755 means executable file
------------------------------------------

git log --> to see how many commits history
git log --oneline --> to see less lines
git log --oneline -2 --> to see last 2 commits history
git show --> Show the changes of the last commit
git show 150eb87 --> to see which file is committed for this id

git log -p -2  --> shows last 2 commits with diff
git log --stat --> summary of the changes

git log	--pretty=oneline --> to see less info about commit details in singleline
git log --pretty=format:"%h-%an,%ar:%s"   [h = hash/commitnumber, an = authorname, ar = time, s = commit message ]
git shortlog --> Summarize git log output

vi index.html
this is first line

git add index.html
git commit -m "first line" index.html


git blame index.html --> Show who changed which line in a file

-------------------------------------
--> modify index.html file

vi index.html
this is second line .. new line
git add index.html
git commit -m "new line" index.html

Now see the difference between the commits

git log --oneline -2
git diff 150eb87..1fg0e787  -- see the diff between commits

Vi index.html
This is the third line

git diff  ------> compare changes working directory to staging area

git add index.html

git diff --staged --> compare changes staging to local repo

git commit -m "comparison" index.html

vi index.html
change something

git diff HEAD ----compare changes local directory to local repo [HEAD in caps]

---------------------------------------------

git log

--> you see root user in commit log --> in real time we should we our name not root

This is User Configuration

git config user.name "reyaz"
git config user.email "reyaz@gmail.com"

git config user.name  [to see who is the user]
git config user.email


Another way of adding user configuration

git config --list
git config --global --edit
or
cd /root
cat .gitconfig

vi index.html
added a new line

git add index.html
git commit -m "added new line" index.html

git log --oneline --author='reyaz'  --- this show only committed from certain person
git log --author="reyaz"

--> now create a new file
touch file1
git add file1
git commit -m "file1 commit" file1
git status
git log
--> now we can see Reyaz username

=========================
Git Amend - Change the commit history
=========================
if you want to change the last commit history

git commit --amend -m "an updated commit message"   --> this is replace latest commit message

==================
git rebase - If you want to change multiple commit histories
==================
Note: if you want to use this command, you need to have minimum 3 commits in your repo

git rebase -i HEAD~3 

The option -i means interactive. HEAD~3 is a relative commit reference, which means the last 3 commits from the current branch HEAD.

The git rebase command above will open your default text editor (Vim for example) in an interactive mode showing the last 3 commits

Now replace pick with reword and keep changing the commit messages for 3 times and wq!

git log --oneline -2   [All commit messages has been changed]


Squashing in Git is the process of combining multiple commits into a single, consolidated commit. This is typically done to clean up the commit history before merging a feature branch into the main branch.



==================================
Adding all files to stage at a time: git add .
=================================

touch python{1..5}
git add --a   [to add all files at a time to staging]
git add . or git add *
git commit -m "python files" .
============================================

======================================================

GIT Checkout : To restore previous version files in git 

======================================================

clean up 
--------
rm -rf * 
git status
git add .  
git commit -m "clean" .



vi index.html
This is version 1

git add .
git commit -m "version 1 commit" index.html

vi index.html
This is version 1
This is version 2

git add .
git commit -m "version 2 commit" index.html

vi index.html
This is version 1
This is version 2
This is version 3

git add .
git commit -m "version 3 commit" index.html

git log
or
git log --oneline -2

git show f86060a685535d6d68ecaa84f3d36e9f5d04814d:index.html

This will show first version file

If you want to restore previous version files 

git checkout f86060a685535d6d68ecaa84f3d36e9f5d04814d -- *  [* will restore all files in that commit]

git checkout f86060a685535d6d68ecaa84f3d36e9f5d04814d -- index.html [It will restore only index.html]

cat index.html

Again if you want latest

git checkout master -- *  [coming back to latest file]

cat index.html

Another scenario of checkout
---------------------------
use git rm to remove the file and restore using checkout HEAD

vi test.html
this is test

git add test
git commit -m "test" test.html

git rm test.html --> this will remove the file

git checkout HEAD -- test.html ---> this will restore the file

======================================================
GIT RESTORE : 
git checkout and git restore does the same work: reverting to previous state

Scenario 1: if you want to undo change after saving and before commit
----------

How to undo changes after save. If you are working on latest file, you do changes and saved. If you want previous one 
use git restore index.html

vi test.html
This is demo

git add .
git commit -m "test" test.html

vi test.html
This is demo
ajffjkdhgdkjghdf
save it

cat test.html

if you want now to restore

git restore test.html

cat test.html

=================================
 Scenario 2 = To restore files from stagged to unstage

Accidently you have added to stage using "git add" command and if you want to unstage or untrack
=======================================================

How to untrack files

vi test.html
This is devops demo

git add test.html 
git status --> now accidently i have stagged or tracked test.html , but I want to untrack/unstage

git restore --staged test.html
(or)
git restore --staged . -->  [. all]
(or)
git rm --cached test.html --> same like restore command above but another command to untrack/unstage

git status --> you can see now untracked

git add test.html  --> Add it to the stage again

git commit -m "test" test.html

Scenario - 3 : If you accidently delete the file from your directory and want to get back
--------------

touch newfile
git add newfile
git status ---> it shows green color, it is stagged

git add newfile  ----> now stage it

rm -rf newfile  -----> delete the file locally 

ls  ---> no local file

git restore newfile   --> this will help to get the deleted file back from stage to local machine

git status

git commit -m "newfile" newfile


=================
GIT RESET --- Uncommit last commit [if you committed accidentally and want to uncommit or come the file back to local repo to staging]
or
Unstage files that were mistakenly added to the staging area.

vi Reyaz.html
Welcome to DevOps Classes
git add Reyaz.html
git commit -m "reyaz" Reyaz.html


vi Reyaz.html
Welcome to DevOps Classes
ijkdfhgdkfghdkjfhgjdfhg

git add Reyaz.html
git commit -m "reyaz" Reyaz.html

oh shit, i commit wrong code

Accidentally you commited now, but you want to get it back from local repo to staging

git status -->  all clean, nothing to commit

git reset --soft HEAD^   [uncommit and keep the changes] 

git status -- we see Reyaz.html now in stage 

again git add Reyaz.html
git commit -m "reyaz" Reyaz.html
now try  below

git reset --hard HEAD^   [uncommit and discard the changes]
cat Reyaz.html


Feature  	              git restore	                        git reset
-------------------------------------------------------------------------------------------
Scope	      Working directory or staging area	       Commit history, staging area, working directory
*****
Affects Commit History	               No	                                Yes (can modify HEAD)
*********************
Primary Use Case	Undo changes or unstage files	Move HEAD or reset state to a previous commit
*************
Safety	                    Less destructive               Can be destructive, especially with --hard
******
--------------------------------------------------------------------------------------------------------

HEAD is a pointer to the current branch or commit you are working on
In Git, HEAD is a source to the current branch or commit you are working on. HEAD normally shows the recent commit of the current branch and moves when you switch branches or check out exact commits.

git Restore: Does not modify commit history or the repository's HEAD pointer. It only reverts changes in the working directory or staging area.

git reset:  Can modify commit history by moving the HEAD pointer and potentially discarding changes.

==========
help
===========

git help


=================
GIT STASH -- If you want to hide/park all the changes you did on the repo locally to a tmp place and proceed with another story changes then stash the old story, explain in diagram

The git stash command is a useful feature in Git that allows you to temporarily save changes in your working directory without committing them

===============
rm -rf *
git add .
git commit -m "clean" .

Now create a story1 file
------------------------
vi story1
 i am working on story 1
 very difficult task
 it task 2 hours time

--> Now suddenly manager called and asked to work on story2

git status
git add story1

git stash --> story1 will be removed from the working dir, do ls, it will be placed in tmp location

ls -- story1 file is not there

git status --> working tree clean

using stash we kept story 1 aside

Now create story2 file
---------------------
vi story2
i need to complete this story2 first

git status
git add story2
git commit -m "story2" story2

git status

Now lets work back on story1
----------------------------

git stash apply --> now restore do ls
ls
git status -> you can see the file again

git stash list -- it will list the stashes
git stash clear -- it will clear the stash


if you want to stash multiple files

git stash push -m "commit message" -- story1 story2
git status


=========================================================

rm -rf *


==========================================
Now time to commit to Remote Repo to GitHub
--------------------------------------------

Sign up to GitHub and create a sample test public repository

touch python{1..10}
touch hello.txt
git add .
git commit -m "python files" .

git remote add origin https://github.com/ReyazShaik/testrepo.git  ---> copy this command from GitHub --> this will be done only first time

once above command executed , in .git folder, cat config file, remote origin got added --> show this file

git push -u origin master --> this will be used to commit the changes from local to central repo  

git username:
git password or personal token [generate the token from github]

git config --global credential.helper cache        ---> not to ask for the password every time
git credential-cache exit    --> TO remove credentials

git remote -v

Example
--------

Now refresh the GitHub page and you can see the files

--> open hello.txt and add some content here  
--> git status --> now you can see the file in red color which is modified, now you need to stage this file 
--> git add hello.txt --> now you have stagged this file, if you want all the files to stage use 
git add --a 
--> git status --> now this is green color, ready to commit 



--> git commit -m 'modified hello.txt' --> this is now committed to local repo, you need to push to central pro GitHub
--> git push --> no need to add origin, that is only for first time and also no need to add remote origin that is also for first time

Lets try again

--> modify another file test.txt
--> git status
--> git add test.txt

If you want to un-stage existing file use RESET command .
if you want already added file to un-stage use reset command , for brand new file use rm command (difference rm - brand new file to un-stage , reset - existing file to un-stage)

git reset HEAD <filename>

git add .

-Now, redo what ever you have modified in the file

--> git commit -m 'modified test'
--> git push



========
git pull
========

--> Now, create a new file in GitHub  python11 file, to pull the files from remote to local

ls

now see python11 

git log ---- to see latest commit message

==========
git clone : If you want to download all the remote code to local
==========


mkdir herodir
cd herodir

git clone https://github.com/ReyazShaik/test.git

git log

This is new user, he/she can see what ever that has commited before on this repo that is why it is distributed version control system

vi python12
This is python12

git add python12
git commit -m "python12" python12
git push

see in github

git credential-cache exit   -- remove credentials 

Another way to Authenticate and push to GIT with SSH
================================================

### To generate SSH Key

ssh-keygen (give this command into your terminal)

copy the publickey id_rsa.pub

paste into github ssh keys 

path : settings-->ssh and GPG keys--> Add new ssh key and give any name and paste it your publickey over there 


cd /home/ec2-user/

git remote add origin git@github.com:ReyazShaik/test1.git

git remote -v  --> to check the remote repo details

git push -u origin master


Git commands learned
====================

init
add
commit
log
show
restore
rm
help
stash
checkout
reset

=============================================================================


Explain Branch Concept 

keep the master branch clean first

touch index.html
git add index.html
git commit -m "index commit" index.html
git status
git branch --> it will show the branch master, default branch

if you commit atleast 1 file, it will show the master branch

Its not good to work everyone on master branch, individual developer create their own branch

dev1branch
==========
git branch dev1branch  --> it will create a new branch dev1branch from master
git branch --> to list the branches, * represent the current branch
git checkout dev1branch --> to switch branch
git branch 
touch dev1file{1..5}
git add dev1file*
git commit -m "dev1" dev1file*

dev1branch has 1 + 5 files = 6 (it got 1 file from master)

dev2branch
==========
git branch dev2branch --> this will create a new branch dev2branch from dev1branch as we are now in dev1branch
git branch
git checkout dev2branch
git branch
touch dev2file{1..5}
git add dev2*
git commit -m "dev2 commit" dev2*

dev2branch has 1 + 5 + 5 = 11

dev3branch
===========
git branch 
git branch dev3branch --> this will create a new branch from dev2branch
git checkout dev3branch
              or
              git checkout -b dev3branch --> this will also create and checkout branch dev3branch, 2 commands in 1 shot
git branch 
touch dev3file{1..5}
git add dev3*
git commit -m "dev3 commit" dev3*

dev3branch has 11 + 5= 16

dev4branch
===========
Now i want to create a new branch from master not from dev3 

Now I want to checkout from master not from dev3 , so first checkout to master and create a branch, it will get only 

git branch
git checkout master
ls
git branch dev4branch
git checkout dev4branch
ls

ls --> it should list only index.html
touch dev4file{1..5}
git add dev4*
git commit -m "dev4 commit" dev4*

dev4branch has 1 + 5 = 6


**** rename branch

git branch -m dev5branch devnewbranch  [renaming dev5branch to devnewbranch]

=======================
GIT MERGE - Merge between 2 branches 
======================

git branch 
be in master

git checkout master

git merge dev1branch  --- what ever we have files in dev1branch will come to master

Now push to GitHub, create a GitHub account
========================================
git branch

git remote add origin https://github.com/ReyazShaik/testrepo.git 

git config --global credential.helper cache  


git push origin master --> push the code of master to GitHub
username:
password: paste the token here
GitHub removed password authentication and put token system for better security
Generate token from GitHub
Profile --> settings --> developer settings --> personal access token --> classic -->
Generate new token --> classic --> name:gitlearn --> select repo scopes --> generate 

Now push dev1branch files

git push origin dev1branch --> now see 2 branches in GitHub
git push origin dev2branch --> now see 3 branches in GitHub
git push origin dev3branch --> now see 4 branches in GitHub
git push origin dev4branch --> now see 5 branches in GitHub

=======================
GIT MERGE - Merge between 2 branches 
======================

git branch

git checkout dev2branch -- see the files

git checkout dev1branch -- see the files

git merge dev2branch  --- what ever we have files in dev2branch will come to dev1branch

another command instead merge use rebase

git rebase dev4branch

Git Merge: Keeps the history intact and is safer for shared branches. Ideal for collaborative projects.
Git Rebase: Rewrites history for a cleaner, linear commit log. Best suited for private or feature branches.

======================
GIT REVERT -- accidental merges from one branch to another branch and undo those

git branch

git merge dev2branch

ls

git revert dev2branch --> this will delete the files from current branch which was merged
(don't to any changes to file, just quit the file)

========================
GIT BRANCH another example
=========================

--> Create one test repo in GitHub


mkdir branchdemo
cd branchdemo
git init
touch python{1..10}

vi python1
This is python1 
nice code. log code
 
git add .
git commit -m "pythonfiles" .

git branch

== what ever we are doing is in master branch
== A new developer came and said i will do some changes, is it good to do directly in master?
== create a new branch for developer

git branch developer
git branch
git checkout developer

vi python11
This is python11

vi python1
added new lines

git add .

git commit -m "pythonfilesnew added" .

== No impact on master branch 
== Now lets go to master branch and see the data

git checkout master

ls

== Here we dont have python11 and python1 changes

if i want developer changes to master

git merge developer  [merging code from developer branch]

git remote add origin https://github.com/ReyazShaik/branching.git

Note: if you want to change remote origin " git remote set-url origin https://github.com/ReyazShaik/branching.git"

git push -u origin master

== Now see in GitHub, but you can see only one branch in GitHub because we pushed from master, GitHub dosent know developer branch yet

git checkout developer
vi python1
added worst code
version 1.2.3

git add .

git commit -m "pythonfilesnew added" .

git push origin developer [we need to push from developer branch]

== Now in GitHub, you get Compare and Pull Request --> Merge Request
== After merging master branch contains the code of Developer branch

In your terminal, if you want to get latest code from GitHub

git checkout master

git pull


git merge
========

git checkout developer
vi python1
version 1.2.3.4

git add .
git commit -m "1.2.3.4" python1

git checkout master
vi python1
version 1.2.3.4.5

git add .
git commit -m "1.2.3.4.5" python1

git merge developer
-- merge conflit

vi python1 

remove which ever you need, we can commit only 1
save it

git add python1
git commit --> no need to put filename

merge file will come , just wq!

git status




=======================
GIT REBASE - same as MERGE
=======================

git rebase dev3branch -- now you see dev3 also in dev1branch

==========================

MERGE vs REBASE

Merge for public repos, rebase for Private
Merge stores history, rebase will not store the entire history (commits)
merge will show files, rebase will not show files


====================================

========================
GIT CLONE -- download remote repo to local
======================

Now delete the local directory gitproject and clone from GitHub

rm -rf gitproject

git clone https://github.com/ReyazShaik/branching.git
ls
cd branching 
git checkout master
ls
git checkout dev1branch
ls
git checkout dev2branch
ls


=================================
SKIP Another Example = MERGE CONFLIT - when we merge 2 branches with same files
===============================

mkdir abcd
cd abcd
git init

vi index.html
I am dev1 writing java on branch-1
git add index.html
git commit -m "dev1-commits" index.html

git branch

git branch -m master branch1 --> renaming master to branch1 (just for clarity)

git branch

git checkout -b branch2 --> this will create a new branch from master

vi index.html
I am dev2 writing python on branch-2
git add index.html
git commit -m "dev2-commits" index.html

git branch 
*branch2
cat index.html

git checkout branch1
ls
cat index.html
vi index.html
add some line
git add index.html
git commit -m "dev1-newline-commits" index.html

Now in both branches we have same index.html --> see files ,

git merge branch2 -> error: conflit

git diff index.html

cat index.html

--- Now resolve conflit manually by developer, because he know what can be kept and removed

vi index.html
delete dev2 code
git add index.html
git commit -m "new-commits" [dont give filename]
git status




=======================
GIT FORK - same as clone (remote to local) but fork (download code from one GitHub account to another GitHub account)
=====================

= If i want to enhance the code of another guy who has his own repo and code, instead me doing changes in his account directly i will fork it in my GitHub repo and do changes



In GitHub browser - search for another GitHub account(https://github.com/ReyazShaik/test) and fork to my account(trainerreyaz account)

GIT FORK and GIT CLONE -- Both repos should be public, for private use token 

you cannot unfork, you  need to delete the repo



PR - PULL REQUEST
---------------

Be in trainerreyaz account and add or modify python1 file and commit

--> contribute --> pull request --> Finish
--> Go to actual reyazshaik GitHub account --> on top --> pull requests --> Click on Request and Merge

-- Again go to trainerreyaz account modify the file --> save --> contribute --> Create pull request

-- Come to reyazshaik account --> pull requests --> see what exactly code change is --> click on commits --> REview changes --> Add comment --> i dont like --> close the request


Sqaush and merge : combining the commits history
Use squash and merge for branches with many small or work-in-progress commits to keep the history clean.

Squash and Merge is a method used during the pull request (PR) or branch merging process to combine all the commits from a feature branch into a single commit on the target branch. This is particularly useful for maintaining a clean and concise commit history.

Rebase and Merge :  Retains individual commits


======================
PR - Pull Request
=======================

Everything what we did on git, now its do on GitHub

merge, rebase etc we did on git(local),

TO merge in github use PR

MERGE ON GITHUB - PR should be done on GitHub - from one branch to another branch

create a repo in GitHub and add a file in master branch...

create another branch test and add a file

Now create a PR

In GitHub on top --> Pull request --> New Pull Request --> select branches from where to where (test to master) --> see differences  --> Create Pull request --> pull request --> Merge pull request and confirm

Now code merges from branch to master


====================
Rename the Branch Name - GIT
=====================

touch python{1..5}

git add .

git commit -m "python" .

git branch dev1branch

git branch -m dev1branch testbranch

git checkout testbranch

touch java{1..5}

git add .

git commit -m "java" .


create a repo and push the files and branch

git remote add origin https://github.com/ReyazShaik/branching.git

git push origin master
git push origin testbranch

create a pull request and merge in GitHub 

===============
GIT REVERT in GITHUB
=============
select View all branches in branch section --> which branch you want to revert --> select that branch --> Click PR and revert
-> it will create a revertbranch and revert

git fetch
==========

modify one file in GitHub and do fetch 

git fetch will just shows the changes / Downloads updates but does not integrate them.
git pull will pull the entire data / Downloads and integrates updates into the current branch.

=======
git clean : delete untrack files

create a file and dont track
git clean -n filename  -- first git will ask
git clean -f filename  -- delete without asking



=========================
CHERRY PICK - Merging the specific files based on commits
========================
mkdir Reyaz
cd Reyaz
git init

touch file1
git add file1
git commit -m "file1" file1

git branch

git branch -m master branch1  [Rename master branch to branch1 for clarity]

git checkout -b branch2  [create branch2]

git checkout branch1  [be in branch1]

touch java{1..5}
git add java*
git commit -m "javafiles" java*

touch python{1..5}
git add python*
git commit -m "pythonfiles" python*

touch php{1..5}
git add php*
git commit -m "phpfiles" php*

to see how many commits
--------
git log --oneline
ls

git checkout branch2 [switch to branch2, in this branch we have only one file]

i want to merge branch2 with branch1 we use git merge,  but i dont want all files, i need only few [cherry pick]

git cherry-pick commit-id

git cherry-pick 16dufg(pythonfiles)
git cherry-pick 13gh56g(phpfiles)
ls

Now, this merge only pythonfiles and phpfiles

if you want to revert -- git revert commit-id

GIT TAG - For reference
====== =
git log --oneline

git tag tagname commitid

example:  git tag dev/prod 0620179

git log --oneline

=========================
GIT IGNORE - if you don't want to track the file, add filename in .gitignore
========================

In Same directory 

touch file{1..50}
git status 
-- If I don't want to track
vi .gitignore
file1
-- git status --- cant see file1

================
Delete Branch in GIT 
==================
- git branch -D branchname 
To get Branch restore - git checkout branchname
git branch

Restore deleted branch in GIT
===========================

git reflog

git checkout -b dev 3e98e1c(commitid)

Be in deleted branch and ls

=====================
Host website on GitHub
=====================

Create a repo and create index.html --> put some html code

setting --> pages --> branch master --> /root --> save --> you get the link



=================
MAKE REPO Public to Private in GitHub --> Repo settings --> danger zone --> change visibility
=================


===================
Delete branch in GitHub
================
view branches --> delete icon -- undo there itself -- rename the branch also there


===========================================
HOW to add multiple developers to GitHub
============================================
In GitHub --> Repo --> Settings --> Collaborators --> Add people

Like Kanban board --

Create a Project --> Select Table --> Create Task --> press tab --> and fields and assign people(tasks, assignees, start date, enddate)

Go to Project Settings --> Manage access --> give permissions

Create Task and assign tasks
==================================
Long living branches - master, Main
Short Living branches - feature, bugfix, 



Troubleshooting
----------------

error: src refspec main does not match any
error: failed to push some refs to 'https://github.com/ReyazShaik/branching.git'

Solution: present branch is master, but remote branch is main
git push origin master --> master branch will create automatically in github
===========================================================================================================================================================================================================================
                                                                                                ***MAVEN****
============
LAB
==========

Launch Amazon Linux 2 EC2 instance

we need to get source code and build so we need to install git and maven

yum install git java-1.8.0-openjdk maven tree -y

mvn -v

git clone https://github.com/ReyazShaik/java-project-maven-new.git  

cd java-project-maven-new

ls

--> now we took the code and we need to build

--> if you want to see all the folders and subfolders use tree command

tree

To use Maven, maven has lifecycle called goals : each goal is a task

=======
MAVEN LIFECYCLE
==========
GOALS : is a command use to run task
Goals refer pom.xml to execute

PLUGIN: its a small software which makes our work automated
Instead of downloading tools we can download plugins
this plugins will download automatically when we run GOALS

Maven goals include compile, test, package, and install

1. mvn compile : is used to compile the code (.java[src] --> .class [target]) [in source code we have .java files, we if run compile it will create .class in target folder]
tree -- see what files are there
now, run the below command

--> mvn compile
--> tree -- it will create target folder and .class files

2. mvn test : used to test the code (.java[test]--> .class [target])

run the below command

mvn test

IT will create a test folder with .class files

tree


3. mvn package : use to create artifact

run the below command

mvn package

tree

It will show the war file, the project has frontend + backend code


4. mvn install : use to copy artifacts to .m2 (project folder --> .m2, under root/.m2 folder(maven home path))

Run the below command -- artifacts will be copied to maven home folder

mvn install

tree



5. mvn clean : to delete the target folder

Run the below command

mvn clean

===============
IN real time, we will not do one by one step , just use below command mvn clean package

It will generate the arifacts --> if you want to generate use below command, it will delete the existing target folder and create new


6. mvn clean package : It compile to install (all steps)

run below command

mvn clean package

===============================================================

PROBLEMS WITHOUT MAVENS
--> we cant create artifacts
--> we cant create project structure
--> we cant build and deploy the apps
============================================================================================================== **JENKINS***======================================================================================================
                                                                                                    **JENKINS***
==================================

SETUP : Launch EC2 instance -  Amazon Linux 2
-------------------------------------------

In SG, allow protocols, 8080, 80 http

To setup Jenkins, we need to take code from git , build using maven , that's the reason we install below all

Step 1: installing git, java 17 and maven
------------------------------------------

yum install git java-17-amazon-corretto maven -y

Step 2: Getting the REPO (Jenkins.io --> download --> RedHat) - show in browser Jenkins.io --> LTS --> Redhat
---------------------------------------------------------------

sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo

sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD and install JENKINS
--------------------------------------

yum install jenkins -y

#STEP-4: RESTARTING JENKINS (when we download service it will on stopped state)
---------------------------------

systemctl start jenkins.service

systemctl status jenkins.service

NOTE: chkconfig jenkins on

above command will make jenkins service automatically restart itself when sevrver is stopped and started

CONNECTION:
-----------
public-ip-server:8080 (browser)

cat /var/lib/jenkins/secrets/initialAdminPassword

give this password in browser and install plugins
give sample usernames and passwords --> start Jenkins

For logs:

journalctl -u jenkins
========================================================
Creating a sample job
=======================================================

Example 1: Firstjob - Execute shell
--------

CREATING A JOB:
---------------
New item -- > Name -- > Free Style -- > Ok
Description --> THis is my first job
Build Steps --> Execute Shell --> echo "Welcome to Jenkins" --> Save
Build Now

--> Now Modify Build --> echo "Welcome to Jenkins" > /tmp/first-job.txt

Run the build

see in Jenkins server --> cd /tmp/  [What ever we create in pipeline, all files will be stored in jenkins server]

--> lets create a small bash script

    vi     basic.sh
    #!/bin/bash
    echo "This is sample script under tmp"

sh basic.sh

Edit the job and modify the script . If you want to run any script which is there on server from pipeline

echo "Welcome to Jenkins" > /tmp/first-job.txt
bash /tmp/basic.sh

Run the Build and see Output

Build with Parameters
------------------

Edit the Job --> THis Project is Parameterized
Example --> Select --> String Parameter
Name = FirstName
Default Value = Reyaz

One more

Select --> String Parameter
Name = LastName
Default Value = Shaik


Nows lets use these parameters in script

echo "I am" $FirstName $LastName
echo "Welcome to Jenkins"
echo "Welcome to Jenkins" > /tmp/first-job.txt
bash /tmp/basic.sh

Run the Build

By default, you get Reyaz shaik, but you can override it with other string/name

see the output


Another Example:  Lets add choice parameter: Edit the Build Job
-----------------
--> THis Project is Parameterized --> Add Parameter --> choice Parameter
Name: Gender
Choices:
MALE
FEMALE
OTHER

Edit the Script and add $Gender

echo $FirstName $LastName $Gender
echo "Welcome to Jenkins"
echo "Welcome to Jenkins" > /tmp/first-job.txt
bash /tmp/basic.sh

Another Example: Lets add choice Parameter : Edit the Build Job
-----------------
--> THis Project is Parameterized --> Add Parameter --> choice Parameter
Name: ENV
Choices:
Production
Development

echo "I am" $FirstName $LastName $Gender
echo "Welcome to Jenkins Pipeline by Reyaz" > /tmp/first-job.txt
bash /tmp/basic.sh
echo "Deploying to environment: $ENV"
if [ "$ENV" = "Production" ]; then
  echo "Warning: You are deploying to Production!"
else
   echo "Deploying to $ENV Environment."
fi



Scheduling the job
----------------

If you want to run the job based on schedule, we can use Build Periodically option

Create a new free style job --> Name = cron-job

Description --> THis is scheduled job
Build Periodically --> *****  -- every min  */5 * * * *  -- Every 5 mins

search in google -- cronjob generator -- https://crontab.cronhub.io/

Build Steps --> Execute Shell --> echo "Welcome to Jenkins" --> Save

Build Now

This job will build every min



CREATING A JOB : Integration with GitHub
----------------------------------------

Note: To integrate any tool, we need to install plugins first.

Check if git plugins are available, if available, no need to install. If not, install plugins

Example 1 : Simple, Lets get source code to Jenkins
-------------------------------------------
New item -- > GetCodefromGit -- > Free Style -- > Ok --> source code management --> GIT

--> Repo URL(take sample project repo) https://github.com/ReyazShaik/website.git

--> Branch : master , keep default or change    

Run the Build

Now see the data Click on GetCodefromGit job --> WorkSpace

Where is the code now in Jenkin server?

cd /var/lib/Jenkins/workspace/

cd GetCodefromGit



Example 2:
----------
New item -- > Name -- > Free Style -- > Ok --> source code management

--> Repo URL(take sample maven project repo) https://github.com/ReyazShaik/java-project-maven-new.git

or take sample maven project https://github.com/jenkins-docs/simple-java-maven-app.git

--> Branch : master , keep default or change    

Build step --> Select Execute shell --> mvn clean package (sample) --> save --> build now

If fails: In general, GitHub uses main branch instead master --> in job replace /master with /main

GO to Jenkins Linux machine

--> cd /var/lib/Jenkins/workspace/Firstjob/target/

ls --> war file will be here


Example 2: Secondjob
----------
New item -- > Name -- > Free Style -- > Ok --> source code management

--> Repo URL(take sample maven project repo) https://github.com/ReyazShaik/java-project-maven-new.git

--> Branch : master , keep default or change

--> Build step --> Invoke top-level Maven targets --> just clean package (no need to give mvn)    

--> Build Now

GO to Jenkins Linux machine

--> cd /var/lib/Jenkins/workspace/Firstjob/target/

ls --> war file will be here


Now, change the file in GitHub --> index.html or pom.xml and build now

Every time developer do changes he will do build --> this is called continuous integration

Email Notification
==================

If build fails, we should get notifications

Manage Jenkins --> System --> Email Notification

SMTP server = smtp.gmail.com
Default user e-mail suffix= @jenkinstest.com
Advance --> Use SMTP Authentication = Username = reyazr3f@gmail.com
For password = Go to browser --> gmail profile--> Manage your Google Account --> Security --> Turn ON 2-steps authentication -->  On Top --> Search for App Password --> App name --> Jenkins --> Create --> Copy code --> miqx ynqp dprc qgpt --> Done
                  copy the code and paste in password section


Use SSL
SMTP Port = 465
Test configuration by sending test e-mail = reyazr3f@gmail.com = Test Configuration

Edit the job --> Configuration --> Post-build Actions --> Recipients = reyazr3f@gmail.com
And Fail the job to get email
Transferring Files from Jenkins to Remote Server
===============================================

1. Setup connection from Jenkins to remote server
2. Install SSH plugins
3. Create a job to copy files to remote server


Have 2 Servers
---------------
1. Jenkins Server
2. Test Server (Amazon Linux 2)

In Jenkins Server
-------------------
ssh-keygen

eval $(ssh-agent -s)

vi MyKey.pem
copy paste pem data

chmod 400 MyKey.pem

ssh-add MyKey.pem

cd /root/.ssh/

ssh-copy-id -i id_rsa.pub ec2-user@172.31.22.33

Testing to connect = ssh ec2-user@172.31.22.33

install ssh plugins
---------------

Manage Jenkins --> Plugins --> Publish Over SSH

Manage Jenkins --> System --> Publish Over SSH

cd /root/.ssh
cat id_rsa

--> Publish over SSH --> key -->

Paste this id_rsa data in "Key" section in Jenkins

SSH Server --> ADD
Name = TestServer
Hostname = Private IP
Username = ec2-user
Test Configuration

Create a new job
-----------------

New item --> Remotefiles --> FreeStyle --> OK

Build Steps --> Send files or execute commands over SSH -->
Name = autoselect

Build now --> First build empty job because it will create a directory in workspace

cd /var/lib/jenkins/workspace/remotefiles/
mkdir files
touch test.py


Sourcefiles = files/test.py  or top copy multiple files files/*

Remove Prefix = empty [if you want to remote prefix directory and copy only files]
Remote Directory = empty [you can specify remote server directory to copy]
Exec command = Empty [if you want to execute file, sh /home/ec2-user/files/bash.sh]

Advance --> Exclude files if you want -->Exclude files --> files/test1.py, files/test2.py

Build now

See the files in remote server --> /home/ec2-user/files/

================================


Example for Local variable :
===========================

Create a new job --> Free Style --> Build Step --> Execute shell (no git required)

echo "This is DevOps Course, here you are learning DevOps, DevOps is good "

--> Here every time you use DevOps , instead mention it as a Variable course = devops

course=DevOps  --> no space
echo "This is $course Course, here you are learning $course, $course is good "

save and build now

and configure build again and change course variable as AWS and build now

Example for Global variable :
===========================

Now remove, course = DevOps in build and build now, Jenkins will not understand

Now set Global Variables

Dashboard -> Manage Jenkins --> System --> Global properties --> Environment variables --> ADD --> name = course, value = DevOps

Example for Jenkins Env variables :
===========================

Create a new Job --> Free Style --> Build Step --> Execute shell (no git required)
click on list of available environment variables
echo "The Current Build Number is $BUILD_NUMBER, Job Name is $JOB_NAME"
Build now and see the output

if you want all env to print use in shell : printenv and run the build

Restore Deleted Job
=================

If you accidentally delete the job , how to restore

Install plugin = job configuration history

Delete a job now --> Go to Dashboard --> click on job config history --> show delete jobs --> restore

note: If you install the plugin initially then all jobs create after plugin installation , restore option is available

Create a new job and delete

Delete a job now --> Go to Dashboard --> click on job config history --> show delete jobs --> restore
=================================
===================================
JENKINS Port Number Change:
==================================
TO search a file -- find / -name Jenkins.service

vi /usr/lib/systemd/system/jenkins.service
line 72 (8080=8069)
systemctl daemon-reload
systemctl restart jenkins.service

===========================
Jenkins password less login LOGIN:
=========================
find / -name config.xml
vi /var/lib/jenkins/config.xml (true=false)
<useSecurity>true</useSecurity>   ---> False
systemctl restart jenkins.service

Put IP in browser and directly it will login without asking username and password

====================


==================
BUILD Executors - Builds running parallelly
=================

By default Jenkins runs the jobs sequentially --> click on the build 2 times , it will run only 2 at a time

Manage Jenkins --> System --> # of executors = 2 by default

if you want to run parallelly --> Job --> configure --> Execute concurrent builds if necessary --> build now (now run parallel)

or

Go to Dashboard --> Click on Build Executors Status-->Built-In Node--> Configure --> Number of executors --> 10
=======
POLLSCM
=======

Create a sample Build with git and maven targets

Go to Job --> configure --> remove Build periodically and enable POLLSCM --> 20 14 8 7 1 (UTC time, 14:20 8th July monday)

Change the Code in index.jsp and wait for the time, it will build automatically only if code change(POLLSCM)

IF dev want to run the build immediately , he cannot wait for the CRON to build as per POLLSCM , we use webhook

============
WebHook
============

Webhook - It will trigger the build immediately when you commit the code in SCM

Go to GitHub --> Select Repo --> Settings --> Webhook --> Add Webhook --> Payload URL --> http://13.200.252.207:8080/github-webhook/
--> Content Type --> Application/Json --> Add webhook

-> Now Create a new job --> Free Style --> git --> Repo url(java-project) --> branch (/main or /master) --> Build Trigger --> GitHub hook trigger for GITScmpolling --> Build Step --> Maven target --> clean package

--> in AWS SG enable ALL traffic because GitHub needs to connect to your ec2 Jenkins

===============
THROTTLE BUILD:
===============
 It will restrict the number of builds per interval.
create a  job -- > Throttle build -- > builds: 3 time period: hours -- > save --> make a build and test

================
Remote Triggering
=================
Create a job --> Build Triggers --> Trigger build remotely --> Authentication Token : Reyaz --> Save

Copy the URL , change values and put in browser

JENKINS_URL/job/cijob/build?token=TOKEN_NAME   /  http://13.233.214.180:8080/job/firstjob/build?token=reya

Build will start automatically now

================================
DAY 4 - Project on FreeStyle Job - Deployment on Tomcat
=================================

1 Server Jenkins
Another server for Tomcat

Now lets deploy our application war file on tomcat

Launch Amazon Linux 2 instance and install tomcat manually ---


Tomcat Setup
============

Launch amazon Linux 2 and install tomcat manually first and then automatically using script

All apache software are available in http://dlcdn.apache.org --> tomcat --> tomcat-9--> v9.0.89--> bin--> apache-tomcat-9.0.89.tar.gz



Step1: install java
-------------------
amazon-linux-extras install java-openjdk11 -y

Step 2: download tomcat
----------------------
wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.34/bin/apache-tomcat-10.1.34.tar.gz

wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz
wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.33/bin/apache-tomcat-10.1.33.tar.gz--- copy link


Step3: Extract
--------------
tar -zxvf apache-tomcat-10.1.34.tar.gz

Step4: Configure user, passwords and roles
-------------------------------------------
 --> cd apache-tomcat-10.1.34/conf/
 --> vi tomcat-users.xml
     Add new files --> copy 3 lines and paste, line 56
     <role rolename="manager-gui"/>
     <role rolename="manager-script"/>
     <user username="tomcat" password="root123456" roles="manager-gui,manager-script"/>

Step 5: Delete line 21 and 22 -- if not delete, we need to give public ip
------------------------------
  --> cd apache-tomcat-10.1.34/webapps/manager/META-INF
  --> vi context.xml
       :22

Step 6: Start Tomcat
--------------------
  --> cd apache-tomcat-10.1.34/bin
  --> sh startup.sh

Connection
=========
http://IP:8080
manager apps --> username: tomcat, password: root123


================================================

Install Plugin
-------------
In Jenkins server first install a plugin(Deploy to container) to integrate with Tomcat

Create a Job
------------

--> Create a New item --> Free Style --> Source Code Management --> GIT --> https://github.com/ReyazShaik/java-project-maven-new.git

--> Build Steps --> Invoke top level maven target --> clean package

--> Post Build actions -->
 Deploy war/ear to a container --> **/*.war
 Context path = mywebapp
--> Add container --> Select tomcat 9 --> credentials --> Add Jenkins --> username=tomcat, password= root123, ID = tomcatcred    
    --> tomcat url = http://35.154.10.118:8080/ (ec2 url)

--> Build Now

--> Go to tomcat page , refresh /mywebapp

------------------------------------

Now Store the Artifacts to S3
================================
🔹 Step 1: Install the Plugin

Go to Jenkins Dashboard → Manage Jenkins → Manage Plugins.

Search for "S3 publisher" and "Pipeline: AWS Steps"(optional) install it.

🔹 Step 2: Configure AWS Credentials
Go to Manage Jenkins → System → Amazon S3 profiles --> ProfileName=s3creds --> access key and secret key - Save

Go to your Jenkins Job → Configure -->  
 --> Post-build Actions.
 → Select "Publish artifacts to S3 bucket".
 --> Source = **/*.war
 --> Destination Bucket = jen-test-me-reyaz/
 --> Bucket Region = ap-south-1
 --> Server side Encryption
short cut to remember how to create a pipeline :
PASSS -  PAS3
Pipeline,
Agent,
Stages,
Stage ,
Steps

Below pipeline has one stage

pipeline {
    agent any
   
    stages {
        stage('one') {
            steps {
                sh 'touch file1'
            }
        }
    }
}

--> now, file1 is created at cd /var/lib/jenkins/workspace/



Can have single stage or multi stage, depend upon requirement

pipeline {
    agent any
    stages {
        stage('one'){
            steps {
                sh 'touch file1'
            }
        }
        stage('two'){
            steps {
                sh 'ls -al'
            }
        }
        stage('three'){
            steps{
                sh 'df -h'
            }
        }
    }
}

For syntax, click on pipeline syntax and select git (in git master branch should be there), give info and generate sntax

if you need master branch in git


==============
Now create a CI Pipeline
==============

CODE --> BUILD --> TEST -->Code Review --> ARTIFACT --> Deployment


pipeline {
    agent any
    stages {
        stage('Clone Git Repo') {
            steps {
                echo 'Cloning Git Repository'
            }
        }
        stage('Building') {
            steps {
                echo 'Building the Project'
            }
        }
        stage('Code Test') {
            steps {
                echo 'Testing the Project'
            }
        }
        stage('Code Review') {
            steps {
                echo 'Code Quality Checking'
            }
        }
        stage('Artifact') {
            steps {
                echo 'Generating war file'
            }
        }
        stage('Deployment') {
            steps {
                echo 'Code is deployed'
            }
        }
    }
}





pipeline {
    agent any
======================


pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Deployment') {
            steps {
                echo 'Code is deploy - actual we need to learn tomcat, using tomcat we deploy'
            }
        }
    }
}

or

Pipeline as a Code - Running more than one command inside a single stage (Advantages: save time, can run all commands in one stage, reduce length of code)
===================

pipeline {
    agent any
    stages {
        stage('one') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
        sh 'mvn compile'
        sh 'mvn test'
        sh 'mvn package'
            }
        }
       
     
    }
}


Multi stage pipeline as a Code
=============================


pipeline {
    agent any
   
    stages {
        stage('one') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven.git&#39;
        sh 'mvn compile'
            }
        }

        stage('two') {
            steps {
                sh 'mvn test'
        sh 'mvn package'
            }
        }
       
    }
}


Pipeline as a Code over single shell
===================

pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
        sh '''
        sh mvn compile
        sh mvn test
        sh mvn clean package
        '''
            }
        }
    }
}

HOW TO ADD PARAMETERS:
=========================
PARAMETERS: Used to pass input for jobs


CHOICE: to pass single input at a time.
STRING: to pass multiple inputs at a time.
MULTI-LINE STRING: to pass multiple inputs on multiple lines at a time.
FILE: to pass the file as input.
BOOL: to pass input either yes or no.


This project is parameterized
Name: Environment
Choices:
Dev
Test
Prod



=================
INPUT PARAMETERS - Manual input :  In real time, we use input to continue the pipeline like yes or no
=================

Create a new pipeline for input parameter and copy the below code



pipeline {
    agent any  
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage ('deploy') {
            input {
                message "is your inputs correct ?"
                ok "yes"
            }
            steps {
                echo "my code is deployed"
            }
        }
    }
}
===========
Linked Jobs - Link the jobs with each other --> first job will trigger another job
==========

Create job1 and job2 with build steps --> execute shell --> echo "welcome"

Upstream and downstream


job1 - Post-build Actions -->Build other projects--> Projects to build --> job2

Build the job now, and once first job build done and second job will get trigger


============================
- MASTER and SLAVE
============================

MASTER AND SLAVE:
when we continuously build jobs on Jenkins ,it will get load and have the chance to crash.
to avoid this load on Jenkins we use salve servers to build the jobs

Setup Master
============

Install Jenkins with Script


SETUP: SLAVE 1
---------------
CREATE AN EC2 INSTANCE Amazon Linux 2 AND INSTALL JAVA-11/17

sudo yum install java-17-amazon-corretto-devel maven git -y

Dashboard -- > Manage Jenkins -- >Nodes -- > New node-- > name: salve1 -- > Permanent Agent -- > create

Number of executors    : 3 (number of parallel build we can do)
Remote root directory   : /tmp (where your op is going to store)
Labels            : slave1 (way of assigning work to particular slave)
Usage            : last opt
Launch method        : last opt
Host            : private-ip of slave
Credentials        : add -- > Jenkins -- >
kind: ssh username with private key
Username: ec2-user
Private Key: enter directly -- > copy paste the pem content
Host Key Verification Strategy: last opt


================
Now Setup Slave 2

Follow above process -- but one thing generally miss - install amazon-linux-extras install java-openjdk11 -y on slave 2 first and then add node in Jenkins

Create a job , execute on build

if build fails --> on slave see java version

update-alternatives --config java

run the build again

--> if you want to run the pipeline using slave not in configuring Jenkins(Restrict where this project can be run), but directly through pipeline use the below script as agent label

Example 2 with Pipeline
-------------------------


pipeline {
    agent {
        label 'slave1'
    }
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'    
            }
        }
        stage('Deployment') {
            steps {
                echo 'Code is deploy - actual we need to learn tomcat, using tomcat we deploy'
            }
        }
    }
}


================
Monitor Slave machines --> Click on Node --> left side --> Load statistics, System info , Build history etc




==================================
Integrate tomcat in Jenkins Pipeline
-------------------------------------------------------------------

Install plugin : deploy to container
-----------------------------------------------------

Restart Jenkins

systemctl restart Jenkins.service

Manage Jenkins --> Credentials --> System --> Global credentials (unrestricted) --> Add credentials --> Username:tomcat, password: root123456, id = tomcatcreds



Open Pipeline Syntax
-------------------
select deploy:Deploy war/ear to a container
WAR/EAR files = **/*.war
Context path = myapp
Add container --> select tomcat 9
credentials = tomcatcreds
Tomcat URL = http://13.126.17.44:8080/

Generate

pipeline {
    agent {
        label 'slave1'
    }
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('deploy') {
            steps {
                deploy adapters: [tomcat9(credentialsId: 'tomcatcreds', path: '', url: 'http://13.126.17.44:8080/')], contextPath: 'myapp', war: '**/*.war'
            }
        }
       
    }
}
	
Upload artifacts to S3
======================

Instal plugins = S3 publisher

RESTART the Jenkins

Manage Jenkins --> System --> S3 Profile --> Name = s3creds, Access Key and Secret Key and Test

Create a New Build job

Use Pipeline Script S3upload and fill all details


pipeline {
    agent {
        label 'slave1'
    }
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Upload to S3') {
            steps {
                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'jenkins-artifacts-demo-test', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: false, noUploadOnFailure: false, selectedRegion: 'ap-south-1', showDirectlyInBrowser: false, sourceFile: '**/*.war', storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: true]], pluginFailureResultConstraint: 'FAILURE', profileName: 's3creds', userMetadata: []
            }
        }        
        stage('deploy') {
            steps {
                deploy adapters: [tomcat9(credentialsId: 'tomcatcreds', path: '', url: 'http://13.126.17.44:8080/')], contextPath: 'myapp', war: '**/*.war'
            }
        }
       
    }
}



===============================================================================================================================================================================================================================
                                                                                                     **SONARQUBE**


====== TAKE SonarQube.sh to install ======


SETUP: Launch another EC2 instance - Amazon linux 2 - t2.medium is must , it will not run on micro

============ script ========================
#! /bin/bash
#Launch an instance t2.medium, port 9000
cd /opt/
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.6.50800.zip
unzip sonarqube-8.9.6.50800.zip
amazon-linux-extras install java-openjdk11 -y
useradd sonar
chown sonar:sonar sonarqube-8.9.6.50800 -R
chmod 777 sonarqube-8.9.6.50800 -R
su - sonar
# use the below command manually after installation
#sh /opt/sonarqube-8.9.6.50800/bin/linux/sonar.sh start
#echo "user=admin & password=admin"

=================================================

Once installed manually type below command

sh /opt/sonarqube-8.9.6.50800/bin/linux-x86-64/sonar.sh start

dont run this - echo "user=admin & password=admin"

==================

==============================
If you have stopped the sonar, start the sonar manually
su - sonar
sh /opt/sonarqube-8.9.6.50800/bin/linux-x86-64/sonar.sh start
===============================

Open http://IP:9000

username: admin,
Password : admin



Add project --> Manually --> project key =boomapp --> generate token--> boomapp --> click on maven

935d92d63296213c6262bd6ac803fccc12a0a6f4

74ec73b493b9c1b6fd5cf682dcb4618480652233

In Jenkins: Add plugin,
======================

SonarQube Scanner,
Maven Integration plugins
Sonar Scanner Quality Gates
RESTART JENKINS

Go to Credentials
-----------------
--> Kind= Secret text , secret = c7fc89bc61e79ad060081f980aa3b0a40dfe9115 (sonar project key)
id --> sonar , description = sonar

Go to System
-------------
--> SonarQube servers --> Enable Environment Variables --> Add SonarQube Server
     Name = SonarQube
     URL = http://3.110.190.217:9000
     Server authentication token = select token - sonar


Go to  Tools
-------------
--> SonarQube Scanner installations , Name = sonarscanner --> dont click add installer


===============================

pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar'
                }
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
    }
}



OWASP Dependency-Check is a tool that identifies project dependencies and checks if there are any known, publicly disclosed, vulnerabilities. It can be used in various software development contexts to enhance the security of applications by identifying and alerting developers about vulnerable components that may be included in their projects.

Go to Plugins and install  OWASP Dependency-Check and restart

Go to Tools --> Dependency-Check installations --> Name = DC --> install automatically --> Save


OWASP Integration to Jenkins
-----------------------------
Go to Manage Jenkins --> Tools -->Dependency-Check installations --> Add
Name = DC
Install automatically
Add installer --> install from GitHub


        stage('OWASP Dependency Check'){
            steps {
                dependencyCheck additionalArguments: '--scan ./', odcInstallation: 'DC'
                dependencyCheckPublisher pattern: '**/dependency-check-report.xml'
            }
        }
        stage('Sonar Quality Gate Scan'){
            steps {
                timeout(time: 15, unit:"MINUTES"){
                    waitForQualityGate abortPipeline: false
                }
                   
            }
        }
==========================================================================================================================================================================================================
                                                                                              **NEXUS**
Nexus
------------
Take the script using github and install nexus . Use ubuntu t2,medium
http://server_IP:8081  
========================================================================================

Click on SignIN --> Username = admin, password = cat /opt/nexus/sonatype-work/nexus3/admin.password

Creating Repo
=============
Click on Setting Symbol --> Repositories --> Create repository --> maven2(hosted) --> name(hotstar) --> save

Version Policy --> Snapshot

Deployment policy --> allow to redeploy



Integrate Nexus to Jenkins Pipeline
================================

Jenkins (Code --> Build --> Test --> Artifact) --> Nexus

If you want to integrate any 3rd party to Jenkins, install plugins

1. Download the Plugins (nexus artifacts uploader)
   --> Manage Jenkins --> Plugins --> Available Plugins --> Just type only Nexus , it will give nexus artifacts uploader
2. Configure in Jenkins pipeline in stage

Note: all the information will be available in pom.xml file

First create Credentials for Nexus
----------------------------------

Manage Jenkins --> Credentials --> Username = admin, password = root123  , id = nexus , description = nexus

Create a new pipeline job --> and copy paste the script and click on pipeline systax

sample step : nexusartifactuploader
Nexus Version : see nexus version in browser left top 3.0
Protocol:http
Nexus URL : IP:8081 (dont put http)
Credentials : Add --> Jenkins --> username:admin, password: root123, description: nexus
credentials: select admin
Groupid: see pom file (in.reyaz)
version: see pom file (1.2.2) 8.3.3-SNAPSHOT
Repository: repo that you created in nexus  : hotstar
Artifact: Add : Artifactid: see pom (myapp) , Type = .war, classifier = empty,  File = target/myapp.war
 --> generate pipeline script

nexusArtifactUploader artifacts: [[artifactId: 'myapp', classifier: '', file: 'target/myapp.war', type: '.war']], credentialsId: 'nexus', groupId: 'in.reyaz', nexusUrl: '13.126.193.100:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'hotstar', version: '8.3.3-SNAPSHOT'

pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('nexus') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'myapp', classifier: '', file: 'target/myapp.war', type: '.war']], credentialsId: 'nexuscreds', groupId: 'in.reyaz', nexusUrl: '13.127.129.23:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'hotstarapp', version: '8.3.3-SNAPSHOT'
            }
        }
       
    }
}

=====================================================================================================================================================================================================================
                                                                                                 **JENKINS COMPLETE PROJECT**
Jenkins Project
===============

Setup Jenkins (AL2 with t2.micro)
Integrate GIT
Integrate Maven
Setup Sonar (AL2 with t2.medium)
Setup Tomcat (AL2 with t2.micro)
Setup S3 for Artifacts
Setup Nexus (Ubuntu 24 with t2.medium)

Login and set the hostnames
-------------------------

hostnamectl set-hostname Jenkins
hostnamectl set-hostname sonar
hostnamectl set-hostname tomcat
hostnamectl set-hostname nexus

----------------------------------------
Install Jenkins using Script from GitHub, 8080
Install Sonar using Script from GitHub , 9000
Install Tomcat using Script from GitHub. 8080
Install Nexus using GitHub, 8081

----------------------------------------


Plugins
-------
Deploy to Container,
S3 Publisher,
nexus artifacts uploader
SonarQube Scanner, 
Maven Integration plugins ,
Sonar Scanner Quality Gates ,
OWASP Dependency-Check.


==================================
Integrate tomcat in Jenkins Pipeline 
--------------------------------

Install plugin : deploy to container
-----------------------------------

Restart Jenkins

systemctl restart Jenkins.service

Manage Jenkins --> Credentials --> System --> Global credentials (unrestricted) --> Add credentials --> Username:tomcat, password: root123456, id = tomcatcreds



Open Pipeline Syntax 
-------------------
select deploy:Deploy war/ear to a container
WAR/EAR files = **/*.war
Context path = myapp
Add container --> select tomcat 9
credentials = tomcatcreds
Tomcat URL = http://13.126.17.44:8080/

Generate 
=========

Tomcat integration Pipeline
--------------------------

pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git'
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }       
        stage('deploy') {
            steps {
                deploy adapters: [tomcat9(credentialsId: 'tomcatcreds', path: '', url: 'http://13.126.17.44:8080/')], contextPath: 'myapp', war: '**/*.war'
            }
        }
        
    }
}
	




Upload artifacts to S3
======================

Instal plugins = S3 publisher

RESTART the Jenkins

Manage Jenkins --> System --> S3 Profile --> Name = s3creds, Access Key and Secret Key and Test

Create a New Build job 

Use Pipeline Script S3upload and fill all details 

S3 profile = s3creds
Source = **/*.war
Destination bucket = jen-pro-demo/
Bucket Region = ap-south-1
Server Side Encryption ON


pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git'
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Upload to S3') {
            steps {
                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'jenkins-artifacts-demo-test', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: false, noUploadOnFailure: false, selectedRegion: 'ap-south-1', showDirectlyInBrowser: false, sourceFile: '**/*.war', storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: true]], pluginFailureResultConstraint: 'FAILURE', profileName: 's3creds', userMetadata: []
            }
        }        
        stage('deploy') {
            steps {
                deploy adapters: [tomcat9(credentialsId: 'tomcatcreds', path: '', url: 'http://13.126.17.44:8080/')], contextPath: 'myapp', war: '**/*.war'
            }
        }
        
    }
}


=====================================
SONARQUBE 
=====================================

====== TAKE SonarQube.sh to install ======


SETUP: Launch another EC2 instance - Amazon linux 2 - t2.medium is must , it will not run on micro

============ script ========================
#! /bin/bash
#Launch an instance t2.medium, port 9000
cd /opt/
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.6.50800.zip
unzip sonarqube-8.9.6.50800.zip
amazon-linux-extras install java-openjdk11 -y
useradd sonar
chown sonar:sonar sonarqube-8.9.6.50800 -R
chmod 777 sonarqube-8.9.6.50800 -R
su - sonar
# use the below command manually after installation
#sh /opt/sonarqube-8.9.6.50800/bin/linux/sonar.sh start
#echo "user=admin & password=admin"

=================================================

Once installed manually type below command

sh /opt/sonarqube-8.9.6.50800/bin/linux-x86-64/sonar.sh start

dont run this - echo "user=admin & password=admin"

==================

==============================
If you have stopped the sonar, start the sonar manually
su - sonar
sh /opt/sonarqube-8.9.6.50800/bin/linux-x86-64/sonar.sh start
===============================

Open http://IP:9000

username: admin, 
Password : admin



Add project --> Manually --> project key =myapp --> generate token--> myapp --> click on maven

d38f10c7e6d3d7aab93b9eb6b398869068308b90

Go to Credentials 
-----------------
--> Kind= Secret text , secret = c7fc89bc61e79ad060081f980aa3b0a40dfe9115 (sonar project key)
id --> sonar , description = sonar

Go to System 
-------------
--> SonarQube servers --> Enable Environment Variables --> Add SonarQube Server
     Name = SonarQube
     URL = http://3.110.190.217:9000
     Server authentication token = select token - sonar


Go to  Tools 
-------------
--> SonarQube Scanner installations , Name = sonarscanner --> dont click add installer


===============================


pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git'
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar' 
                }
            }
        }        
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Pushing Artifacts to S3') {
            steps {
                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'jen-pro-demo/', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: false, noUploadOnFailure: false, selectedRegion: 'ap-south-1', showDirectlyInBrowser: false, sourceFile: '**/*.war', storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: true]], pluginFailureResultConstraint: 'FAILURE', profileName: 's3creds', userMetadata: []
            }
        }
        stage('deploy') {
            steps {
                deploy adapters: [tomcat9(credentialsId: 'tomcatcreds', path: '', url: 'http://3.111.245.237:8080/')], contextPath: 'myapp', war: '**/*.war'
            }
        }
        
    }
}



OWASP Dependency-Check is a tool that identifies project dependencies and checks if there are any known, publicly disclosed, vulnerabilities. It can be used in various software development contexts to enhance the security of applications by identifying and alerting developers about vulnerable components that may be included in their projects.

Go to Plugins and install  OWASP Dependency-Check and restart

Go to Tools --> Dependency-Check installations --> Name = DC --> install automatically --> Save


OWASP Integration to Jenkins
-----------------------------
Go to Manage Jenkins --> Tools -->Dependency-Check installations --> Add
Name = DC
Install automatically
Add installer --> install from GitHub


        stage('OWASP Dependency Check'){
            steps {
                dependencyCheck additionalArguments: '--scan ./', odcInstallation: 'DC'
                dependencyCheckPublisher pattern: '**/dependency-check-report.xml'
            }
        }
        stage('Sonar Quality Gate Scan'){
            steps {
                timeout(time: 15, unit:"MINUTES"){
                    waitForQualityGate abortPipeline: false
                }
                    
            }
        }  

==============================
NEXUS
============================
Launch Ubuntu 24 with t2.medium(must) and install Nexus

Architecture

GitHub --> Jenkins (Jenkins delivery artifacts war) --> this war file now pushed to Nexus

Open --> http://ip:8081 --> it takes 13 secs

Click on sign in --> username = admin, password = cat /app/sonatype-work/nexus3/admin.password
next --> root123456 --> Disable anonymous access

Creating Repo
=============
Click on Setting Symbol --> Repositories --> Create repository --> maven2(hosted) --> name(hotstar) --> save

Version Policy --> Snapshot 

Deployment policy --> allow to redeploy



Integrate Nexus to Jenkins Pipeline
================================

Jenkins (Code --> Build --> Test --> Artifact) --> Nexus 

If you want to integrate any 3rd party to Jenkins, install plugins

1. Download the Plugins (nexus artifacts uploader)
   --> Manage Jenkins --> Plugins --> Available Plugins --> Just type only Nexus , it will give nexus artifacts uploader 
2. Configure in Jenkins pipeline in stage

Note: all the information will be available in pom.xml file

First create Credentials for Nexus
----------------------------------

Manage Jenkins --> Credentials --> Username = admin, password = root123456  , id = nexus , description = nexus

Create a new pipeline job --> and copy paste the script and click on pipeline syntax

sample step : nexusartifactuploader
Nexus Version : see nexus version in browser left top 3.0
Protocol:http
Nexus URL : IP:8081 (dont put http)
Credentials : Add --> Jenkins --> username:admin, password: root123, description: nexus
credentials: select admin
Groupid: see pom file (in.reyaz)
version: see pom file (1.2.2) 8.3.3-SNAPSHOT
Repository: repo that you created in nexus  : hotstar
Artifact: Add : Artifactid: see pom (myapp) , Type = .war, classifier = empty,  File = target/myapp.war
 --> generate pipeline script

nexusArtifactUploader artifacts: [[artifactId: 'myapp', classifier: '', file: 'target/myapp.war', type: '.war']], credentialsId: 'nexus', groupId: 'in.reyaz', nexusUrl: '13.126.193.100:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'hotstar', version: '8.3.3-SNAPSHOT'

Final Pipeline
--------------

pipeline {
    agent any
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git'
            }
        }
        stage('compile') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar' 
                }
            }
        }        
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Pushing Artifacts to S3') {
            steps {
                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'jen-pro-demo/', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: false, noUploadOnFailure: false, selectedRegion: 'ap-south-1', showDirectlyInBrowser: false, sourceFile: '**/*.war', storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: true]], pluginFailureResultConstraint: 'FAILURE', profileName: 's3creds', userMetadata: []
            }
        }
        stage('deploy') {
            steps {
                deploy adapters: [tomcat9(credentialsId: 'tomcatcreds', path: '', url: 'http://3.111.245.237:8080/')], contextPath: 'myapp', war: '**/*.war'
            }
        }
        stage('Push to Nexus') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'myapp', classifier: '', file: 'target/myapp.war', type: '.war']], credentialsId: 'nexus', groupId: 'in.reyaz', nexusUrl: '3.110.32.212:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'hotstar', version: '8.3.3-SNAPSHOT'
            }
        }
        
    }
}
======================================================================================================================================================================================================================
                                                                                                **ANSIBLE**
Launch Amazon Linux 2 , t2.micro

Installation, Ansible needs python
----------------------------------

amazon-linux-extras install ansible2 -y

yum install python3 python-pip python-dlevel -y

ansible --version

Examples: Adhoc Commands
------------------------

ansible -m ping localhost    [Ping localhost]

ansible localhost -a "yum install git -y"
ansible localhost -a "git --version"
ansible localhost -a "yum install maven -y"
ansible localhost -a "mvn --version"
ansible localhost -a "touch file1"

ansible localhost -a "ls"

ansible localhost -a "yum install httpd -y"

ansible localhost -a "systemctl status httpd"

ansible localhost -a "systemctl start httpd"

ansible localhost -a "user add reyaz"

ansible localhost -a "cat /etc/passwd"

ansible localhost -a "yum remove git* maven* httpd* -y"

SETUP:
======

CREATE 5 SERVERS - Amazon Linux 2 [1=ANSIBLE Master, 2=PROD, 2=DEV]

EXECUTE THE BELOW COMMANDS ON ALL SERVERS:
--------------------------------------------


sudo -i
hostnamectl set-hostname ansible/prod-1/prod-2/dev-1/dev-2
sudo -i

Now Root user from ansible server needs to login to all servers using root username and password

Do below commands in all servers using multi-exec

## first set the password for root
passwd root  

set new password: root123456

## enable all server to login as root

vi /etc/ssh/sshd_config (38 & 61 uncomment both lines)
systemctl restart sshd
systemctl status sshd
hostname -i  -- to see the private ip

Go to Ansible Master
--------------------
Lets Generate SSH Keys, using this KEY Ansible server will communicate with worker nodes

ssh-keygen

ll .ssh


ssh-copy-id root@private ip of prod-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of prod-2 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of dev-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of dev-2 -- > yes -- > password -- > ssh private ip -- > ctrl d

just to see the connection

ssh root@privateip


Inventory file
=============

There are 2 types of inventory file, static and Dynamic

vi /etc/ansible/hosts
# Ex 1: Ungrouped hosts, specify before any group headers.
[prod]
172.31.20.40
172.31.21.25
[dev]
172.31.31.77
172.31.22.114


Another method for authentication
---------------------------------
In host file create variables

[all:var]
ansible_user=ec2-user
ansible_ssh_private_key_file=/home/ec2-user/MyKey.pem

-----------------------------------------------------


ansible-inventory --list   --> This will show the inventory list

ansible -m ping all : To check worker node connection with ansible server.

1. ADHOC COMMANDS:
-----------------
these are simple Linux commands.
ADHOC commands  are great for tasks you repeat daily
These are used for temp works.

-a = argument

ansible all -a "yum update -y"
ansible all -a "yum install git -y"
ansible all -a "git --version"
ansible all -a "yum install maven -y"
ansible all -a "mvn --version"
ansible all -a "touch file1"
ansible all -a "touch reyaz.txt"
ansible all -a "ls"
ansible all -a "yum install httpd -y"
ansible all -a "systemctl status httpd"
ansible all -a "systemctl start httpd"
ansible all -a "user add reyaz"
ansible all -a "cat /etc/passwd"
ansible all -a "yum remove git* maven* httpd* -y"
Modules supports
-----------------


ADHOC commands  are great for tasks you repeat daily

ADHOC     : ansible all -a "yum install git -y"
          ansible all -a "yum remove git* -y"
-----

Ansible Modules are units of code that can control system resources or execute system commands

MODULE    : ansible all -m yum -a "name=git state=present"  --> -m is module , name = packagename, , state = present(install)
------

States:
-------
yum modules (States for Package Management)
-----------
Present = install
absent = uninstall
latest = install or upgrades to latest version

Services module (States for Service Management)
---------------
started = start
stopped = stop
restarted = restart the service
enabled = Ensures the service starts on boot.






install packages with modules
--------------------------------
ansible all -m yum -a "name=git state=present"

ansible all -a "git -v"

ansible all -m yum -a "name=maven state=present"

To install Apache service
------------------------

ansible all -m yum -a "name=httpd state=present"

TO see the Version of Apache
----------------------------

ansible all -a "httpd -v"

TO start Apache
----------------

ansible all -m service -a "name=httpd state=started"  

---> this is use to start the service {we changed module yum to service, because yum to install, service to start}

TO see the status of Apache
------------------------

ansible all -a "systemctl status httpd" --> to see the status of httpd in all servers

To Stop Apache service
---------------------

ansible all -m service -a "name=httpd state=stopped" --> to stop the service

Update Apache to latest version
------------------------------

ansible all -m yum -a "name=httpd state=latest"   --> to update to latest version

uninstall apache:
---------------

ansible all -m yum -a "name=httpd state=absent"  ---> to uninstall httpd, absent = uninstall


==================================

TO create users in servers
--------------------

ansible all -m user -a "name=reyaz state=present"  --> present = to create

ansible all -a "cat /etc/passwd"

create another user

==============================

To create a file
--------------

touch file1

ansible all -m copy -a "src=file1 dest=/home/ec2-user/"

ansible all -a "ls /home/ec2-user"

====================

Remove all packages
----------------

ansible all -a "yum remove  maven* git* httpd* -y"

==============================================

yum , service, user etc all these are modules


PLAYBOOK:
its a collection of modules.
we can execute multiple modules inside playbooks.
playbook is written on YAML langauge.
YAML=YET ANOTHER MARKUP LANGUAGE
Extension: .yml or .yaml


PLAYBOOK-1:

# for comments, no option for multiline comment, only single line with #

- meaning description


===========================================
Test all Hosts Connectivity: ping module
===========================================


vi pb1.yml

---
- name: First Playbook
  hosts: all
 
  tasks:
  - name: Test connectivity
    ping:

ansible-playbook --syntax-check pb1.yml

ansible-playbook pb1.yml



===========================================
Installing APACHE and print Message
===========================================

vi pb2.yml

---
- name: Installing Apache
  hosts: all
  become: yes
  tasks:
    - name: Installing Apache
      yum:
        name: httpd
        state: present
    - name: Print Message
      debug: msg="Apache Installed"


ansible-playbook pb2.yml

===========================================
Installing and start Apache, yum and service module
===========================================


vi pb2.yml

---
- name: Installing Apache
  hosts: all
  become: yes
  tasks:
    - name: Installing Apache
      yum:
        name: httpd
        state: present
    - name: Print Message
      debug: msg="Apache Installed"

    - name: Start Apache
      service:
        name: httpd
        state: started


ansible-playbook pb2.yml


===========================================
Installing GIT and Docker and starting docker
===========================================

vi pb3.yml

---
- name: Install GIT and DOCKER Playbook
  hosts: all
  tasks:
    - name: Installing GIT
      yum: name=git state=present

    - name: Installing Docker
      yum: name=docker state=present

    - name: Starting Docker Service
      service: name=docker state=started

ansible-playbook pb3.yml

==================================
Creating User and Copy files playbook
====================================

vi pb4.yml
---
- name: Create users and copy files
  hosts: all
  tasks:
    - name: Creating User
      user: name=ramesh state=present

    - name: Copy files
      copy: src=file.txt dest=/home/ec2-user/


ansible-playbook --syntax-check pb4.yml  --> this will show you syntax errors

you can correct yourself syntax or use online tool called yamllint.com to see where are the errors

ansible-playbook pb3.yml

COLOR CODES
-----------
Yellow = successfully executed
Red = Failed
Green = Already executed
Blue = Skipped


Gather facts : First it will get all information about all worker nodes , default task performed by ansible

OK = total number of tasks
changed = no of tasks successfully executed


--> again run the same playbook --> now ok =6 , changed =0

---> now all are installed on workernodes , now try to uninstall , to uninstall , state=absent

--> use sed command to replace present to absent

=============================================
Uninstall docker and GIT playbook
=============================================

sed -i 's/present/absent/g' pb3.yml

vi pb3.yml

---
- name: Uninstall GIT and DOCKER Playbook
  hosts: all
  tasks:
    - name: Uninstalling GIT
      yum: name=git state=absent

    - name: Uninstalling Docker
      yum: name=docker state=absent

    - name: Starting Docker Service
      service: name=docker state=started


ansible-playbook pb3.yml

---> now you see some error for docker , because dokcer package is removed do that it is not starting

--> we can ignore these error by placing ignore_errors: true in playbook

============================
Ignore ERROR messages
=============================

---
- name: Uninstall GIT and DOCKER Playbook
  hosts: all
  ignore_errors: true
  tasks:
    - name: Uninstalling GIT
      yum: name=git state=present

    - name: Uninstalling Docker
      yum: name=docker state=present

    - name: Starting Docker Service
      service: name=docker state=started
================================
Install NGINX using Playbook
=================================

vi nginx.yml

---
- name: Install Nginx and Start it
  hosts: all
  become: yes # Run as root
  tasks:
    - name: Enable Nginx Repo    
      command: amazon-linux-extras enable nginx1 # Add nginx to amazon package manager repo

    - name: Install Nginx
      yum:
        name: nginx
        state: present

    - name: Start Nginx
      service:
        name: nginx
        state: started
        enabled: yes


ansible-playbook nginx.yml


================================
Deploy application on NGINX using Playbook
=================================

vi nginx.yml

---
- name: Install Nginx and Start it
  hosts: all
  become: yes # Run as root
  tasks:
    - name: Enable Nginx Repo
      command: amazon-linux-extras enable nginx1

    - name: Install Nginx
      yum:
        name: nginx
        state: present

    - name: Start Nginx
      service:
        name: nginx
        state: started
        enabled: yes

    - name: Deploy a web page
      copy:
        src: index.html
        dest: /usr/share/nginx/html/index.html

    - name: Restart Nginx
      service:
        name: nginx
        state: restarted



==============================
Setup module
=============================

ansible all -m setup

this command will give you full information about all workernodes, the above command shows a lot of info, but if you want to get only particular data grep

ansible all -m setup | grep -i cpu
ansible all -m setup | grep -i mem

cpu         : ansible_processor_vcpus
mem        : ansible_memtotal_mb
host        : ansible_nodename
flavour        : ansible_os_family
pkg manager    : ansible_pkg_mgr
blk        : block_available
kernel        : BOOT_IMAGE

DEBUG Module - to print values
---------

debug module: it will print messages

- hosts: all
  tasks:
    - name: printging a msg
      debug:
        msg: "server name is: {{ansible_nodename}}, number of cpus: {{ansible_processor_vcpus}}, total memsize: {{ansible_memtotal_mb}}, the flavour is: {{ansible_os_family}}, package manager is: {{ansible_pkg_mgr}}"



TAGS: used to execute or skip specific tasks in playbook.
======================================================

- hosts: all
  tasks:
    - name: installing git
      yum: name=git state=present
      tags: git
    - name: installing docker
      yum: name=docker state=present
      tags: dockerinstall
    - name: starting docker
      service: name=docker state=started
      tags: dockerstart
    - name: create user
      user: name=suresh state=present
      tags: user

SINGLE TAG:ansible-playbook reyaz.yml --tags user
MULTI TAG: ansible-playbook reyaz.yml --tags dockerinstall,dockerstart

--> remove all packages now

sed -i 's/present/absent/g' reyaz.yml   or  sed -i 's/present/absent/; s/installing/uninstalling/' reyaz.yml  ---> replace words using sed


ansible all -a "yum install git -y"

SKIPPING SINGLE TAG: ansible-playbook reyaz.yml --skip-tags "git"

SKIPPING MULTI TAG: ansible-playbook reyaz.yml --skip-tags "dockerinstall,dockerstart"

===================================


VARIBALES: Static variables and Dynamic Variables

===============

remove all packages first to install again

ansible all -a "yum remove git* maven* httpd* docker* -y"

STATIC VARS: these variables will declared inside playbook.
it will not change until unless we change.

vi staticvar.yml

- hosts: all
  vars:
    a: git
    b: maven
  tasks:
    - name: installing git
      yum: name={{a}} state=present
    - name: installing maven
      yum: name={{b}} state=present


ansible-playbook staticvar.yml

---> now uninstall again

sed -i 's/present/absent/g' staticvar.yml

ansible-playbook staticvar.yml


DYNAMIC VARS: these variables will declared outside playbook.it will  change frequently as per our requirement.
------------


vi dynamicvar.yml

- hosts: all
  tasks:
    - name: installing pkg
      yum: name={{a}} state=present
    - name: installing pkg
      yum: name={{b}} state=present    

ansible-playbook dynamicvar.yml --extra-vars "a=git b=maven"

uninstall again
------

sed -i 's/present/absent/' dynamicvar.yml

ansible-playbook dynamicvar.yml --extra-vars "a=git b=maven"

=====================================
LOOPS - Reduce the number of lines of code
====================

Make sure no software are there on workernodes, we will run the commands again

vi loop.yml

- hosts: all
  gather_facts: false
  tasks:
    - name: installing pkgs
      yum: name={{item}} state=present
      with_items:
        - git
        - tree
        - docker
        - httpd
        - java-1.8.0-openjdk
        - maven

ansible-playbook loop.yml

see all packges are installed
ansible all -a "mvn -v"
ansible all -a "docker -v"

uninstall
---------

vi loopuninstall.yml

- hosts: all
  tasks:
    - name: installing pkgs
      yum: name="{{item}}" state=absent
      with_items:
        - git*
        - tree*
        - httpd*
        - java-1.8.0-openjdk*
        - maven*

ansible-playbook loopuninstall.yml

ansible all -a "mvn -v"
ansible all -a "docker -v"


creating users - replace yum module with user in playbook
-------------

vi loopusers.yml


- hosts: all
  tasks:
    - name: Creating Users
      user: name="{{item}}" state=present
      loop:
        - lucky
        - imthiaz
        - siva
        - rajesh
        - pavan

ansible-playbook loopusers.yml

ansible all -a "cat /etc/passwd"

remove users
-----------

vi removeloopusers.yml

- hosts: all
  tasks:
    - name: installing pkgs
      user: name="{{item}}" state=absent
      loop:
        - lucky
        - imthiaz
        - siva
        - rajesh
        - pavan

ansible-playbook removeloopusers.yml
	
==============================
Handlers:
===============================

one task will depend on another task.
when task-1 is executed it will ask to run task-2.
notify is used to tell task-2 to execute.

---> apache should be installed first and then start apache later, so starting service is depending on installation
--> notify is calling the handler

vi handlers.yml

---
- name: Handlers
  hosts: all
  tasks:
    - name: installing apache
      yum: name=httpd state=present
      notify: starting apache

  handlers:
    - name: starting apache
      service: name=httpd state=started

ansible-playbook handlers.yml

--->
sed -i 's/present/absent/g; s/install/uninstall/g' handlers.yml

or

vi handlersuninstall.yml  ---> httpd will be uninstalled, but there is no service to start , so ignore errors

---
- name: Handlers uninstall
  hosts: all
  tasks:
    - name: uninstalling apache
      yum: name=httpd state=absent
      notify: starting apache

  handlers:
    - name: starting apache
      service: name=httpd state=started
      ignore_errors: yes


ansible-playbook handlersuninstall.yml

============================
SHELL in Playbook
========================

We can directly run shell / Linux commands in playbook , use module shell or command or raw module, all are same -->

raw is latest module, raw is commonly used

✅ Use command whenever possible (it's safer).
✅ Use shell only if needed (for pipes, loops, variables).
✅ Use raw only for bootstrapping systems without Python.


vi shell.yml


---
- name: Shell in Playbook
  hosts: all
  tasks:
    - name: installing apache
      shell: yum install httpd -y

    - name: installing git
      command: yum install git -y

    - name: installing maven
      raw: yum install maven -y


ansible-playbook shell.yml

uninstall
---------

sed -i 's/install/remove' shell.yml

or

vi shellremove.yml

- hosts: all
  tasks:
    - name: installing apache
      shell: yum remove httpd -y

    - name: installing git
      command: yum remove git -y

    - name: installing maven
      raw: yum remove maven -y




=======================================
CONDITIONS
===========================================

CLUSTER: group of servers/nodes which communicate with each other.
HOMOGENIUS: all servers with same os and flavour.
HETROGENIUS: all servers with different os and flavour.

RedHat: yum
Ubuntu: apt
Python : pip  (if you want to install numpy and pandas in python use pip)


ansible all -m setup
ansible all -m setup | grep -i family


CONDITIONS:

- hosts: all
  tasks:
    - name: installing apache on RedHat
      yum: name=httpd state=present
      when: ansible_os_family == "RedHat"

    - name: installing apache on Ubuntu
      apt: name=apache2 state=present
      when: ansible_os_family == "Debian"

ansible all -m setup
ansible all -m setup | grep -i family


- hosts: all
  gather_facts: false
  tasks:
    - name: installing apache on RedHat
      yum: name=httpd state=present
      when: ansible_os_family == "RedHat"

    - name: installing apache on Ubuntu
      apt: name=apache2 state=present
      when: ansible_os_family == "Debian"

if you want to install only on dev-1 node

- hosts: all
  gather_facts: false
  tasks:
    - name: installing apache on RedHat
      yum: name=httpd state=present
      when: ansible_nodename == "dev-1"

=======================================

LAMP:

L : LINUX
A : APACHE
M : MYSQL
P : PHP or Python

WAMP - IN WIndows, Windows, apache, MySQL and php or python

In node machine we are using amazon Linux 2 , it has already python installed , so last module will not work

---
- name: LAMP
  hosts: all
  tasks:
    - name: Installing Apache
      yum: name=httpd state=present

    - name: Installing Mysql
      yum: name=mysql state=present

    - name: Installing Python
      yum: name=python3 state=present

=============================
LOOKUPS -
============================

Lookup --> this module is used to read the data from file, db, and key value

For Example: create a file creds.txt  --> username:reya, password:123

---
- name: Lookups
  hosts: all
  vars:
    creds: "{{lookup('file', '/root/creds.txt')}}"
  tasks:
    - debug:
        msg: "My Credentials are {{creds}}"

=========================
Jinja2 Templates in Ansible  
===========================

Jinja2 is a templating engine used in Ansible for dynamically generating files, configurations, and scripts based on variables.

The Jinja2 templating engine is quite powerful and widely used with other frameworks and applications such as Flask and Django.


🔹 Example: Nginx Configuration Template
------------------------------------------

mkdir templates
cd templates

📌 Create a Jinja2 Template (templates/nginx.conf.j2)

vi nginx.conf.j2

server {
    listen {{ nginx_port }};
    server_name {{ server_name }};

    location / {
        root {{ web_root }};
        index index.html;
    }
}

This template uses variables ({{ variable_name }}) that will be replaced dynamically.


vi nginx.yml

---
- name: Deploy Nginx Config using Jinja2
  hosts: all
  become: yes
  vars:
    nginx_port: 80
    server_name: mywebsite.com
    web_root: /var/www/html

  tasks:
    - name: Enable Nginx Repo
      command: amazon-linux-extras enable nginx1

    - name: Install Nginx
      yum:
        name: nginx
        state: present

    - name: Copy Nginx Config with Template
      template:
        src: templates/nginx.conf.j2
        dest: /etc/nginx/nginx.conf
      notify: Restart Nginx

  handlers:
    - name: Restart Nginx
      service:
        name: nginx
        state: restarted

ansible-playbook nginx.yml

=========================
Jinja2 HTTPD another Example
==========================

Below is a Jinja2 template for an Apache (httpd.conf) configuration file. It dynamically sets port, document root, and virtual hosts using Ansible variables.

🔹 Step 1: Create Jinja2 Template (templates/httpd.conf.j2)
----------------------------------------------------------

mkdir templates

# Apache HTTPD Configuration File

vi httpd.conf.j2

Listen {{ http_port }}

<VirtualHost *:{{ http_port }}>
    ServerName {{ server_name }}
    DocumentRoot {{ document_root }}

    <Directory "{{ document_root }}">
        AllowOverride None
        Require all granted
    </Directory>

    ErrorLog /var/log/httpd/{{ server_name }}_error.log
    CustomLog /var/log/httpd/{{ server_name }}_access.log combined
</VirtualHost>


Variables Used:
--------------

http_port → Defines the listening port.
server_name → Sets the domain name.
document_root → Defines the root directory for website files.

🔹 Step 2: Ansible Playbook Using the Template
-----------------------------------------------

Create a playbook (apache_setup.yml) to deploy the template:


---
- name: Configure Apache HTTPD with Jinja2
  hosts: all
  become: yes
  vars:
    http_port: 80
    server_name: example.com
    document_root: /var/www/html

  tasks:
    - name: Install Apache (httpd)
      yum:
        name: httpd
        state: present

    - name: Deploy Apache Configuration
      template:
        src: templates/httpd.conf.j2
        dest: /etc/httpd/conf/httpd.conf
      notify: Restart Apache

  handlers:
    - name: Restart Apache
      systemd:
        name: httpd
        state: restarted

What This Playbook Does:
-------------------------

Installs Apache (httpd).
Copies the Jinja2 template (httpd.conf.j2) to /etc/httpd/conf/httpd.conf.
Notifies the handler to restart Apache when the config changes.
================================

Ansible Strategies  

================================


✅ linear (default) = Default execution (wait for all hosts per task)
                      Moves to the next task only after all hosts complete the current task.
   
✅ free = Fastest execution (tasks don't wait for each other)    
           Each host executes tasks as fast as it can, without waiting for others
       Faster execution but tasks can finish out of order.

✅ host_pinned  = Host-by-host execution (one host at a time)    

✅ debug = Debugging playbooks (step-by-step execution)    

✅ custom strategy = Custom execution needs

---
- name: Strategies
  hosts: all
  strategy: Linear
  tasks:
    - name: Installing Apache
      yum: name=httpd state=present

=============================
PIP - its a package manager used to install libs/modules
=============================

Python : pip  (if you want to install numpy and pandas in python use pip)

using pip module install NumPy and Pandas
----------------------------------------
- name: Playbook using pip
  hosts: all
  tasks:
    - name: install pip module
      yum: name=pip state=present

    - name: installing Numpy
      pip: name=NumPy state=present

    - name: installing Pandas
      pip: name=Pandas state=present


=====================
Ansible ROLES
====================

Ansible roles help organize and structure your playbooks by breaking them into reusable components. They simplify automation by grouping related tasks, handlers, variables, and templates in a standardized directory format.


Ansible Roles used to divide playbook into directory structure.

Ansible roles are resuable.


🔹 Why Use Roles?
✅ Modular & Reusable: Write once, use multiple times.
✅ Scalable: Easily manage configurations across many systems.
✅ Organized: Avoid large, cluttered playbooks.

🔹 Role Directory Structure
When you create a role, Ansible generates a standard folder structure:



yum install tree -y ---> to see folders in tree structure



mkdir playbooks
cd playbooks

mkdir -p roles/pkgs/tasks

mkdir -p roles/users/tasks

tree

mkdir -p roles/webserver/tasks

tree

vi roles/pkgs/tasks/main.yml

- name: install pkgs
  yum: name={{item}} state=present
  loop:
    - git
    - java-1.8.0-openjdk
    - tree
    - docker
    - maven


vi roles/users/tasks/main.yml

- name: create users
  user: name={{item}} state=present
  with_items:
    - luckyy
    - imthiaz
    - siva
    - rajesh
    - pavan


vi roles/webserver/tasks/main.yml

- name: install webserver
  yum: name=httpd state=present

- name: starting httpd
  service: name=httpd state=started

--> tree

 --> create a new master yml and mention which role you want to execute

vi master.yml

---
- name: ROLE PLAYBOOK
  hosts: all
  roles:
    - pkgs
    - user
    - webserver

ansible-playbook master.yml


Now uninstall all

sed -i 's/present/absent/g' roles/pkgs/tasks/main.yml

Above command need to do one by one for all yml files, use exec command

find . -type f -exec sed -i 's/present/absent/g' {} \;


find . =     Searches in the current directory (.) and all subdirectories.
-type f =     Finds only files (not directories).
-exec ... \;    Executes a command (sed) on each file found.
sed -i 's/present/absent/g' =     Uses sed (stream editor) to replace "present" with "absent" globally (g) within each file.
{} =        A placeholder for each file found by find.
\; =        Ends the -exec command.

now run the playbook

ansible-playbook master.yml


=========================
Ansible-Galaxy -
=========================

Ansible Galaxy is a repository for sharing pre-built Ansible roles. You can download, install, and use roles to speed up automation.

Use ansible galaxy website and use the roles locally by copying the command like search for tomcat and run the command

https://galaxy.ansible.com/ui/standalone/roles/

search for tomcat and copy paste the command ansible-galaxy role install criecm.tomcat

ansible-galaxy role install criecm.tomcat

By default it will install in /root/.ansible/roles

If you want to change the path , edit ansible.cfg and uncomment

vi /etc/ansible/ansible.cfg
role_path = /etc/ansible/roles

It will create so many roles itself and we can use the roles

if we want to create roles, use either mkdir manually or init command (it will create folder structure)

cd /etc/ansible/roles

ansible-galaxy init Reyaz

tree

.
└── reyaz
    ├── defaults
    │   └── main.yml
    ├── files
    ├── handlers
    │   └── main.yml
    ├── meta
    │   └── main.yml
    ├── README.md
    ├── tasks
    │   └── main.yml
    ├── templates
    ├── tests
    │   ├── inventory
    │   └── test.yml
    └── vars
        └── main.yml




examples

  ansible-galaxy search tomcat
  ansible-galaxy install amtega.tomcat


==========================
Ansible - VAULT - ENCRYPTION
=========================

Ansible Vault allows you to encrypt and secure sensitive data like passwords, API keys, SSH keys, and confidential variables within playbooks.

Ansible vault is used to encrypt the data.

In real time to keep our sensitive data protectively we use vault.

Technique: AES256

we can restrict the playbooks to run.

-------------------------------------------------

ansible-vault create secret.txt  --> to create a vault

give password
db_password: mysecurepassword
api_key: "12345-abcde-67890"


cat secret.txt

Encrypted

ansible-vault edit secret.txt   --> to edit a vault
give password
change something

cat secret.txt

ansible-vault rekey secret.txt   --> to change the password

ansible-vault decrypt secret.txt --> to decrypt and see the content

cat secret.txt

ansible-vault encrypt secret.txt --> to encrypt again

cat secret.txt

ansible-vault view secret.txt  --> to view the content without decryption

======================================
ASYNCHRONOUS & POLLING ACTIONS:
========================================

By default, Ansible runs tasks synchronously, meaning it waits for each task to finish before moving to the next one.
However, for long-running tasks (e.g., software installation, backups, database updates), you can use asynchronous execution with polling to avoid timeouts


For every task in  ansible we can set time limit

If the task is not performed in that time limit ansible will stop playbook execution

This is called as asynchronous and polling.

---
- name: Async and poll playbook
  hosts: all
  ignore_errors: yes
  tasks:
    - name: sleeping
      command: sleep 30
      async: 20
      poll: 10
    - name: install git
      yum: name=git state=present


async: time we set for task to complete
poll: it will check if task is completed or not for every 10 sec

=======================================================
Using Multiple Playbooks in a Single Playbook in Ansible
=======================================================

Method 1: Using import_playbook (Recommended)
-----------------------------------------

---
- import_playbook: setup_webserver.yml
- import_playbook: deploy_application.yml
- import_playbook: configure_firewall.yml

Method 2: Using include_tasks (For Tasks Inside a Playbook)
-----------------------------------------------------------

---
- name: Setup Server
  hosts: all
  become: yes

  tasks:
    - include_tasks: install_nginx.yml
    - include_tasks: configure_firewall.yml




===================
MINI PROJECT - How to setup Front end code
====================

WEB SERVER : TO SHOW THE APP : httpd  : 80  : /var/www/html
frontend code
APP SERVER : TO USE THE APP : Tomcat  : 8080  : tomcat/webapps
frontend code + backend code

vi miniproject.yml
---
- name: Mini Project
  hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: installing git
      yum: name=git state=present

    - name: checkout
      git:
        repo: https://github.com/ReyazShaik/amazon.git
        dest: /var/www/html


copy paste the ip in browser

TO encrypt playbook


ansible-vault create miniproject1.yml

now ansible-playbook miniproject1.yml   --> you cannot run the playbook in encrypt mode

decrypt the playbook and run if you want

ansible-vault decrypt miniproject1.yml

ansible-playbook miniproject1.yml
=========================================================================================================================================================================================================================
                                                                                                      **ANSIBLE FINAL PROJECT**
==================================================
PROJECT - Jenkins, Sonar, Nexus and Ansible Modules deployment to Tomcat
====================================================

Draw architecture diagram

developers push code --> Jenkins(Ansible) --> deploy --> Nodes(tomcats)
                     (CI = CodeBuild+ CodeTest)
                           Artifacts

checkout --> Build --> Test --> Artifacts --> Nexus --> Deploy

Step 1 --> Push code to GitHub
Step 2 --> Generate Artifacts
Step 3 --> Storing the Artifacts
Step 4 --> Installing Tomcat
Step 5 --> Deploy the artifacts to nodes

Pipelines will be integrated with Ansible Playbook

use the same machines, install Jenkins on ansible server

Total 5 machines
---------------

Launch EC2 instance t2.micro = Jenkins + Ansible
Launch EC2 instance t2.medium = Nexus, 8081
Launch EC2 instance t2.medium = Sonar ,9000
Launch EC2 instance 2 t2.micro = worker nodes where we have application running with tomcat. tomcat1 and tomcat2

Step 1 --> keep the code in GitHub (use java-project-new)
----------------------------------------------------

https://github.com/ReyazShaik/java-project-maven-new.git


Step 2 -->  install Jenkins and ansible in server using script
-----------------------------------------------------------

Step -3 --> Install Ansible
----------------
amazon-linux-extras install ansible2 -y

yum install python3 python-pip python-dlevel -y

ansible --version

============================================================================================================
SETUP:
Login to Jenkins+Ansible Server and setup SSH connections to 2 Worker Nodes(tomcat1 and tomcat2)

Inventory file
=============

vi /etc/ansible/hosts
# Ex 1: Ungrouped hosts, specify before any group headers.
[Prod]
172.31.20.40
172.31.21.25

Lets Generate SSH Keys, using this KEY Ansible server will communicate with worker nodes

ssh-keygen

ssh-copy-id root@private ip of prod-1
ssh-copy-id root@private ip of prod-2

========================================================================================================

Step 3: Setup SonarQube


Now Store the Artifacts to S3
================================

install Nexus
------------

==================================================================================================================================
Step 5 : Create a Jenkins pipeline
-----------------------------------

First install plugins - Nexus and Ansible, sonar, maven, deploy to container, S3 Publisher
---------------------

Jenkins --> Manage Jenkins --> plugins --> nexus artifact upload, SonarQube Scanner, Maven Integration, deploy to container and ansible --> install --> restart jenkins

Manage Jenkins --> tools --> Add ansible --> Name=ansible, path = /bin

pipeline code till artifacts and run the pipeline --> it will generate war file

pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}

**Build now***

Integrate with Sonar
-------------------

pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar'
                }
            }
        }

        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}


**Build now***

Integrate with Nexus
--------------------
pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar'
                }
            }
        }
        stage('nexus') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'myapp', classifier: '', file: 'target/myapp.war', type: '.war']], credentialsId: 'nexuscreds', groupId: 'in.reyaz', nexusUrl: '13.233.124.23:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'hotstarapp', version: '8.3.3-SNAPSHOT'
            }
        }
    }
}

S3 Integration
=============

pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar'
                }
            }
        }

        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Upload to S3') {
            steps {
                s3Upload consoleLogLevel: 'INFO', dontSetBuildResultOnFailure: false, dontWaitForConcurrentBuildCompletion: false, entries: [[bucket: 'jenkins-artifacts-demo-test', excludedFile: '', flatten: false, gzipFiles: false, keepForever: false, managedArtifacts: false, noUploadOnFailure: false, selectedRegion: 'ap-south-1', showDirectlyInBrowser: false, sourceFile: '**/*.war', storageClass: 'STANDARD', uploadFromSlave: false, useServerSideEncryption: true]], pluginFailureResultConstraint: 'FAILURE', profileName: 's3creds', userMetadata: []
            }
        }
    }
}





**Build now***

Now Deployment to Production Nodes using Ansible
--------------------------------------------------

Go to Ansible master Server

Install tomcat in all Production nodes using Ansible
---------------------------------------------------

vi tomcat.yml

---
- name: Setup Tomcat
  hosts: all
  become: yes
  tasks:
    - name: Download tomcat from dlcdn
      get_url:
        url: "https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.35/bin/apache-tomcat-10.1.35.tar.gz&quot;
        dest: "/root/"

    - name: untar the apache file
      command: tar -zxvf apache-tomcat-10.1.35.tar.gz

    - name: Rename the tomcat
      command: mv apache-tomcat-10.1.35 tomcat

    - name: Install the latest available Java (OpenJDK)
      yum:
        name: java-17-amazon-corretto
        state: present

    - name: Setting the roles in tomcat-users.xml file
      template:
        src: tomcat-users.xml
        dest: /root/tomcat/conf/tomcat-users.xml

    - name: Delete two lines in context.xml
      template:
        src: context.xml
        dest: /root/tomcat/webapps/manager/META-INF/context.xml
    - name: Create Tomcat systemd Service File
      copy:
        dest: /etc/systemd/system/tomcat.service
        content: |
          [Unit]
          Description=Apache Tomcat Server
          After=network.target

          [Service]
          User=root
          Group=root
          Type=forking
          Environment="JAVA_HOME=/usr/lib/jvm/jre"
          Environment="CATALINA_HOME=/root/tomcat"
          ExecStart=/root/tomcat/bin/startup.sh
          ExecStop=/root/tomcat/bin/shutdown.sh
          Restart=on-failure

          [Install]
          WantedBy=multi-user.target
    - name: Reload systemd
      systemd:
        daemon_reload: yes
    - name: Start tomcat Service
      service:
        name: tomcat
        state: started
        enabled: yes


================================================================================

sed -i 's/87/93/g' tomcat.yml  -- if required


--> instead changing tomcat-user.xml and context.xml in all nodes just create files locally and replace in worker nodes

--> vi tomcat-users.xml
   this file is in git-hub copy paste the content --it has all changed usernames etc
--> vi context.xml
   this file is in git-hub copy paste the content --it has all changed usernames etc

ansible-playbook tomcat.yml

run the playbook --> it will install tomcat in all worker nodes
check with workernodes ip, tomcat is working or not

Now artifacts are in /var/lib/Jenkins/workspace/project/targets

cd /var/lib/Jenkins/workspace/project/target

now create another playbook

Note: In below playbook, change the source to your target project directory

vi deploy.yml
---
- name: Deploy war to tomcat servers
  hosts: all
  tasks:
    - name: task1
      copy:
        src: /var/lib/jenkins/workspace/project/target/myapp.war
        dest: /root/tomcat/webapps

RUn the playbook

ansible-playbook deploy.yml

--------> this is manual work, not good, now use pipelines

Integrate ansible to Jenkins
===========================

Manage Jenkins --> TOOLS --> Ansible --> name = ansible, path = /bin

Manage Jenkins --> Credentials --> username and password --username = root , password = rooot123456 (password while setting up ssh connection to tomcat servers from ansible ssh) , ID = linuxcreds

or

Manage Jenkins --> Credentials --> SSH username with Private Key --username = ec2-user , id = linuxcreds
Private Key -> Enter Directly--> Add --> pem file key


mv deploy.yml /etc/ansible

cd /etc/ansible --> all ansible files are in this folder
   
open pipeline --> add deploy stage --> generate pipeline syntax -->

        stage('Run Ansible Playbook') {
            steps {
               
            }
 

Sample Step = ansibleplaybook:invoke an ansible playbook

Ansible tool : ansible  
Playbook file path in workspace = /etc/ansible/deploy.yml
Inventory file path in workspace =  /etc/ansible/hosts
SSH connection credentials = linuxcreds
disable ssh host key check --> check it --> rest all defaults

Below code will come
-------------------
ansiblePlaybook credentialsId: 'linuxcreds', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''


Optional:
(host subset - we can give test or dev or prod , no need to give in playbook host: dev)
(in host subset give $server = prod)
copy the script and put in deploy section in pipeline --> in pipeline ansible stage  see limit : '$server' will come

Below code will come
-------------------
ansiblePlaybook credentialsId: 'tomcatcreds', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', limit: '$server = prod', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''


before executing pipeline, undeploy application from tomcat by going to browser of workernodes
pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/java-project-maven-new.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('SonarQube Analysis') {
            steps {
                withSonarQubeEnv('SonarQube') {
                    sh 'mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar'
                }
            }
        }

        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Run Ansible Playbook') {
            steps {
                ansiblePlaybook credentialsId: 'linuxcreds', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''
            }
        }
    }
}



run the pipeline

update the code in GitHub --> and click on build now

How to add PARAMETERS - no need to give host subset use parameters , host : all in pipeline
------------------------------------------------------
The Project is parameterized
Name: server
choices parameter:  -- pass only single choice
dev
test
prod

if string parameter -- multiple choices if you want to pass
Name: server

save

run the build with parameters  -- for string parameters give dev, test

In below pipeline code changed to limit: '$server' , previously it was limit: '$server = prod'


                ansiblePlaybook credentialsId: 'tomcatcreds', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', limit: '$server', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''
===============================================================================================================================================================================================================================
                                                                                                 **DOCKER**
Docker Installation
-------------------

-- yum install docker -y

-- systemctl start docker

-- systemctl status docker

-- docker -v


Commands
--------

-- docker images        [ To show list of images ]

-- docker ps             [To Show list of Containers]

There are two ways to get images

   1. Create image from Dockerfile

   2. Pull the images from dockerhub


-- docker pull amazonlinux    [ To get image amazonlinux or ubuntu from dockerhub ]

-- docker pull ubuntu           [ To get image ubuntu from dockerhub ]

-- docker images



A Docker image is made up of multiple layers, which are stacked on top of each other. Each layer represents a set of file changes (like adding, modifying, or deleting files) and helps in optimizing storage and reusability.


Now, lets try creating a container using this images. Below command will directly login to contains as we are using -it

-- docker run -it --name amazonlinuxcontainer amazonlinux   [ To create container , -it = interactive(go inside container), amazonlinuxcontainer is a container name , amazonlinux is image ]
      -- ls
      -- yum install git maven tree httpd -y
      -- check the version
      -- touch file{1..10}

      -- ctrl p q   [ To exit from container]

If you dont want to login to container and just want to create a container and run in detach mode use -d

-- docker run -it -d --name ubuntucontainer ubuntu

-- docker ps -a  [To list all running and stopped containers]

-- docker ps    [ To list only running containers ]

If you want to access the container

-- docker attach ubuntucontainer
   apt update
   apt install nginx -y
 
      -- ctrl p q

If you want to run direct commands on container use exec

-- docker exec -it 1275f407dc04 /bin/bash

        -- apt update -y
        -- apt install git -y
        -- apt install apache2 -y
        -- service apache2 start
        -- service apache2 status

        -- ctrl p q

Docker attach and exec will help to access the container

-- docker attach 1275f407dc04

       -- ctrl p q

-- docker ps -a

docker attach
-------------
is used to attach your terminal to the standard input, output, and error streams of the main process running inside a container.
It allows you to interact with the container's main process as if you were running it directly in your terminal.

docker exec
-----------

is used to run a new command in an already running container.
This command allows you to start additional processes inside the container independently of the main process.

docker attach interacts with the container's main process (PID 1).
docker exec starts a new, separate process inside the container.

EXEC is preferred

Summary
-------

docker images
docker pull ubuntu
docker run -it --name ubuntucontainer ubuntu
docker pull amazonlinux
docker run -it -d --name amazoncontainer amazonlinux
docker exec -it 1275f407dc04 /bin/bash
docker attach 1275f407dc04

Few container commands
----------------------

docker search ubuntu        : to search images
docker ps -a            : to list all containers
docker ps             : to list runnings containers
docker stop testcontainer    : to stop the container , it will be in exit state
docker start testcontainer    : to start the container
docker pause testcontainer    : to pause the container,it will be in pause state. you cannot connect to this container,
          try docker exec -it contid /bin/bash
docker unpause cont_name        : to unpause container
docker attach testcontainer    : to go inside the container
docker inspect testcontainer    : to get the complete info container
docker kill testcontainer       : to kill the container or forcibly stop the container
docker rm testcontainer         : TO delete the container

docker stop containerid/containername

STOP: will wait to finish all process running inside container
KILL: wont wait to finish all process running inside container

=============================================================================================================

TASK: Pull ubuntu image and install apache and mysql server software's
------------

docker images
docker pull ubuntu
docker run -it --name ubuntucontainer ubuntu
  -- apt update -y
  -- apt install apache2 python3 mysql-server -y
  -- ctrl p q

Now we have container with name ubuntucontainer with apache2 and mysql server, lets create our image with this container ubuntucontainer
This command is used to create a image what ever data we have in the container

-- docker commit ubuntucontainer myimagewithapachenmysql

-- docker images

If you want to create a new container with this image myimagewithapachenmysql

-- docker run -it -d --name mycontwithapachemysql myimagewithapachenmysql

-- docker attach mycontwithapachemysql          [Access the container and check the version]
        -- mysql --version

ctrl pq --> after exit, container is still running
ctrl d --> container will  be stopped if you use ctl d

docker ps -a  [See container is in exit state]
docker start cont-name
docker ps -a

What we did?

Ubuntu image from dockerhub -> create a container -> Take image(custom image) -> From this images create containers again
                                 install softwares     this image
                               contains software's
manually downloaded  --> Manually created container and installed

This is not a good practise , this is manual

Clean up
---------

docker ps -a

Delete all containers
----------------------

docker ps -aq  [List only container-ids]

docker kill $(docker ps -aq)  

docker ps -a   [ All containers are in exit state ]

docker rm $(docker ps -aq)

docker ps -a

Delete all the images
---------------------

docker images -q

docker rmi -f $(docker images -q)

docker images

===========================
DAY 2
===========================================================================================================================================

If you want to automate all these use Dockerfile

DOCKERFILE
----------

It is used to automate image creation.
Inside Dockerfile we use components to do our works.
Components will be on Capital Letter.
In Dockerfile D will be capital.
We can create image directly without container help
To create image from Dockerfile we need to build it.

COMPONENTS:
----------

FROM        : used to get base image
RUN        : used to run linux commands (During image creation)
CMD        : used to run linux commands (After container creation)
ENTRYPOINT    : high priority than cmd
COPY        : to copy local files to container
ADD        : to copy internet files to container
WORKDIR        : to open req directory
LABEL        : to add labels for docker images
ENV        : to set env variables (inside container)
ARGS        : to pass env variables (outside containers)
EXPOSE        : to give port number

Dockerfile --> Docker Build -->  Image --> Container
docker rm -f [container]        : Forcefully remove even if it is running
docker logs [container]        : View logs for a running container:
docker top  [container]        : Show running processes in a container:
docker stats [container]    : View live resource usage statistics for a container:
docker diff [container]        : Show changes to files or directories on the filesystem:

docker cp [file-path] CONTAINER:[path] : Copy a local file to a directory in a container:
         
             docker cp hello.txt cont1:/tmp

docker save IMAGE > IMAGE.tar   : Save an Image to tar file

             docker save ubuntu > ubuntu.tar

docker load -i IMAGE.tar    : Load an image from tar file

              docker load -i ubuntu.tar

docker history IMAGE        : Shows image history

docker image prune        : Delete unused images
Example 1
=========
vi Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install git maven tree apache2 -y
RUN touch file1

-- docker build -t reyaz:v1 .    [  . represent current directory where we have dockerfile ]

-- docker run -it --name cont1 reyaz:v1   [ You will be now in container and see versions of softwares installed ]

   -- ctrl pq

Example 2
===========
RUN will execute while image creation
CMD will exeute after container creation

vi Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install git maven tree apache2 -y
RUN touch file1
RUN apt install python3 -y
CMD apt install mysql-server -y

Note already first few lines are execute, it will not execute again, it will run last lines

CMD will not executed, because it will execute after container creation

-- docker build -t reyaz:v2 .

-- docker run -it --name cont2 reyaz:v2   [dont use -d here for now to see the installation]

--> now mysql-server is now installing because we used CMD command


Example 3
========
COPY --> copy local files to container
ADD --> copy internet files to container

touch index.html

vi Dockerfile

FROM ubuntu
COPY index.html /tmp
ADD https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.35/bin/apache-tomcat-10.1.35.tar.gz /tmp


-- docker build -t reyaz:v3 .
-- docker run -it --name cont3 reyaz:v3

now you are in container
cd /tmp
ls


Example 4
========
WORKDIR --> by default , when you are in container it will be in /, if you want to have a default path WORKDIR will use
LABEL --> just like a tag , we are labeling

vi Dockerfile

FROM ubuntu
COPY index.html /tmp
ADD https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.35/bin/apache-tomcat-10.1.35.tar.gz /tmp
WORKDIR /tmp
LABEL author Reyaz


-- docker build -t reyaz:v4 .
-- docker run -it --name cont4 reyaz:v4

--> now see the path is /tmp
--> to see the entire info about container

-- docker inspect cont4
-- docker inspect cont4 | grep Reyaz



Example 5
==========

vi Dockerfile

FROM ubuntu
ENV course devops
ENV trainer Reyaz
EXPOSE 8080

docker build -t reyaz:v5 .
docker run -it --name cont5 reyaz:v5

echo $course
echo $trainer

ctrl pq

docker ps -a  [ Now you can see the port number for cont5 ]


First Dockerfile Task for application Deployment
===============================================

Code - GitHub --> DockerFile --> Build Image --> Create Container --> Access application

yum install git -y

git clone https://github.com/ReyazShaik/website.git

--> dont go inside website folder, create Dockerfile outside website

vi Dockerfile

FROM ubuntu
RUN apt update
RUN apt install apache2 -y
RUN apt install apache2-utils -y
RUN apt clean
COPY website/ /var/www/html/
RUN service apache2 restart
EXPOSE 80
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


-D FOREGROUND → Runs Apache in the foreground mode, meaning:
Apache does not daemonize (does not run in the background).
Logs are printed to the container's stdout/stderr, making it visible with docker logs.
The process stays alive, preventing the container from exiting immediately.
Containers stop when the main process exits. Running Apache in foreground mode ensures the container remains active.


-- docker build -t firstproject:v1 .

-- docker run -itd --name newwebcont1 -p 80:80 firstproject:v1

In AWS SG , allow All traffic

http://ip

Note: docker rmi firstimage:v1 [if required]

Another example
----------------
Tomcat installation on Container
-------------------------------

vi Dockerfile

# Use the official Tomcat image as the base
FROM tomcat:latest

# Set environment variables (optional)
ENV CATALINA_HOME /usr/local/tomcat
ENV PATH $CATALINA_HOME/bin:$PATH

# Remove default webapps (optional: keeps Tomcat clean)
RUN rm -rf $CATALINA_HOME/webapps/*

# Copy your application WAR file into the Tomcat webapps directory
COPY jenkins.war $CATALINA_HOME/webapps/jenkins.war

# Expose Tomcat's default HTTP port
EXPOSE 8080

# Start Tomcat
CMD ["catalina.sh", "run"]


-- docker build -t tomcatimage:v1 .

-- docker run -itd --name tomcont -p 8080:8080 tomcatimage:v1
                                     host:container

docker exec -it 1275f407dc04 /bin/bash

http://ip:8080
======================================================================================================================


VOLUME:
--------
Docker volumes are a way to persist data generated by and used by Docker containers.

Volumes are stored on the host filesystem outside of the container's filesystem, which means they are not deleted when the container stops.

This makes them ideal for managing persistent data.

In Short
-------

If you want to store the data in container use volumes
volume is just a folder in container
containers use host resources (cpu, ram, )
volume can be shared to multiple containers.
At a time we can share single volume.
data inside volume will store on local.

Method 1 -- creating volume from Dockerfile
----------------------------------------

vi Dockerfile

FROM ubuntu
VOLUME ["volume1"]


-- docker build -t reyaz:v1 .

-- docker run -it --name cont1 reyaz:v1

ls --> you will be in container automatically and do ls -- you can see volume1

cd volume1

touch file{1..5}

--> what ever data you have created in container in volume1 the same data will be in docker host and vice versa
it is bidirectional -- if you create data in docker host it will be available in container also

comeout of the container ctrl pq and go to the below path

/var/lib/docker/volumes -- is the place where volume data is stored

touch python{1..5}  --> create some sample files, it should be visible inside container

-- docker attach cont1

if stopped docker start cont1

docker attach cont1

cd volume1

ls

------------------ SHARE THE VOLUME ------------------

If you want to get the same volume to another container

-- docker run -it --name cont2 --volumes-from cont1 ubuntu

Optional
docker run -it --name cont2 --volumes-from cont1 --privileged=true  ubuntu

ls

cd volume1


come out of the container ctrl pq

docker ps -a

--> Ex: create a new container with same volume : just an example

docker run -it --name cont3 --volumes-from cont2 ubuntu


Method 2 - Creating volume from the command directly
-------

docker images

docker run -it --name cont4 -v /volume2 ubuntu

cd volume2/

touch python{1..5}

ctrl pq

--> now share the volume from cont4 to cont5

docker run -it --name cont5 --volumes-from cont4 ubuntu
ls
cd volume2
now you can see the same data
create files here , we can see the same data in cont4 also
touch python{6..10}

ctrl pq

docker attach cont4  --> connect to cont4 to see the data , same data available
ls

--> you can see the same data also in docker host /var/lib/docker/volumes/



Method 3 -- Volume Mounting  
--------
Now you are in docker host

docker volume ls

docker volume create volume3

docker volume inspect volume3

cd /var/lib/docker/volumes/volume3/_data  -- in docker host

touch html{1..5}

docker run -it --name cont6 --mount source=volume3,destination=/volume3 ubuntu

now you are in container 6 , cd /volume3 and ls

now create files here , it will be available on docker host

if you delete, everywhere it is deleted


4. SHARING LOCAL FILES to Container
----------------------------------

be in docker host
touch reyaz{1..10}
docker volume inspect volume3
cp * /var/lib/docker/volumes/volume3/_data
docker attach cont6
cd volume3


Use AWS EBS Volumes to docker containers
---------------------------------------

Create 20GB EBS volume in AWS Console

Attach to the EC2 instance as /dev/xvdb

lsblk
sudo mkfs -t ext4 /dev/xvdb        ----- format the disk before use
sudo mkdir /mnt/ebs-volume
sudo mount /dev/xvdb /mnt/ebs-volume
docker images
docker run -it  --name my-container1 -v /mnt/ebs-volume:/mynewebsvolume reyaz:v3
  -- ls
  -- cd mynewebsvolume
  -- touch hello

-- create a another container with new name from before container

docker run -it --name mynewcont2 --volumes-from my-container1  ubuntu

  -- ls
  -- cd mynewebsvolume

you see the same data in all containers



DOCKER SYSTEM COMMANDS:
----------------------
to know the docker components resource utilization
docker system df
docker system df -v

docker ps -a

docker inspect cont5

docker inspect cont5 | grep volume -i

RESOURCE MANAGEMENT:
--------------------
By default, docker containers will not have any limits for the resources like cpu ram and memory so we need to restrict resource use for container.

By default docker containers will use host resources(cpu, ram, rom)
Resource limits of docker container should not exceed the docker host limits.

docker stats  --> to check live cpu and memory

docker run -it --name cont7 --cpus="0.1" --memory="300mb" ubuntu
docker update cont7 --cpus="0.7" --memory="300mb"

JENKINS SETUP BY DOCKER:
docker run -it -d --name jenkins -p 8080:8080 jenkins/jenkins:lts
-- docker exec -it jenkins /bin/bash


======== Delete all container ============
docker kill $(docker ps -aq)  

docker rm $(docker ps -aq)

docker ps -a

======== Delete images =====================
docker rmi -f $(docker images -q)

=========================================

======================================================
Docker Compose
======================================================

Before docker compose how we use to do?



Lets Use HDFC Bank example : internetbanking, MobileBanking, Insurance and Loans

We need to create for all modules a separate containers with some webserver to run, for this write Dockerfile

Create a directory called internetbanking , MobileBanking, Insurance and Loans

In Every directory have index.html and Dockerfile.

Create a image and container with that index.html

how?

yum install -y git

git clone https://github.com/ReyazShaik/hdfcwebsite.git

-- cd hdfcwebsite

-- cd internetbanking

Here already index.html is available, just create a Dockerfile

vi Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

-- docker build -t internetbanking:v1 .

-- docker run -itd --name cont1 -p 81:80 internetbanking:v1

-- docker ps -a

http://instanceip:81

-- docker logs cont1 [If required]

-----------

MobileBanking
------------

cd ..

cd mobilebanking

vi Dockerfile


FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


-- docker build -t mobilebanking:v1 .

-- docker run -itd --name cont2 -p 82:80 mobilebanking:v1

http://instanceip:82

------------

Insurance
---------
cd ..
cd insurance

vi Dockerfile


FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t insurance:v1 .

docker run -itd --name cont3 -p 83:80 insurance:v1

http://instanceip:83

docker ps -a

docker images
--------------------------------------------------

Loans
---------
cd ..
cd loans

vi Dockerfile


FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t loan:v1 .

docker run -itd --name cont4 -p 83:80 loan:v1

http://instanceip:83

docker ps -a

docker images


What ever we did is not good process and not used in realtime - use docker compose


Docker Compose
-------------

Docker Compose is a tool that allows you to define and run multi-container Docker applications using a YAML file (docker-compose.yml). Instead of running multiple docker run commands manually, you can define everything in one file and start your services with a single command.

Docker Compose is a tool for defining and running multi-container Docker applications.

With Docker Compose, you can use a YAML file to define the services, networks, and volumes required for your application, and then bring up the entire stack with a single command.

It is a tool used to create multiple containers.

It will work on single host.

we can create, stop, start and delete all containers together

we can write a file called docker-compose which will be on yaml format.

Docker Compose Installation
---------------------------

vi dockercompose.sh

sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version

sh dockercompose.sh


First, lets kill all containers

docker kill $(docker ps -a -q)

docker rm $(docker ps -a -q)


vi docker-compose.yml

version: '3.8'
services:
  internetbanking:
    image: internetbanking:v1
    ports:
      - "81:80"
  mobilebanking:
    image: mobilebanking:v1
    ports:
      - "82:80"
  insurance:
    image: insurance:v1
    ports:
      - "83:80"
  loan:
    image: loans:v1
    ports:
      - "84:80"

docker ps -a ---> No containers

docker-compose up -d  --> detach mode

docker-compose ps

refresh http://IP:81 and all in browser

docker-compose stop
docker-compose start
docker-compose kill -- to stop, use stop or kill
docker-compose start
docker-compose

TO remove containers first kill and rm

docker-compose kill
docker-compose rm

To create container again

docker-compose up -d
docker-compose ps
docker-compose pause
docker-compose unpause

docker-compose logs

docker-compose images --> these images are managed by docker-images , docker images shows managed by docker

docker-compose top

docker-compose restart

docker-compose scale loan=10  --> it will create but port conflict will come , if you want to scale we have dockerswarm

docker-compose down --> it will stop and kill automatically


CHANGING THE DEFULT FILE:

by default the docker-compose will support the following names
docker-compose.yml, docker-compose.yaml, compose.yml, compose.yaml

mv docker-compose.yml reyaz.yml
docker-compose up -d    : throws an error

docker-compose -f reyaz.yml up -d
docker-compose -f reyaz.yml ps
docker-compose -f reyaz.yml down
Load Balancing Project with Docker-Compose
==========================================
📌 Nginx Load Balancer - Dockerfile Setup
--------------------------------------


                                   --- Container1- backend1
User --------> NGinx Load Balancer --- Container2- backend2
                                   --- Container3- backend3


This Dockerfile will set up Nginx as a Load Balancer for multiple backend services inside Docker.

Step 1: Create a Dockerfile
Step 2: Create nginx.conf locally and copy to containers through docker-compose
Step 3: Create docker-compose file to create containers in one shot

1️⃣ Create the Dockerfile
---------------------

vi Dockerfile
# Use the official Nginx image
FROM nginx:latest

# Remove default config and copy custom nginx.conf
RUN rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/conf.d/

# Expose port 80
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]



is used to keep Nginx running in the foreground when running inside a Docker container.
🔹 Why is daemon off; Needed?
By default, Nginx runs as a background (daemon) process.
In a Docker container, the main process must stay in the foreground.
If Nginx runs as a daemon, Docker thinks the container has exited.
Setting "daemon off;" prevents it from running in the background, keeping the container alive



2️⃣ Create the Nginx Load Balancer Config (nginx.conf)
------------------------------------------

vi nginx.conf

# Define the upstream backend servers
upstream backend {
    server backend1:5000;
    server backend2:5001;
    server backend3:5002;
}

server {
    listen 80;

    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

3️⃣ Create a docker-compose.yml (Optional)
-------------------------------------

vi docker-compose.yml

version: '3'
services:
  nginx:
    build: .
    container_name: nginx-lb
    ports:
      - "80:80"
    depends_on:
      - backend1
      - backend2
      - backend3

  backend1:
    image: ghcr.io/benc-uk/python-demoapp
    container_name: backend1
    expose:
      - "5000"

  backend2:
    image: ghcr.io/benc-uk/python-demoapp
    container_name: backend2
    expose:
      - "5001"

  backend3:
    image: ghcr.io/benc-uk/python-demoapp
    container_name: backend3
    expose:
      - "5002"


docker-compose up -d
docker-compose ps

(or)

docker build -t nginx-lb .
docker run -d -p 80:80 --name nginx-lb nginx-lb


✅ Explanation:

Nginx Load Balancer (nginx-lb) forwards traffic to backend1, backend2, and backend3.
Each backend service listens on different ports.

================================================================
📌 Docker Load Balancing with Python Application using Nginx
===============================================================

✅ Deploy multiple Python Flask applications as backend services.
✅ Use Nginx as a load balancer to distribute traffic.
✅ Use Docker Compose for easy deployment.


1️⃣ Create the Python Flask App (app.py)
==================================
vi app.py

from flask import Flask
import socket

app = Flask(__name__)

@app.route('/')
def hello():
    return f"Hello from {socket.gethostname()}!"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

2️⃣ Create requirements.txt
====================

vi requirements.txt
flask

3️⃣ Create the Python App Dockerfile
==============================

vi Dockerfile

# Use Python base image
FROM python:3.9

# Set working directory
WORKDIR /app

# Copy and install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy the application code
COPY . .

# Expose the port Flask runs on
EXPOSE 5000

# Start the Flask app
CMD ["python", "app.py"]


4️⃣ Create Nginx Load Balancer Config (nginx.conf)
=======================================
vi nginx.conf

# Define the upstream backend servers
upstream backend {
    server backend1:5000;
    server backend2:5000;
    server backend3:5000;
}

server {
    listen 80;

    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}


5️⃣ Create docker-compose.yml
=========================

vi docker-compose.yml

version: '3'

services:
  nginx:
    image: nginx:latest
    container_name: nginx-lb
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - backend1
      - backend2
      - backend3

  backend1:
    build: .
    container_name: backend1
    expose:
      - "5000"

  backend2:
    build: .
    container_name: backend2
    expose:
      - "5000"

  backend3:
    build: .
    container_name: backend3
    expose:
      - "5000"


docker-compose up -d --build

docker ps -a

http://IP



=======================
DOCKERHUB
=======================


docker images

instead storing images in local docker host , push to dockerhub

go to docker and signup

[Lets first tag our image]

-- docker tag internetbanking:v1 trainerreyaz/ib-image

-- docker push trainerreyaz/ib-image --> it will fail, no authentication

-- docker login

    username:
    password:

-- docker push trainerreyaz/ib-image

**** Now see the images in dockerhub
*** One time docker login is enough

docker tag mobilebanking:v1 trainerreyaz/mb-image
docker push trainerreyaz/mb-image

docker tag insurance:v1 trainerreyaz/insurance-image
docker push trainerreyaz/insurance-image

docker tag loans:v1 trainerreyaz/loans-image
docker push trainerreyaz/loans-image

--- Now all your images are in dockerhub, just delete the images from local docker host and you can get them from dockerhub again

-- docker rmi -f $(docker images -aq)

-- docker pull trainerreyaz/ib-image
-- docker pull trainerreyaz/mb-image
-- docker pull trainerreyaz/insurance-image
-- docker pull trainerreyaz/loans-image

Now pull images from dockerhub

-- docker pull trainerreyaz/ib-image:latest

-- docker run -itd --name cont1 -p 80:80 trainerreyaz/ib-image:latest
=============
DOCKER SWARM - High Availability
=============

If container deleted we can re-create manually or using docker-compose, by if the docker host is terminated ?

Docker swarm uses multiple host machines

Its an orchestration tool for containers.
It is cluster used to manage containers.
Cluster means group of servers.
Cluster will have manager/Leader and worker nodes.
Multiple servers will have same container.
If we can access container from one server we can access from another server.
Manager node will distribute containers worker node.
Worker node will maintain containers.    
port : 2377
Worker nodes join


SETUP:
1. CREATE 3 SERVERS AND INSTALL DOCKER
2. SET HOSTNAMES (hostnamectl set-hostname manager/worker1/worker2)
3. GO TO MANAGER NODE (docker swarm init) -- > copy token to all nodes
4. docker node ls


Install docker in master, node1 and node2

yum install -y docker

systemctl start docker


--> docker swarm init  -- [ In manager node ]

It will generate a token and command, run below command in all worker nodes

In Node1
--------
docker swarm join --token SWMTKN-1-5knz8d9ypaqvd6s2sm1mlyy8d9bznuwemwm18dbz5p7jfxplad-b35y8xpsgq2xgza04913lv91z 172.31.27.158:2377

In Node2
--------
docker swarm join --token SWMTKN-1-5knz8d9ypaqvd6s2sm1mlyy8d9bznuwemwm18dbz5p7jfxplad-b35y8xpsgq2xgza04913lv91z 172.31.27.158:2377

Now create a container in manager node, it should create in all worker nodes

On Manager
----------

docker node ls    

-- docker run -itd --name contl -p 81:80 trainerreyaz/ib-image     [use dockerhub image]

But the above command will create a container only in manager as we used just docker command. If we want to have container in all nodes use docker service



-- docker service create --name internetbanking --replicas 3 -p 81:80 trainerreyaz/ib-image:latest

-- docker ps -a

Now see containers in all worker nodes, go to each workernodes and do ps -a

docker ps -a

Access application on all 3 servers http://IP:81  , http://workernodeip:81


-- docker service ls

-- docker service ps internetbanking

-- docker service logs internetbanking

-- docker service inspect internetbanking

-- docker service scale internetbanking=10    --- it will scale out , equally distributed to all worker nodes including manager

-- docker service scale internetbanking=3  -- it will scale down, it uses LIFO,

give example again
docker service scale internetbanking=15
docker service scale internetbanking=5

-- docker service rollback internetbanking  --- going back to how many containers was there before
it will show 15
again rollback it will show 5

scenario: if we delete the container in worker node docker kill containerid , docker rm containerid and check the http://IP:81 it still work
because, nodes has self healing , if you do docker ps -a  , you can see a new container created automatically

delete the container again, it will re-create it automatically
docker stop contids
docker rm contids

docker ps -a

--> now in manager node delete the service which contain containers

-- docker service ls

-- docker service rm internetbanking

-- docker service ls

if you want to re-create  

docker service create --name loan --replicas 3 -p 82:80 trainerreyaz/loan:latest

docker service ps loan

docker service create --name mobilebanking --replicas 3 -p 83:80 trainerreyaz/mb-image:latest


********** No auto-scaling and load balancer in DockerSwarm that why we use Kubernetes

Note: If entire swarm is not working
docker swarm init --force-new-cluster

===================================================================
Example:

docker service create --name jenkins --replicas 3 -p 8080:8080 jenkins/jenkins:lts   ---> create a new service called Jenkins and setup Jenkins containers in all nodes

jenkins/jenkins:lts -> username/image:tag


=============================================================================================================================================

Cluster Activities
=================

docker swarm leave  --> this will leave the swarm, do this in worker node
if you want to join back -> use the same join command to join back to swarm (up arrow or history command) or

docker swarm join-token manager --> this command will generate a token to join
docker swarm join-token worker --> this command will generate a token for worker nodes to join

docker node ls

docker node rm nodeid --> it will remove the down node

===================
Docker Networking
===================

Docker networking are used to make communication between the multiple containers that are running on same or different docker hosts

Different Docker Networks
 --> Bridge Network = Same hosts (if 2 containers wants to communicate with each other in same docker host)
 --> Overlay Network = different hosts (2 containers want to communicate with each other in different docker hosts)
 --> Host Network = if you want to use HOST meaning EC2 networking
 --> None Network = if you dont want to expose any application


-- docker node ls

-- docker network ls

Explain Name and Driver as per output

-- docker network create reyaz

-- docker network ls


-- docker run -itd --name cont1 -p 81:80 nginx:latest
-- docker run -itd --name cont2 -p 82:80 nginx:latest

-- docker inspect cont1
-- docker inspect cont2

BY default networking part it say Bridge by default

Lets change the network for cont1 to Reyaz network that we created before

-- docker network connect reyaz cont1  
-- docker network connect reyaz cont2

docker network inspect Reyaz  --> this will show cont1 and cont2 in same network

Lets see if 2 containers can communicate each other or not

Get the IP address of cont1
 
 -- docker inspect cont1


first connect to cont2

-- docker exec -it cont2 /bin/bash

now you are in cont2

ping IP of cont1 --> ping will not work

apt update
apt install iputils-ping -y
ping cont1ip

ctrl p q

docker network disconnect Reyaz cont2 --> if you want to remove cont2 from Reyaz network

docker network inspect Reyaz  --> now you cannot see cont2 in that json

docker network prune --> this will delete unused networks

docker system prune --> also delete the networks

=========================
Docker Python FLask Project
=========================

yum install -y docker

yum install -y git

git clone https://github.com/ReyazShaik/docker-python-flask-project.git

cd docker-python-flask-project

Show files

vi Dockerfile

FROM python:3.6
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
EXPOSE 5000
CMD ["python", "app.py"]

Create image
-----------

docker build -t pythonapp .

Create Container
----------------
docker run -d -p 5000:5000 pythonapp


http://ip:5000

======================================================================================================================================

====================================================================

DOCKER - PROJECT - 

Manager  - t2.micro
WorkerNode1 -micro
WorkerNode2 - micro



1. Launch 3 Amazon Linux 3 instances 1. DockerSwarm 2. Worker Node 1 , 3. Worker Node 2
2. Open MobaXterm and connect to all machines
3. Go to multi-exec and install docker and start
   yum install -y docker
   systemctl start docker

set hostnames for all

hostnamectl set-hostname manager  --> In manager node
sudo -i
hostnamectl set-hostname worker1
sudo -i
hostnamectl set-hostname worker2
sudo -i

In Manager Node

docker swarm init

IN worker node1

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Worker Node2

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Manager node

docker node ls

--> install Jenkins on master node
vi Jenkins.sh
copy paste the script

sh Jenkins.sh -- selection 2

--> if doesn't work see if Jenkins containers is already there as we did docker swarm -- for that docker service rm Jenkins

http://IP:8080 and ready Jenkins

Run the below command to work docker in Jenkins pipeline otherwise pipeline will not work
-----------------------

RUn in docker host - manager

chmod 777 /var/run/docker.sock
systemctl daemon-reload
systemctl restart docker.service


Now lets create a pipeline in Jenkins
-------------------------------------

Get code from GitHub --> as dockerfile exits in the repo, we can build using pipeline
But we dont have only 1 image, we need to create multiple images like internetbanking, Mobilebanking, loan and insurance
For this, lets have image as a variable not a direct entry in pipeline as docker build -t internetbanking:v1 , instead use docker build -t $image

create image as a parameter

In Pipeline select : This project is parameterized --> Add Parameter --> choice parameter
Name:  image
choices :
internetbanking:v1
mobilebanking:v1
insurance:v1
loans:v1


pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/dockerproject.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'docker build -t $image .'
            }
        }
     }
}



Run the Pipeline

in Docker host / manager , see image got created

docker images --> you should see internetbanking:v1 image

----
Now lets tag
----

As we need to create multiple repo and images, we need to create a parameter for repo , go up in same pipeline --> Add parameters

Name: repo  -- keep always small letters because in pipeline also $repo
choices :
trainerreyaz/ib-image
trainerreyaz/mb-image
trainerreyaz/insurance-image
trainerreyaz/loans-image


IN pipeline add

 stage('tagging') {
            steps {
                sh 'docker tag $image $repo'
            }
        }



Now in docker host --> docker images -> you can see now new image with new tag

Now images are stored locally , now push to dockerhub , for this we need to authenticate to dockerhub

In Jenkins, we have local and global variables

Manage Jenkins --> System --> Environment variables
Name: password
value: give here dockerhub password


        stage('push') {
            steps {
                sh 'docker login -u trainerreyaz -p $password'
                sh 'docker push $repo'
            }
        }

Now build the pipeline with internetbanking --> image will be stored in dockerhub --> login to dockerhub and show the image

Now, change index.html in GitHub from InternetBanking to MobileBanking --> Build Pipeline --> MobileBanking --> trainerreyaz/mb-image
Now, change index.html in GitHub from MobileBanking to Insurance --> Build Pipeline --> Insurance --> trainerreyaz/insurance-image
Now, change index.html in GitHub from Insurance to Loans --> Build Pipeline --> Loans --> trainerreyaz/Loans-image

Now a new repos and image will be created in dockerhub

Note: In realtime we dont use webhook because if developer push automatically pipeline will run which is danger

Till now we created images lets now do deployment

=== NOw Deployment ===

Lets use docker swarm to have HA , for this we need containers, lets use dockercompose
And docker compose is use to create multiple containers but in single host but we need containers in all hosts(workernodes)
Lets use docker stack

show docker-compose.yml in GitHub

version: '3.8'
services:
  internetbanking:
    image: trainerreyaz/ib-image:latest
    ports:
      - "81:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/internetbanking
  mobilebanking:
    image: trainerreyaz/mb-image:latest
    ports:
      - "82:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/mobilebanking
  insurance:
    image: trainerreyaz/insurance-image:latest
    ports:
      - "83:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/insurance
  loan:
    image: trainerreyaz/loan-image:latest
    ports:
      - "84:80"
    deploy:
      replicas: 3
    volumes:
      - /var/lib/loan



And add this below to pipeline , and bank is application name

 stage('deploy') {
            steps {
                sh 'docker stack deploy -c docker-compose.yml bank'
            }
        }

-C - compose file

Now build the pipeline with insurance image and insurance repo

Some docker stack commands
=====================

docker service ls
docker service rm stackname
docker stack ls
docker stack ps bank


pipeline {
    agent any
   
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/ReyazShaik/dockerproject.git&#39;
            }
        }
        stage('build') {
            steps {
                sh 'docker build -t $image .'
            }
        }
        stage('tag') {
            steps {
                sh 'docker tag $image $repo'
            }
        }
        stage('push') {
            steps {
                sh 'docker login -u trainerreyaz -p $password'
                sh 'docker push $repo'
            }
        }
        stage('deploy') {
            steps {
                sh 'docker stack deploy bank -c docker-compose.yml'
            }
        }
    }
}



PORTAINER - GUI for managing containers
=======================================


Portainer Using SWARM
===========

Must have swarm mode and all ports enable with docker engine

In Manager Node
----------------

docker swarm init

IN worker node1
----------------

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Worker Node2
---------------

docker swarm join --token SWMTKN-1-52jo6saicq2fg67gicpy2t77t1xjhaeqb41520jfux4y77rib7-ah49urfxgvcgmdg09yehhkbbd 172.31.21.171:2377

In Manager node
---------------

docker node ls


Download portainer yaml
----------------------

curl -L https://downloads.portainer.io/ce2-16/portainer-agent-stack.yml -o portainer-agent-stack.yml

docker stack deploy -c portainer-agent-stack.yml portainer

docker ps

public-ip of swarm master:9443

https://13.233.198.125:9443
http://13.233.198.125:900

Environment --> Primary


Portainer on Standalone docker
-------------------------------



docker volume create portainer_data

docker run -d -p 8000:8000 -p 9443:9443 --name portainer \
--restart=always \
-v /var/run/docker.sock:/var/run/docker.sock \
-v portainer_data:/data \
portainer/portainer-ce:2.9.3

docker ps

https://localhost:9443   [use https forcely in browser]

Following command to start a Portainer Agent container on the system you want to add:

docker run -d -p 9001:9001 --name portainer_agent --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/docker/volumes:/var/lib/docker/volumes portainer/agent:2.9.3


Add environment --> Name = Docker, Environment URL = EC2 instance Public IP:9001

Local --> Add container --> Name = Apache --> image= httpd,  publish a new network port --> host=8080, container =80 --> deploy


explore application

https://phoenixnap.com/kb/docker-portainer-install





TO setup MySQL
==============

docker run -d --name mysql-db \
  -e MYSQL_ROOT_PASSWORD=my-secret-pw \
  -e MYSQL_DATABASE=mydb \
  -e MYSQL_USER=myuser \
  -e MYSQL_PASSWORD=mypassword \
  -p 3306:3306 \
  -v mysql_data:/var/lib/mysql \
  --restart unless-stopped \
  mysql:latest


docker ps


docker logs mysql-db

docker exec -it mysql-db mysql -u root -p

pwd= my-secret-pw


create database test123;
show databases;
drop databases test123;
Docker Swarm and Kubernetes are both container orchestration platforms, but they have different strengths and use cases

Docker Swarm is easier to use and is better for smaller applications, while Kubernetes is more robust and better for large-scale applications.

Docker Swarm is Docker’s native clustering and orchestration tool, designed to manage and scale containerized applications.

While it provides simplicity and integration with Docker, it has several limitations compared to other orchestration solutions like Kubernetes.



Limitations
-----------

Docker Swarm is generally considered less scalable than Kubernetes. While it can handle clusters with a reasonable number of nodes and services, Kubernetes is designed for larger-scale deployments and can manage thousands of nodes and pods.

Swarm provides basic load balancing and routing but lacks the advanced capabilities like provided by K8S


Kubernetes offers more granular security controls, including Role-Based Access Control (RBAC), network policies, and security policies. Docker Swarm’s security features are less comprehensive in comparison.

Docker Swarm has a smaller community and less industry adoption compared to Kubernetes

Docker carries containers as shown in logo
And those are carried by K8S as it is a steering for docker with containers

Docker is used to create containers, while Kubernetes is used to manage them

DockerSwarm                  Kubernetes
===========                  ==========
Cluster                      Cluster
Nodes                 Nodes
Containers             PODS
Applications               Containers
                 Applications


======================================================================================================================================================================================================================================================                              ==================================================================================================================
                                                                                                    **Kubernetes**
                                 

Kubernetes is a powerful open-source platform for automating the deployment, scaling, and operation of application containers.

Kubernetes is a Container Orchestration tool

Orchestration is nothing but Cluster

Kubernetes creates Cluster , deploy and manage clusters. Cluster is combination of 1 master and multiple nodes / Minion

Kubernetes master will not take the load, it will distribute load to Nodes / Slaves

Smallest object that Kubernetes can create is POD. WithIn POD, we have Container

Cluster is a bunch of PODS and POD contains docker containers

By using Kubernetes we form a cluster (group of PODS with docker containers)

K8S schedules, runs and manage isolated PODS

Kubernetes does not understand Containers

Kubernetes can understand only PODS




Features of Kubernetes
---------------------

Orchestration -- Clustering any number of containers on different hardwares / nodes
Autoscaling
Auto Healing -- New containers will create in case of containers crash. Same like Docker Swarm
Load Balancing
Roll Back  - Going to Previous Versions




Here's a brief history of Kubernetes, from its origins to its current state:

Origins and Predecessors
========================

Google's Borg:
-------------

The origins of Kubernetes can be traced back to Google's internal systems, particularly a system called Borg. Borg was a large-scale cluster management system that Google developed internally to manage its vast infrastructure. It allowed Google to deploy, manage, and scale applications across thousands of servers efficiently.

Omega:
-----

After Borg, Google developed Omega, which was an evolution of Borg. Omega introduced a more flexible and modular architecture. It further refined the ideas of scheduling, resource management, and container orchestration.

The Birth of Kubernetes
=======================

2014 - Project Launch:
---------------------

Kubernetes was officially announced by Google in mid-2014. It was initially developed by Google engineers Joe Beda, Brendan Burns, and Craig McLuckie.

Kubernetes was designed based on the lessons learned from Borg and Omega, but it was intended to be open-source and available to the broader community.

Open-Source and Community Involvement:
--------------------------------------

From the start, Kubernetes was designed as an open-source project, allowing developers from outside Google to contribute. It was released under the Apache 2.0 license, making it freely available for anyone to use and modify.


Partnership with the Cloud Native Computing Foundation (CNCF):
--------------------------------------------------------------

In 2015, Kubernetes was donated to the CNCF, which was formed to foster and sustain the development of cloud-native technologies. This move helped Kubernetes gain significant traction in the open-source community and among enterprises.



**************************************************************************************

Kubernetes Architecture
-----------------------

Kubernetes architecture is designed to manage containerized applications across a cluster of machines efficiently.

It follows a master-worker architecture where the master nodes control and manage the worker nodes.

1. Master Nodes  - Control Plane
2. Worker Nodes
3. PODS
4. Networking
5. Volumes
6. Namespaces
7. Services

MW- PNVNS


Master Nodes (Control Plane)
============================

The master node is responsible for managing the Kubernetes cluster.

It coordinates all activities within the cluster, including scheduling, scaling, and maintaining the desired state of the system.

Key Components of the Master Node:
------------------------------------

 1. API Server (kube-apiserver): The receptionist
    ----------------------------------------------

    The API server is the entry point for all REST commands used to control the cluster. communicate with user (takes command execute & give output).  kubectl is the command to communicate with API Server

 2. Scheduler (kube-scheduler): This will take Action
------------------------------------------------------
 
    The scheduler is responsible for placing the pods onto nodes within the cluster based on resource availability and other constraints. This Kube-Scheduler will communicate with the Nodes

 3. Controller Manager (kube-controller-manager):
---------------------------------------------------

    This component runs various controllers that regulate the state of the cluster.Node Controller (which handles node failures), Replication Controller (which maintains the correct number of pods), and more. It just control the k8s objects (n/w, service, Node)

 4. etcd
---------

    etcd is a key-value store used by Kubernetes to store all cluster data, including configuration data, state information, and metadata.  Database of the Cluster. It is a critical component, as it serves as the single source of truth for the cluster’s state.

 5. Cloud Controller Manager(Optional) :
---------------------------------------
 
    This component interacts with the underlying cloud infrastructure. It handles tasks like managing cloud resources (e.g., load balancers, volumes) in public or private cloud environments. It is responsible to make sure that the actual state is same as desired state.

In Short Cut
----------

MASTER:

1. API SERVER: Communicate with user (takes command execute & give op)
2. CONTROLLER: Control the k8s objects (n/w, service, Node)
3. SCHEDULER: Select the worker node to schedule pods (depends on hw of node)
4. ETCD: Database of cluster (stores complete info of a cluster ON KEY-VALUE pair)

ACSE

Worker Nodes
===============

Worker nodes are the machines where the actual workload, in the form of containerized applications, runs.

Each worker node contains several components that allow it to run and manage containers.

Key Components of the Worker Node:
---------------------------------

1. Kubelet:
   -------
The kubelet is an agent that runs on every worker node. It ensures that containers are running in a pod as defined by the Kubernetes API. It communicates with the API server on the master node and manages the lifecycle of the containers on its node.

Kubelet is the only component that will communicate with Master
   
2. Container Runtime:
   ------------------

This is the software responsible for running containers.

The most common runtime is Docker, but Kubernetes also supports others like containerd and CRI-O.

The container runtime pulls container images from a registry, unpacks them, and runs the application inside the containers.

3. Kube-Proxy:
   ----------

Kube-proxy is a network proxy that runs on each worker node. It maintains network rules that allow communication between pods and services within the cluster.  It enables the network routing and load balancing of traffic within the Kubernetes cluster.

Simply Kube-Proxy will Provide IP to the POD

4. PODS:
   -----

Pods are the smallest and simplest Kubernetes objects. A pod represents a single instance of a running process in your cluster and can contain one or more containers.

Pods are ephemeral, meaning they can be created and destroyed as needed, and are the unit of scaling in Kubernetes.


In Short Cut
------------

WORKER NODES:

1. KUBELET : Its an agent in worker node (it will inform all activities to master)
2. KUBEPROXY: It deals with Network (ip, networks, ports)
3. POD: Group of containers (inside pod we have application)

KCKP


Key Networking Concepts:
-----------------------

1. Cluster IP: An internal IP address that is assigned to a service within the cluster.

2. NodePort: Exposes a service on a static port on each node’s IP.

3. LoadBalancer: Provisions a load balancer in supported cloud environments to expose services externally.


Volumes
-------

Kubernetes Volumes provide persistent storage to containers within pods, allowing data to persist across pod restarts.

They can be backed by various storage backends, including local storage, cloud storage like AWS EBS

Namespaces
----------

Namespaces are a way to divide cluster resources between multiple users.

They provide a scope for names, meaning you can have multiple resources with the same name in different namespaces.

Services
--------

Services are used to access our applications which is on the pods using Cluster IP(internal access) or Load Balancer (external access)



K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION

In Kubernetes we need to Create Cluster. Cluster can create in 2 ways:  1. Self Managed 2. Cloud Based

CLUSTER TYPES:
-------------

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)

2. CLOUD-BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKS = GOOGLE KUBERENETS SERVICE


Kubectl is the command line tool for Kubernetes

If we want to execute commands we need to use kubectl.

MINIKUBE:
---------

It is a tool used to setup single node cluster on K8's.

Here Master and worker runs on same machine

It contains API Servers, ETDC database and container runtime

It is used for development, testing, and experimentation purposes on local.

NOTE: But we don't implement this in real-time Prod


Minikube Setup
--------------

Requirement
------------

Launch Ubuntu 24 with t2.small having min 8GB volume

Install minikube and Kubectl

vi minikube.sh

sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256&quot;
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

   

KUBECTL:
--------
kubectl is the CLI which is used to interact with a Kubernetes cluster.

We can create, manage pods, services, deployments, and other resources.

The configuration of kubectl is in the $HOME/.kube directory.

-- minikube status

-- kubectl get nodes

-- kubectl get pods

PODS :
=======

It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.

We can create this pod in two ways,

1. Imperative(command)

2. Declarative (Manifest file) - can be reuse - in real time we go with declarative

By default, one pod has one container, if required we can create, if you create multiple containers in a single pod, all containers inside the pods will share the same volume


1. Imperative(command)
-----------------------

Example:
--------

-- kubectl run pod1 --image nginx    [Creating  pod with name pod1 with image nginx]

-- kubectl get pods/pod/po           [To get the pods , can use pods/po/pods also]

-- kubectl get pod -o wide           [To get details about the pod]

-- kubectl describe pod pod1         [To get more details about the pod]

-- kubectl delete pod pod1           [To delete the pod]

Example 2:
---------

-- kubectl run reyaz --image nginx  [This will create a pod name reyaz , with httpd image ]

-  kubectl get pods

-- kubectl logs reyaz  

-- kubectl exec -it reyaz -- /bin/bash   [to connect to the pod inside]
   cd /usr/share/nginx/html
   cat index.html

-- kubectl delete pod reyaz   [This will delete the pod reyaz]


The above approach we don't do, because need to create PODS manually , so lets go for manifest declarative approach


================================================

2. Declarative (Manifest file)
-----------------------------

In manifest file we have these mandatory parameters

---------------------
apiVersion:
kind:
metadata:
spec:
---------------------

What is apiVersion?
-------------------

apiVersion: Specifies the API version used for the Deployment object. apps/v1 is the stable API version for managing deployments.
            Depending on Kubernetes object we want to create, there is a corresponding     code library we want to use.
            apiVersion refers to Code Library

Examples:
---------

POD : v1
Service: v1
NameSpace: v1
Secrets: v1
Replicaset: apps/v1
Deployment: apps/v1



kubectl api-versions

What is kind ?
--------------

Refers to Kubernetes object which we want to create.

Example:
--------
kind: Pod
kind: Deployment
kind: Service
kind: Ingress
Kind: ReplicaSet
kind: ReplicaController

What is metadata?
----------------

Additional information about the Kubernetes object like name, labels etc

name: The name of the Deployment.
labels: Key-value pairs used for organizing and selecting objects


What is spec?
------------

Contains docker container related information like, image name, environment variables , port mapping etc

How many number of pods (Replica)
About container and that should run on which image
On which port it should expose
Labeling the entire deployment etc



Sample manifest file
--------------------

apiVersion: v1
kind: Pod  -- creating pod
metadata:
  name: pod1 -- name of the pod
spec:  -- specifications
  containers:
    - image: nginx  -- image  name
      name: cont1 -- container name

-----------------

vi pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

-- kubectl create -f pod.yml    [ To create a pod using manifest]

-- kubectl describe pod pod1    [ To describe pod ]

-- kubectl delete pod pod1      [ To delete the pod ]

DRAWBACK: once pod is deleted we can't retrieve the pod.

If any pod deleted, it is deleted, no HA, for HA use ReplicaSET also called RS

KUBECOLOR:

wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
./kubecolor
chmod +x kubecolor
mv kubecolor /usr/local/bin/

kubecolor get po

-- kubectl create -f pod.yml  

-- kubecolor get po

======================================================================================================================================


=======================================================================================================================================

In last class, we created a pod with manifest file, but we deleted that pod.

If any pod deleted, it is deleted, no HA, for HA use ReplicaSET also called RS


=================
REPLICASET
=================

This is used for managing multiple replicas of pod to perform activities like load balancing and autoscaling

Setup minikube with ubuntu 24 as per installation process


It will create replicas of same pod.

we can use same application with multiple pods.

Even if one pod is deleted automatically it will create another pod. It has self healing mechanism

Depends on requirement we can scale the pods.

We create rs --> rs wil create pods

LABELS:
-------

As we are creating multiple pods with same application, all these pods have different names but how to group all of them as we have 1 application with multiple pods. So we can give a label to group them

Individual pods are difficult to manage because they have different names
so we can give a common label to group them and work with them together

SELECTOR
--------

It is used to select pods with same labels

For replicaset use apiversion as apps/v1

how to find the apiresources to write in manifest file

kubectl api-resources

vi replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: ib-rs    ---------- Name of the Replicaset
  labels:
    app: bank
spec:     ------------- this spec is for PODS
  replicas: 3   --------- how many number of pods
  selector:
    matchLabels:  -Ensures only pods with label app: bank are part of this Replicaset. if there is any pod with label bank, it will be a part of this replicaset
      app: bank
  template:            ------------ Ensures the pods get labeled as app: bank
    metadata:
      labels:
        app: bank
    spec:  ----------------- this spec is for containers
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest




apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: ib-rs
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:  
    metadata:
      labels:
        app: bank
    spec:  
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest



-- kubectl create -f replicaset.yml

-- kubectl get replicaset

       (or)

-- kubectl get rs

-- kubectl get rs -o wide  [This command will get more details about ReplciaSets]

-- kubectl describe rs ib-rs  [This will describe about internetbanking-rs ]

-- kubectl get pods

-- kubectl get pods --show-labels   [This will list the pods with Labels]

If you delete any pod, automatically new pod will be created, if you want to watch live, open another terminate and give.

-- kubectl get pods --watch

-- kubectl get pods

-- kubectl delete pods pod-id      [First new pod create and the existing pod will be deleted]

-- kubectl get pods --show-labels  [New pod got created, this is called ReplicaSet, if one pod delete , another pod will get created automatically]

-- kubectl get pods -l app=bank  [This will list all the pods with label bank, l = label]

-- kubectl delete pods -l app=bank  [To delete all the pods wit label bank]

Note: Replicaset will take image details from manifest file -- replicaset.yml

==============
SCALE REPLICAS - Scale Out and Scale In
==============

Scale Out
--------

First open anotherwindows live

-- kubectl get pods --watch


-- kubectl get rs   [To list the replicasets]

-- kubectl scale rs/ib-rs --replicas=10  [Now see pods creating live]

Scale In
--------

-- kubectl scale rs/ib-rs --replicas=5  [Now see pods creating live]

LIFO: LAST IN FIRST OUT.

IF A POD IS CREATED LASTLY IT WILL DELETE FIRST WHEN SCALE IN

Note: This Scale out and in is manual, later we learn how to automate

Now, all pods are running with ib-image:latest image , but if i want to change the image to mobilebanking and update the POD, not possible in ReplicaSet

-- kubectl describe pod -l app=bank | grep -i ID   [ALl pods are using ib-image:latest]

Update the image in the replicaset, you cannot update in yml file, it will create a new replicaSet so there is a command to edit current replicaset

-- kubectl edit rs/ib-rs    [change internetbankingrepo to insurance]

-- kubectl describe pod -l app=bank | grep -i ID  [Still it shows internetbanking, image is not change , that's the problem with Replica SET, We cannot update the application]


vi replicaset.yml  -- change to insurance

-- kubectl apply -f replicaset.yml

-- kubectl get pods --show-labels

-- kubectl describe pod -l app=bank | grep -i ID   [you still see old image internetbanking ]

But if you scale out, new pods will contains insurance repo

-- kubectl scale rs/ib-rs --replicas=5
-- kubectl describe pod -l app=bank | grep -i ID  [you see mobilebanking . only new image are insurance.
                                                   This is the drawback of replicaset]

Using ReplicaSet we cannot roll out the application

Advantage
-- self healing
-- scaling

Drawbacks
 -- we cannot roll in and roll out, we cant update the applications using ReplicaSet, lets use DEPLOYMENT

rs ---> pods

kubectl delete rs ib-rs
~

ReplicationController:
=======================

Same as ReplicaSet. It also used for handling multiple replicas of specific pod. But it doesn't contain selector and its child field matchField. matchField where it will search for pods based on a specific label name and adds them to Cluster
                                                                                             

Kind : ReplicationController

This below code doesn't contain matchLabels Field
******************************************

apiVersion: apps/v1
kind: ReplicationController
metadata:
  name: ib-rs
  labels:
    app: bank
spec:
  replicas: 3
  template:  
    metadata:
      labels:
        app: bank
    spec:  
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest
==============================
   KOPS
==============================

Kops (Kubernetes Operations) is a command-line tool that helps you create, manage, upgrade, and destroy Kubernetes clusters on cloud providers like AWS, GCP, and more.

It's often referred to as the "kubectl" for clusters, and it is a popular choice for creating production-grade Kubernetes clusters on AWS.

Advantages
-------------
Automate infra creation in AWS and GCE Kubernetes cluster

Deploys HA K8S master

Supports rolling updates

supports homogenous and heterogenous clusters.

save time and work.

Alternatives: EKS, MINIKUBE, KUBEADM, RANCHER
------------

KOPS will just create Production level Cluster, its a 3rd party tool

=======================================================
                          KOPS Setup
========================================================

Launch Amazon Linux 2 EC2 instance with t2.micro

Make sure AWS CLI is installed. Attach a IAM Role with Admin permissions

Install kubectl and kops
=======================

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

copy paste all at a time if required

vi .bashrc

export PATH=$PATH:/usr/local/bin/

wq!

appends /usr/local/bin/ to the system's $PATH environment variable, allowing you to execute binaries located in /usr/local/bin/ from anywhere in the terminal without specifying the full path.

--- source .bashrc

in ubuntu no need to do above steps, but for amazon Linux yes

Creating bucket TO STORE CLUSTER INFO
=====================================

Storing cluster info in s3 bucket because just in case if we loose K8S cluster we can get from S3
Either you can create a bucket manually using console or run below commands

--> aws s3api create-bucket --bucket reyaz-kops-testbkt123.k8s.local --region ap-south-1 --create-bucket-configuration LocationConstraint=ap-south-1

--> aws s3api put-bucket-versioning --bucket reyaz-kops-testbkt123.k8s.local --region ap-south-1 --versioning-configuration Status=Enabled

--> export KOPS_STATE_STORE=s3://reyaz-kops-testbkt123.k8s.local

--> kops create cluster --name reyaz.k8s.local --zones ap-south-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro

--> kops update cluster --name reyaz.k8s.local --yes --admin

kops update cluster → Updates the Kubernetes cluster configuration.
--name reyaz.k8s.local → Specifies the cluster name (replace with your actual cluster name).
--yes → Applies the changes automatically (without requiring manual confirmation).
--admin → Grants the current user admin privileges in the cluster.


--> kops validate cluster --wait 10m

This command checks if your Kubernetes cluster is fully functional and waits up to 10 minutes for all nodes and components to become ready.


For Future Reference

Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster reyaz.k8s.local
 * edit your node instance group: kops edit ig --name=reyaz.k8s.local nodes-ap-south-1a
 * edit your master instance group: kops edit ig --name=reyaz.k8s.local master-ap-south-1a

See in AWS Console, EC2 instances are launched with ASG and load balancer

By default KOPS will create CLB. If you want KOPS to create ALB

kops edit cluster --name reyaz.k8s.local    [change classic to Network]

spec:
  api:
    loadBalancer:
      class: Classic
      type: Public


Note: just try to delete now worker node or master node, ASG will create it immediately

-- kops get cluster

-- kubectl get nodes/no

-- kubectl get nodes -o wide


vi deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 4
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest

-- kubectl create -f deployment.yml

-- kubectl get pods

-- kubectl get nodes -o wide

Manager node will not host pods himself, K8S will create pods only in worker nodes

-- kubectl get pods -o wide   [see the nodes, pods spread across nodes equally]

Example: if you want 4 pods , edit yml file and update replicas to 4

-- kubectl apply -f deployment.yml

-- kubectl get no -o wide

===========================
Few Cluster Admin Activities
==========================

Currently we have now only 1 master nodes and 2 worker nodes

KOPS commands are used for cluster activities.
KUBECTL commands are used for resource activities.

annotations
----------
ig =  instance group

WorkerNodes = nodes-ap-south-1a

Scale Out Worker Nodes
----------------------

-- kops edit ig --name=reyaz.k8s.local nodes-ap-south-1a
           --> max size = 4, min size = 4

-- kops update cluster --name reyaz.k8s.local --yes --admin

-- kops rolling-update cluster --yes

See in AWS Console , 2 additional worker nodes got created

-- kubectl get nodes   [takes time to show]

Scale out Master Nodes
----------------------

-- kops edit ig --name=reyaz.k8s.local master-ap-south-1a
           --> max size = 2, min size = 2

-- kops update cluster --name reyaz.k8s.local --yes --admin

-- kops rolling-update cluster --yes

See in AWS Console , 1 additional Master nodes got created

-- kubectl get nodes  [takes time to show]

*************************************************************************
IF ERROR
=======
🔹 Step 3: If Kubelet is Installed but Not Found in Systemd
If kubelet is installed but still not found in systemd, try reloading systemd:

sudo systemctl daemon-reload
sudo systemctl restart kubelet

If the issue persists, manually add the systemd service file:

sudo nano /etc/systemd/system/kubelet.service

[Service]
ExecStart=/usr/bin/kubelet
Restart=always
StartLimitInterval=0
RestartSec=10
[Install]
WantedBy=multi-user.target
Then reload and start:

sudo systemctl daemon-reload
sudo systemctl enable kubelet
sudo systemctl restart kubelet

🔹 Step 4: Check Logs for Errors
If kubelet is still failing, check logs:

sudo journalctl -u kubelet -f


*******************************************************









Example: Lets do a new deployment again
--------------------------------------

Before that clean up

-- kubectl get deployments

-- kubectl delete deploy ib-deployment

-- kubectl get pods

vi deploynew.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mb-deployment
  labels:
    app: bank
spec:
  replicas: 4
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/mb-image:latest

-- kubectl create -f deploynew.yml

-- kubectl get po -o wide   [Now all pods are spread across 4 worker nodes]

If you want to scale out pods
-------------------------

-- kubectl scale deploy/mb-deployment --replicas=10

-- kubectl get pods -o wide

If you want to scale in pods
-------------------------

-- kubectl scale deploy/mb-deployment --replicas=4

-- kubectl get pods -o wide



============================================================================================================================================
 


===============
NAMESPACES
===============

Generally we cannot create multiple Cluster for each team(expensive), instead we create a NameSpaces in one Cluster for each team

Namespaces will not talk to each other, its an isolated space in one Cluster

Each namespaces can see their own pods

TYPES:

default            : This is default namespace, all objects are created here
kube-node-release    : It will store the object which is taken from one namespace to another
kube-public        : All Public objects are stored here, generally namespace are private, if you want common public namespace
kube-system        : By default K8S will create some object, those are stored here


Note: Every component of kubernetes cluster is going to create in the form of POD, All these PODS are stored in Kube-System Namespace

-- kubectl get namespace/ns

-- kubectl get pods

-- kubectl describe pod   [ By default pods are created in default namespace]


-- kubectl get pods -A  [This will list all pods from all namespaces]

-- kubectl get pods --all-namespaces  [This can also be used to get all pods from all namespaces]

-- kubectl get pods -n default [This will list pods wchich is in default namespace]

-- kubectl get pods -n kube-node-release  -- no pods here

-- kubectl get pods -n kube-public  -- no pods here

-- kubectl get pods -n kube-system  [all kubeproxy, apiserver, controller, schedular pods are here]


Lets create a pods in namespaces
--------------------------------

-- kubectl get ns

-- kubectl create ns dev

-- kubectl get ns

Currently I am in default namespace, how to check

-- kubectl config view   [ output doesn't show namespace,  if you don't see any namespace, this is default]

-- kubectl config set-context --current --namespace=dev  [This is use to switch to move from another namespace]

-- kubectl config view  [ Now this output show dev namespace ]

-- kubectl get pods  [Now you are in dev namespace, now you cannot see any pods of other namespace]

== create a new pods in this namespace

-- kubectl run dev1 --image nginx
-- kubectl run dev2 --image nginx
-- kubectl run dev3 --image nginx

-- kubectl get pods

=== lets create few pods in PROD namespace
------------------------------------------

kubectl get ns

kubectl create ns prod

kubectl get ns

kubectl config view  -- see the output, we are in dev

kubectl config set-context --current --namespace=prod   --- now change the namespace to prod

kubectl config view

kubectl get po

you are in prod namespace , but if you want to see from another namespace

kubectl run prod1 --image nginx
kubectl run prod2 --image nginx
kubectl run prod3 --image nginx

kubectl get po

kubectl get po -n default
kubectl get po -n dev
kubectl get po -n prod

kubectl delete pod dev1 -n dev   [Deleting the  pod dev2 in dev namespace]

kubectl delete pod prod1 -n prod   [Deleting the  pod prod1 in prod namespace]

kubectl delete pod --all : [To delete all pods in the namespace]

If require: kubectl delete namespace <namespace-name>




kops delete cluster --name reyaz.k8s.local --yes
===============================
Role-Based Access Control (RBAC)
===============================

Role-Based Access Control (RBAC) in Kubernetes is a method for regulating access to resources in a Kubernetes cluster based on the roles of individual users or service accounts.

RBAC helps you define what actions (verbs like get, list, create, delete, etc.) can be performed on specific resources (like pods, services, deployments, etc.) within specific namespaces or across the cluster.

Role: A set of permissions (rules) that are defined within a namespace. Roles are used to grant access to resources within a specific namespace.

RoleBinding: Grants the permissions defined in a Role to a user or service account within a specific namespace.

Authentication(who are you?) and Authorization (what can you do?)

Users vs Service Accounts
==========================
Before setting up RBAC, it’s important to understand the Kubernetes user model. There are two ways to create “users” depending on the type of access that’s required:

Users —
------
In Kubernetes, a User represents a human who authenticates to the cluster using an external service. You can use private keys (the default method), a list of usernames and passwords, or an OAuth service such as Google Accounts. Users are not managed by Kubernetes; there is no API object for them so you can’t create them using Kubectl. You must make changes at the external service provider.

Service Accounts —
-------------------
Service Accounts are token values that can be used to grant access to namespaces in your cluster. They’re designed for use by applications and system components. Unlike Users, Service Accounts are backed by Kubernetes objects and can be managed using the API.



Assume authentication has been done on the kubernetes cluster by company with user jack, Let use give permissions to him to cluster only to get, list and watch

Create a namespace in kubernetes

-- kubectl create ns dev [use if already exits]

-- kubectl config set-context --current --namespace=dev

First create a sample serviceaccount here in kubernetes called jack

Create a Serviceaccount ---> Create a ROLE(give permissions) --->  Role Binding

vi serviceaccount.yml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: jack
  namespace: dev

-- kubectl create -f serviceaccount.yml

--------------------------------------------------------------

Create a Role within the dev namespace that allows the user jack to perform certain actions on pods:

vi role.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: dev-pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]


-- kubectl create -f role.yml

Since pods belong to the core API group, we use apiGroups: [""]


------------------------------------------------------------------

Next, bind the pod-reader Role to a user or service account. For simplicity, we'll bind it to a service account named jack:

This RoleBinding binds the pod-reader Role to the jack user, allowing that user to get, list, and watch pods in the dev namespace.

vi rolebinding.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: User
  name: jack
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-pod-reader
  apiGroup: rbac.authorization.k8s.io


-- kubectl create -f rolebinding.yml


Verifying Access
-----------------
To verify that the jack user has the correct permissions, you can impersonate the user using kubectl:

-- kubectl auth can-i list pods --namespace=dev --as=jack   [answer yes]

-- kubectl auth can-i create pods --namespace=dev --as=jack  [answer no]


-- kubectl get roles -n dev    [List all roles in a namespace:


-- kubectl describe rolebinding read-pods -n dev

For more depth
-----------------
https://spacelift.io/blog/kubernetes-rbac


=====================
DAEMONSET
======================

✔️ DaemonSets ensure one pod per node for system-level services.
✔️ Used for monitoring, logging, networking, and security.
✔️ Automatically adds/removes pods when nodes join/leave the cluster.


If you want to deploy only 1 pod per 1 node use DAEMONSET
it is a old version of Deployment

First delete existing deployment

-- kubectl get deploy

-- kubectl delete deploy mobilebanking-rs

-- kubectl get pods


in the below yml , only change is kind: DaemonSet (Capital D and S) and remove Replica

vi daemon.yml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mb-daemon
  labels:
    app: bank
spec:
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/mb-image:latest


In the above manifest, we didn't mentioned any replicas, but daemonset will create one pod per node for sure.

-- kubectl create -f daemon.yml

-- kubectl get po


It shows 2 pods, because DaemonSet will create 1 pod per 1 node, so 2 nodes, 1 pod each

-- kubectl get daemonset

-- kubectl get daemonset --namespace=default  [show daemonset in particular namespace]

-- kubectl delete daemonset <daemonset-name> --namespace=<namespace>

-- kubectl delete daemonset mb-daemon --namespace=default

-- kubectl get daemonset

tip:

-- kubectl delete daemonset --all --namespace=default  [to delete all daemonsets in namespace default]


==============================================================================================================================

                                      

=================================================================================================================================


==========================
SERVICES - if you want to expose the application
=========================

Services allow you to expose your applications running on Pods to other components within the cluster or to external users.

Services provide a stable IP address and DNS name for the set of Pods, which might change over time due to scaling or updates

Key Concepts of Kubernetes Services:
----------------------------------

ClusterIP:
---------

This is the default type of Service. It exposes the Service on a cluster-internal IP. Other services within the same Kubernetes cluster can access the Service, but it is not accessible from outside the cluster.

This creates a connection using an internal Cluster IP address and a Port.

NodePort:
---------

This type of Service exposes the Service on each Node’s IP at a static port. A NodePort Service is accessible from outside the cluster by hitting the <NodeIP>:<NodePort>.

When a NodePort is created, kube-proxy exposes a port in the range 30000-32767:

TargetPort:
-----------

Pod Container port. Pod's container listens on applicationn port Ex: 80 or 8080, if you dont use this line, K8s will assign default 80 port


LoadBalancer:
-------------

This Service type exposes the Service externally using a cloud provider’s load balancer. It is typically used in cloud environments like AWS, GCP, or Azure.

A LoadBalancer is a Kubernetes service that:

Creates a service like ClusterIP
Opens a port in every node like NodePort
Uses a LoadBalancer implementation from your cloud provider (your cloud provider needs to support this for LoadBalancers to work).



-- kops get cluster

-- kubectl get nodes

-- kubectl get events  [This command is used to see all the cluster events]

-- kubectl get svc   [By default kuberenets will create a default svc ]

Let us create a new service

vi service.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/ib-image:latest
---
apiVersion: v1
kind: Service
metadata:
  name: ib-service
spec:
  type: ClusterIP
  selector:
    app: bank
  ports:
    - port: 80        # Exposes the service on port 80
      targetPort: 80  #  Pod's container listens on 80, if you dont use this line, K8s will assign default 80 port


How Traffic Flows
------------------
Client connects to ib-service:80
Service forwards request to the Pod’s container at 8080 if 80 (because targetPort: 8080 / 80)
The container in the Pod listens on 8080 / 80 and processes the request.

targetport = Pod's container
nodeport = Node's external port


-- kubectl create -f service.yml    [To create deployment]

-- kubectl get pods  [To get the list of the pods]

-- kubectl get svc   [Get the list of services created]

-- kubectl get pods -o wide  [Get the pods details ]

-- kubectl describe svc ib-service  [Get the detailed description of service]

===== This has clusterIP service, but  cannot exposed to internet, use this for databases which should be in private

-- kubectl delete -f service.yml    [ This will delete service and deployment ]



===============
NodePort
===============

IT will expose our application in a particular port

Range 30000 - 32767 (in SG, we need to give all traffic)

if we don't specify any port K8s will pick randomly


vi service.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mb-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/mb-image:latest
---
apiVersion: v1
kind: Service
metadata:
  name: mb-service
spec:
  type: NodePort   # Type Nodeport, but if you dont mention port number under ports section, K8s will assign random port
  selector:
    app: bank
  ports:
    - port: 80
      targetPort: 80  # Ensure this matches the container's port, if you dont use this line, K8s will assign default 80 port


-- kubectl apply -f service.yml  [Can use create to create new resources , update is used to update on existing resources]

-- kubectl describe svc mb-service

-- kubectl get svc  [ See the output, it shows nodeport, port number is random ]

Now http://IP:portnumber  it will not work as K8s will create a new vpc and SG, allow all traffic in new SG(nodes.reyaz.k8s.local)
    Custom TCP = 31433 , custom=anywhere
        All traffic , custom = anywhere

==== if you want your own customized port , just edit the before file

vi service.yml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: mb-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: reyadocker/mobilebankingrepo:latest
---
apiVersion: v1
kind: Service
metadata:
  name: mb-service
spec:
  type: NodePort
  selector:
    app: bank
  ports:
    - port: 80
      targetPort: 80  # Ensure this matches the container's port, if you dont use this line, K8s will assign default 80 port
      nodePort: 31433  # External access via <NodeIP>:31234 Must be in range 30000-32767


Client accesses http://<NodeIP>:31234
Traffic reaches Kubernetes Node on nodePort: 31234
The service forwards it to port: 80 (internal Service port)
Then it forwards the request to targetPort: 80 (inside the Pod’s container)


-- kubectl apply -f service.yml   [ we can update existing deployment ]

create vs apply : create will create new resources, apply for updating existing resources

-- kubectl describe svc mb-service

-- kubectl get svc

http://IP:31433


Drawback of nodeport : we should not give IP and port number to customer, he is not interested
----------------

====================================================================
LoadBalancer - it will expose our app to customer using URL, map in R53
====================================================================

first delete the deployment

-- kubectl delete -f service.yml


vi service.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: insurance-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: trainerreyaz/insurance-image:latest
---
apiVersion: v1
kind: Service
metadata:
  name: mb-service
spec:
  type: LoadBalancer
  selector:
    app: bank
  ports:
    - port: 80
      targetPort: 80  # Ensure this matches the container's port, if you dont use this line, K8s will assign default 80 port
      nodePort: 31433 # If you dont mention target and nodeports, K8s will generate random nodeport and target port 80


-- kubectl create -f service.yml

-- kubectl describe svc mb-service

-- kubectl get svc  [Grab the elb, and see in AWS Console : ELB, Target Group. Wait for sometime to come InService]

-- kubectl delete -f service.yml [if required]




Exploring Task
----------------------

Kubernetes Dashboard
	
============================================

Kubernetes Metric Server  - also called Heapster

=============================================

It's a scalable, efficient source for monitoring the overall health and performance of a Kubernetes cluster, providing the data needed for Kubernetes features like Horizontal Pod Autoscaler (HPA) and the Kubernetes Dashboard.

Key Features:
============

Resource Metrics:
----------------
Collects CPU and memory usage metrics from the kubelets and provides aggregated metrics at the node and pod level.

Autoscaling:
------------
Enables features like the Horizontal Pod Autoscaler (HPA), which automatically adjusts the number of pods in a deployment based on observed CPU or memory utilization.

Kubernetes Dashboard:
---------------------
The Metrics Server provides the resource usage data displayed in the Kubernetes Dashboard.

How Metrics Server Works:
-------------------------

Kubelets:
--------
Each node in a Kubernetes cluster runs a kubelet that periodically collects resource usage statistics from the node and the containers running on it.

Metrics Server:
-------------
The Metrics Server collects these metrics from the kubelets and stores them in memory, aggregating them to be accessed by other components (like the HPA).


In Short
========

This metric server in K8S will collect metrics information like cpu, ram etc for all pods and nodes in the cluster

A single deployment that works on most clusters , collect metrics every 15 secs

We can use kubectl top po/no to see the metrics

-- kubectl top po/no   [ Will not work, as we don't have metric server configured ]

Install Metric Server
---------------------

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml

kubectl top pods     [ Wait for 2 mins ]

kubectl top nodes/no

========================
Auto-Scaling
========================

HPA - Horizontal Pod Autoscaling - Scales the number of pods based on CPU/memory utilization or custom metrics.
--------------------------------

VPA  - Vertical Pod Autoscaling - Adjusts the CPU/memory requests/limits of a pod dynamically to improve resource allocation. you'll need to  
--------------------------------  install the VPA components in your Kubernetes cluster, as it is not included by default.
                                  Install the VPA Custom Resource Definitions (CRDs) seperately


In K8S , a HPA automatically updates a workload resource (such as Deployment or ReplicaSet) based on demand.

Give the value example 70%, if going more than 70% scale out and less than it will do Scale In

Metric server will do the major role here as it will collects metrics , based on the value scale out and scale in happens

Before scale In, process will wait for few mins to scale in to complete the traffic requests
this is called COOLING PERIOD

In Kubernetes, the cooling period for the Horizontal Pod Autoscaler (HPA) is the amount of time the HPA waits after a scale event before triggering another scale event.

Scaling can be done only for Scalable Objects(Ex RS, Deployment and RC Replication Controller)

Side note:  Replication Controller(RC), A Replication Controller in Kubernetes is an older mechanism used to ensure that a specified number of pod replicas are running at any given time. Now it is replaced with ReplicaSet

vi auto.yml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: mb-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: reyadocker/mobilebankingrepo:latest


-- kubectl create -f auto.yml

-- kubectl get pods

-- kubectl top pods

Now lets stress the pods

-- kubectl autoscale deployment mb-deployment --cpu-percent=20 --min=1 --max=10

Autoscale deployment called mb-deployment if my cpu is more than 20%, scale out, min 1 , max 10

-- kubectl get hpa    [ This wil show cpu: 1% / 20% , now cpu percent is 1%, it will take time ]

Now lets stress the pod by installing stress inside pod, lets connect to pod using exec and install stress

-- kubectl get pods

-- kubectl exec -it mb-deployment-8585b755c5-c6fb7 -- /bin/bash

         -- apt update
         -- apt install stress
         -- stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 60s
         -- exit

--cpu 8: Launches 8 CPU stressors, each consuming 100% CPU by performing continuous computations.​
--io 4: Initiates 4 I/O stressors, each generating continuous I/O operations to stress the system's disk and filesystem.​
--vm 2: Starts 2 virtual memory stressors, each allocating and deallocating memory repeatedly to test the system's memory management.​
--vm-bytes 128M: Specifies that each virtual memory stressor should allocate 128 megabytes of memory.​
--timeout 60s: Sets the duration of the stress test to 60 seconds, after which all stressors will terminate.




open another terminal to watch the live pods

-- kubectl get po --watch

On main server

-- kubectl top pods

-- kubectl get hpa

-- kubectl get pods

-- kubectl describe hpa mb-deployment     [ This will show scaling activities ]

-- kubectl get events   [ This will also show same things ]

-- kubectl logs mb-deployment-8585b755c5-p2bzv   [ To see logs of the pod ]

After few mins, scale in happens as we dont have load.

-- kubectl delete -f auto.yml

Example using Manifestfile
--------------------------

First we need to have deployment and then we can autscale on that deployment / deployment name

vi auto.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 2
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: reyadocker/internetbankingrepo:latest


-- kubectl apply -f auto.yml

--------------------------------

vi hpa.yml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ib-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ib-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 20


scaleTargetRef specifies the target deployment (ib-deployment) for scaling


-- kubectl apply -f hpa.yml

Open another tab --> kubectl get pods --watch

-- kubectl get hpa

-- kubectl get pods

-- kubectl exec -it mb-deployment-8585b755c5-c6fb7 -- /bin/bash

         -- apt update
         -- apt install stress
         -- stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 60s
         -- exit

-- kubectl get hpa


Note: if you want to delete metric server , just instead apply, use delete

kubectl delete -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml


===========================
Resource Quota
===========================


K8S cluster can be divided into namespaces
By default the pods in K8s will run with no limitation of Memory and CPU
WE need to give the limit for the pod
IT can limit the objects that can be created in a namespace and total amount of resources
pod schedular in master will check the worker nodes on cpu and memory and create a pod in it
we can set limits to CPU, Memory and storage
CPU is measrured on Cores and memory on Bytes
1CPU = 1000 milliCPUs

Requests = how much we want
Limit = how much max we want

limits can be given to pod and nodes also
the default limit is 0

if you mention request and limit everthing will work fine
if you dont mention request and mention limit then Request = limit
if you dont mention request dont mention limit , Request ! = limit

Every pod in the namespace must have CPU limit, no need to mention memory, if you dont mention cpu, pod will take all cpu
The amount of CPU used by all pods inside namespace must not exceed specified limit

-- kubectl create ns dev

-- kubectl config set-context --current --namespace=dev

-- kubectl config view  [To see which namespace we are using]

vi dev-quota.yml


apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi


Pods: A maximum of 5 pods can be created.​
CPU Limits: The total CPU limit across all containers is restricted to 1 CPU.​
Memory Limits: The total memory limit across all containers is capped at 1 GiB.


-- kubectl create -f dev-quota.yml

-- kubectl get quota

NAME        AGE   REQUEST     LIMIT
dev-quota   69s   pods: 0/5   limits.cpu: 0/1, limits.memory: 0/1Gi


-- kubectl run pod1 --image=nginx  [ This will not work because, we need to mention quota for dev namespace as we are in that ns
                                     we can put --cpu and --memory in the command or use manifest file ]

vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
        - name: cont1
          image: reyadocker/internetbankingrepo:latest
          resources:
            limits:
              cpu: "1"
              memory: 512Mi

-- kubectl create -f deploy.yml

-- kubectl get pods  [It has created only 1 pod, as we mention replica = 3 , this is due to restriction we made for this namespace dev]

In the above manifest file, we are mentioning cpu 1 and memory 512 for each pod, but in name space dev we have restricted to 1cpu and 1GB memory, if you run kubectl create -f deploy.yml , it will create only 1 pod because of restriction, if you want all 3 pods put cpu as 0.3 ad 300Mi, it will create all- to do this kubectl delete -f deploy.yml and recreate it

-- kubectl delete -f deploy.yml


vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
        - name: cont1
          image: reyadocker/internetbankingrepo:latest
          resources:
            limits:
              cpu: "0.3"
              memory: 300Mi


-- kubectl create -f deploy.yml

-- kubectl get po

-- kubectl get quota

-- kubectl delete -f deploy.yml

========================================================================================================================

======================
PV - Persistent Volume
======================

Stateless : if i delete pod data is lost, because data is stored locally on the pod and instance
----------
Stateful: if i delete the pod data is persistent, because we can store the data in external storage like AWS EBS
---------

Kubernetes Persistent Volumes (PVs) provide a way to manage durable storage for applications running in a Kubernetes cluster.
----------------------------------
Unlike ephemeral storage tied to the lifecycle of a pod, Persistent Volumes exist independently of pods and remain intact even after pods are deleted.

This makes PVs ideal for stateful applications that require persistent storage, such as databases


Persistent meaning permanent

PV are independent they can exists even if no pod is using them

It is created by administrator or dynamically created by storage class

Once a PV is created , it can be bound to a Persistent Volume Claim (PVC), which is a request for storage by a pod

When a pod requests storage via PVC , K8S will search for a suitable PV to satisfy the requests

PV is bound to the PVC and the pod can use the storage

If no suitable PV is found, K8S will either dynamically create a new one (if the storage class support dynamic provisioning ) or the PVC will remain unbound

PV will maintain total storage , PV Is bound to PVC
Pods will ask PVC, PVC will get from PV


PVC
===

TO use PV we need to claim the volume using PVC
PVC request a PV with your desired specification(size, access, modes & speed etc) from K8S and once a suitable PV is found it will bound to PVC

After bounding is done to pod you can mount is as a volume

Once user finished work, the attached PV can be released the underlying PV can be reclaimed and recycled for future

if you create volume in cluster , if cluster is delete , storage is also deleted. SO use AWS EBS Volumes


First, create a EBS volume in EC2, with 10GB magnetic


-- kubectl delete ns dev   [delete the namespace and come to default]
-- kubectl config set-context --current --namespace=default

vi pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-0771f0561f66408c9
    fsType: ext4

-- kubectl create -f pv.yml

-- kubectl get pv


----------------------------------------------------------------

--- Now create pvc

vi pvc.yml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi


-- kubectl create -f pvc.yml

-- kubectl get pv   [ This will show pv and pvc both ]

Note: kubectl get pvc is showing in pending state, because we need to create a new pods to consume first. kubectl get events -- see logs

--Now lets setup a statefull application, statefull meaning, it will keep the previous data

vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
     app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
      - name: cont1
        image: centos
        command: ["/bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: my-pv
          mountPath: "/tmp/persistent"
      volumes:
        - name: my-pv
          persistentVolumeClaim:
            claimName: my-pvc


-- kubectl create -f deploy.yml

-- kubectl get pods

Now lets go inside container to see the volume mount

-- kubectl exec -it podid-dfgdkjjf -- /bin/bash

-- cd /tmp

-- ls

-- cd persistent

-- touch file{1..5}
vi file1
this is from pod-1 pv

exit

-- kubectl get pods

lets delete the pod now, if we delete the pod, a new pod will be created automatically , data will not get deleted as it is in the persistent volume

-- kubectl delete pods podid-rfgdfdjg

-- kubectl get pods   [ you have a new pod ]

--kubectl exec -it podid-dfdgh435 -- /bin/bash

or

-- kubectl exec -it podid-dfdgh435-- ls /tmp/persistent

cd /tmp/persistent
ls
we can see the same data

exit

This is stateful

=======================================================

If you want to increase the size, increase EBS volume to 25

kubectl describe pv  [ Capacity is shows 10gb only ]

vi pv.yml
in storage change to 20Gi

kubectl apply -f pv.yml

kubectl describe pv

kubectl delete -f .   [Delete all deployments]

================================================================================================================
ENV VARIABLES:
---------------
It is a way to pass configuration information to containers running within pods. To set Env vars it include the env or envFrom field in the configuration file.

ENV: Allows you to set environment variables for a container, specifying a value directly for each variable through CLI/ Command prompt
----
ENVFROM: PASSING Variables FROM FILE - 2 Types, configmaps and secrets
------

CONFIGMAPS
--------------------
It is used to store the data in key-value pair, files, or command-line arguments that can be used by pods, containers in cluster.
But the data should be non-confidential. It does not provide security and encryption
If you want to provide Enryption use Secrets in K8S.

Limit of the configmap is only 1MB

But if you want to store more than 1MB configmap data mount volume or use a separate database or a file service

Example with ENV
----------------

-- kubectl create deploy newdb --image=mariadb         [ This will create a deployment called newdb with single pod having mariadb]

-- kubectl get pods

-- kubectl logs newdb-794dd57dbc-tr7s9   [It is crashed because we haven't specified the passsowrd for MariaDB]

-- kubectl set env deploy newdb MYSQL_ROOT_PASSWORD=root123456

-- kubectl get pods  [ Now it will be in running state, but we are passing password directly from command ]

-- kubectl delete deploy newdb



Pod State : ImagePullBackOff:
----------------------------
When a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff.

The status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image for reasons such as

Invalid image name or
Pulling from a private registry without imagePullSecret.
The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay.

Kubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).


Pod State: CrashLoopBackOff
---------------------------

When you see "CrashLoopBackOff," it means that kubelet is trying to run the container, but it keeps failing and crashing. After crashing, Kubernetes tries to restart the container automatically, but if the container keeps failing repeatedly, you end up in a loop of crashes and restarts, thus the term "CrashLoopBackOff."



Passing from Var file
--------------------

-- kubectl create deploy newdb --image=mariadb

-- kubectl get pods

-- kubectl logs newdb-794dd57dbc-f44f2

This will not work because haven't specified the password, in above example we have passed password using ENV but lets use now ENVFROM

--> create a file called vars

vi vars

MYSQL_ROOT_PASSWORD=root123456
MYSQL_USER=admin

lets use configmap and create configmap called dbvars

-- kubectl create cm dbvars --from-env-file=vars  [It will create a configmap called dbvars using var file we created]

--kubectl get cm

-- kubectl describe cm dbvars

This will show the plain data


Now we just created a configmap called dbvars, using this lets deploy our pods

-- kubectl get cm

-- kubectl set env deploy newdb --from=configmaps/dbvars

-- kubectl get pods

we can give configmaps also in manifest file

-- kubectl get cm

-- kubectl delete deploy newdb

SECRETS:
--------

SECRETS: To store sensitive data in an unencrypted format like passwords, ssh-keys etc it uses base64 encoded format
password=reyaz (now we can encode and decode the value)

WHY: if i dont want to expose the sensitive info so we use SECRETS

By default k8s will create some Secrets these are useful from me to create communicate inside the cluster used to communicate with one resource to another in cluster

These are system created secrets, we need not to delete
TYPES:
Generic: creates secrets from files, dir, literal (direct values)
TLS: Keys and certs
Docker Registry: used to get private images by using the password



-- kubectl create deploy newdb --image=mariadb

-- kubectl get pods  [This will fail because no env variable]

first create secrets , 2 ways to create, from CLI or from File

-- kubectl create secret generic password --from-literal=ROOT_PASSWORD=reyaz123 (from cli)  --- literal meaning direct value

-- kubectl create secret generic my-secret --from-env-file=vars (from file)

-- kubectl get secrets

-- kubectl describe secret my-secret [Name of the secret is my-secret ]

-- kubectl set env deploy newdb --from=secrets/my-secret

-- kubectl get pods  [This will fail because we have not mention the MYSQL_ prefix]

-- kubectl set env deploy newdb --from=secret/my-secret --prefix=MYSQL_

without passing prefix we cant make the pod running status

TO SEE SECRETS:
-------------
-- kubectl get secrets password -o yaml

-- echo -n "LKJSKFHJHi" | base64 -d
or
-- echo -n "LKJSKFHJHi" | base64 --decode


-- kubectl delete deploy newdb

=====================================================================================================================================================

===================
SIDE CAR:
===================

It creates a helper container to main container.
main container will have application and helper container will do help for main container.

Adapter Design Pattern:
----------------------
standardize the output pattern of main container.

Ambassador Design Pattern:
-------------------------
used to connect containers with the outside world

Init Container:
--------------
it initialize the first work and exits later.

​A sidecar container is a secondary container that runs alongside the main application container within the same Kubernetes Pod. This design pattern allows you to extend or enhance the functionality of the main application without modifying its code. Common use cases include log aggregation, data synchronization, and proxying network traffic.​


Example: Log Aggregation with a Sidecar Container
-------------------------------------------------
Below is a YAML configuration for a Pod that demonstrates the sidecar pattern. In this example, the main application container writes log data to a shared volume, and the sidecar container serves these logs over HTTP using Nginx

vi sidecar.yml

apiVersion: v1
kind: Pod
metadata:
  name: log-aggregator-pod
spec:
  containers:
    - name: app-container
      image: alpine
      command: ["/bin/sh", "-c"]
      args: ["while true; do date >> /var/log/app.log; sleep 5; done"]
      volumeMounts:
        - name: shared-logs
          mountPath: /var/log
    - name: sidecar-container
      image: nginx
      volumeMounts:
        - name: shared-logs
          mountPath: /usr/share/nginx/html
  volumes:
    - name: shared-logs
      emptyDir: {}



Explanation:
------------
Main Application Container (app-container):
------------------------------------------
Uses the alpine image, a minimal Docker image based on Alpine Linux.​
Executes a shell command that appends the current date and time to /var/log/app.log every five seconds.​
Mounts a shared volume at /var/log to store log data.​

Sidecar Container (sidecar-container):
------------------------------------
Uses the nginx image to serve content over HTTP.​
Mounts the same shared volume at /usr/share/nginx/html, which is the default directory Nginx serves files from.​
As a result, Nginx serves the app.log file, allowing access to the application's log data via HTTP.​

Shared Volume (shared-logs):
-----------------------------
An emptyDir volume that is created when the Pod is assigned to a node and exists as long as the Pod is running.​
Provides a shared storage space accessible to both containers for log data.​


-- kubectl apply -f sidecar.yaml

-- kubectl get pods

-- kubectl describe pods

-- kubectl exec -it log-aggregator-pod -c sidecar-container -- sh

        cd /usr/share/nginx/html
        cat app.log

we can see app.logs which is generated in app-container but can be accessed from side-car container using shared volumes

-- kubectl delete -f sidecar.yml
=================================================================================================================================================================================================================
                                                                                                    **MINI K8S PROJECT**
MongoExpress and MongoDB Project
===============================

Keep the CLuster Ready

kubectl get all 

1.Create a secret file mongodb-secret.yml
2.Create mongo.yml --> For deployment (Backend)
3.Create a Service for mongodb in mongo.yml
4.Mongo Express Deployment & Service & ConfigMap --> mongo-express.yaml (frontend)


===============================
Secret Configuration file mongodb-secret.yml
==================================

vi mongodb-secret.yml

apiVersion: v1
kind: Secret
metadata:
    name: mongodb-secret
type: Opaque
data:
    mongo-root-username: dXNlcm5hbWU=
    mongo-root-password: cGFzc3dvcmQ=


In terminal , 
echo -n 'username' | base64
echo -n 'password' | base64


--> kubectl apply -f mongo-secret.yml
--> kubectl get secrets



Create a MongoDB Deployment
-------------------------

Note: The below yaml says, to create a single pod(replica 1) and a container name mongodb with image mongo (gets from dockerhub)
      --> MongoDB container database port mention in dockerhub is 27017 default
--> go to dockerhub and search for mongo and see the Environment Variables, it contains variables (MONGO_INITDB_ROOT_USERNAME and MONGO_INITDB_ROOT_PASSWORD)

Start writing from  after image

        ports:
        - containerPort: 27017

lets write env variables because we need to pass username and password but cannot pass here directly, for that we need to create a separate yaml mongodb-secret.yml (Secret Configuration file)



Now we can reference what ever we have mentioned in mongo-secret.yml in mongo.yml for username and passwords

valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username

---> after mongo.yml create , lets create a deployment


vi mongo.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom: 
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password


--> kubectl apply -f mongo.yml

--> kubectl get all
--> kubectl get pods
--> kubectl get pod --watch
--> kubectl describe pod podname  [mention podname, and to see any issues with pod]


---> lets now create a service
In yaml we can put multiple documents using ---

vi mongo.yml and add service

---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
spec:
  selector:
    app: mongodb
  ports:
    - protocol: TCP
      port: 27017
      targetPort: 27017


Note: 
----
Kind = Service
metadata/name = a random name
selector = to connect to pod through app name mongodb
port = service port
targetport = container port of deployment


--> kubectl apply -f mongo.yml

--> kubectl get service

--> kubectl describe service mongodb-service

--> kubectl get pod -o wide



Mongo Express Deployment & Service & ConfigMap
============================================

Create a configmap for DB URL
database url = database server is same as service name in mongo.yml mongodb-service


vi mongo-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-configmap
data:
  database_url: mongodb-service

--> kubectl apply -f mongo-configmap.yaml
--> kubectl get pods


Now create a mongo-service (frontend)

vi mongo-express.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
  labels:
    app: mongo-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-express
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express
        ports:
        - containerPort: 8081
        env:
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom: 
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
        - name: ME_CONFIG_MONGODB_SERVER
          valueFrom: 
            configMapKeyRef:
              name: mongodb-configmap
              key: database_url


--> kubectl apply -f mongo-express.yaml

--> kubectl get pods

--> kubectl logs podname


Now lets create a Service for mongoexpress

vi mongo-express.yaml and add service in same file

---
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer  
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 30000


--> kubectl apply -f mongo-express.yaml

--> kubectl get service

--> http://loadbalancer:8081

username: admin
password: pass

Note: if you don't want to expose with 8081 change port : 8081 to port : 80


================================

vi mongo.yaml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom: 
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
spec:
  selector:
    app: mongodb
  ports:
    - protocol: TCP
      port: 27017
      targetPort: 27017
===================
INGRESS
===================

Ingress is a service to expose application, but we already have cluster ip, node port and load balancer , let see

Ingress helps to expose HTTP and HTTPS routes from outside of the CLuster
Ingress supports Host based routing and path based routing
ingress supports load balancing and SSL termination
IT redirect the incoming requests to the right services based on the web url or path in the address
ingress provides encryption feature and helps to balance the load of the applications

Explain Host based and Path based
-----------------------------------
Host Based Routing: ex: boom.com, web.boom.com, admin.boom.com
Path based routing: boom.com/hello , boom.com/admin, Paytm.com/movies, Paytm.com/recharge etc

but services like load balancer, cluster ip, node port etc donest have these features

General load balancer routes the traffic based on ports and cant handle URL based routing

kubectl get ing --> shows ingress service , no ingress service

To install ingress, firstly we have to install nginx ingress controller:
command:

-- kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.0/deploy/static/provider/cloud/deploy.yaml

kubectl get pods
kubectl get deploy
kubectl get svc
kubectl get ingress
kubectl get service


if required
-----------
kubectl delete svc internetbanking
kubectl delete svc mobilebanking
kubectl delete deploy internetbanking




------------------------------------------------

vi httpd.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd  
spec:
  replicas: 1
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd
        ports:
        - containerPort: 80
        env:
        - name: TITLE
          value: "APACHE APP2"
---
apiVersion: v1
kind: Service
metadata:
  name: httpd  
spec:
  type: ClusterIP
  ports:
  - port: 80
  selector:
    app: httpd

----------------------------------------

vi nginx.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx  
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        env:
        - name: TITLE
          value: "NGINX APP1"
---
apiVersion: v1
kind: Service
metadata:
  name: nginx  
spec:
  type: ClusterIP
  ports:
  - port: 80
  selector:
    app: nginx


------------------------------------




vi ingress.yml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: k8s-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /nginx(/|$)(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: nginx
                port:
                  number: 80
          - path: /httpd(/|$)(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: httpd
                port:
                  number: 80
          - path: /(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: nginx
                port:
                  number: 80

----------------------------------------------------

kubectl apply -f httpd.yml

kubectl apply -f nginx.yml

kubectl apply -f ingress.yml


-- kubectl get deploy
-- kubectl get svc
-- kubectl get pods
-- kubectl get ingress

-- kubectl get services ingress-nginx-controller --namespace=ingress-nginx



Allow All traffic in node security group

Take the ELB and put in the browser http://elb/nginx and http://elb/httpd



Now update vi httpd.yml --> image to trainerreyaz/wordcounter
update vi nginx.yml --> image to trainerreyaz/ib-image:latest

-- kubectl apply -f httpd.yml

-- kubectl apply -f nginx.yml

-- kubect get pods

http://elb/nginx
http://elb/httpd


-- kubectl delete services ingress-nginx-controller --namespace=ingress-nginx


Key Components:
===============
Annotations:
------------
nginx.ingress.kubernetes.io/ssl-redirect: "false": Disables automatic redirection from HTTP to HTTPS.​
nginx.ingress.kubernetes.io/use-regex: "true": Enables the use of regular expressions in path matching. ​
nginx.ingress.kubernetes.io/rewrite-target: /$2: Rewrites the matched URI to the specified target.​

Rules:
------
Traffic matching /nginx, optionally followed by a / or end of the string, and any subsequent characters ((.*)), is directed to the nginx service.​
Traffic matching /httpd, following the same pattern, is directed to the httpd service.​
All other traffic (/(.*)) is directed to the nginx service.​
Considerations:

Path Matching:
-------------
The pathType: ImplementationSpecific allows the Ingress controller to interpret the path matching rules, including regular expressions. Ensure that your Ingress controller supports this path type and regex patterns.
 ​
Rewrite Target:
---------------
The nginx.ingress.kubernetes.io/rewrite-target: /$2 annotation rewrites the incoming request path to the specified target before forwarding it to the backend service. Ensure that this behavior aligns with your application's routing logic.

=================================================================================================================



PODS SCHEDULING
===============

In Kubernetes, Node Selector,Node Affinity,  Taints and Tolerations and are mechanisms that influence how Pods are scheduled onto Nodes within a cluster.

Node Selector
Node Affinity
Taints and Tolerations

1. Node Selector
------------------
NodeSelector is the simplest form of node selection constraint, allowing Pods to be scheduled only on Nodes with specific labels. By specifying a NodeSelector in a Pod's specification, you can ensure that the Pod runs only on Nodes that match the given label criteria.
NodeSelector: Use when you have simple, specific constraints for Pod placement based on Node labels

In the below manifest file, we are creating 2 pods and those pods should be scheduled in ib-node labeled node.

vi nodeselector.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 2
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      nodeSelector:
        node-name: ib-node
      containers:
      - name: cont1
        image: reyadocker/internetbankingrepo:latest


-- kubectl apply -f nodeselector.yml

-- kubectl get pods   --- Pods are not getting created because there is no label to the node called node-name: ib-node

-- kubectl describe pod ib-deployment-7784c9bfb5-8sl8d  

     Warning  FailedScheduling  2m40s  default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.

-- kubectl get nodes

-- kubectl edit node i-043783332483bf9f8

      under labels add  --- node-name: ib-node / We can use command also "kubectl label nodes i-043783332483bf9f8 node-name=ib-node "


-- kubectl get pods  -- Now pods are running

we are forcing kube-schedular to schedule pod on particular node. Its hard match. If it doesn't match, Kube scheduler will not schedule the pod
so pod will be in pending state    

-- kubectl delete deploy ib-deployment

2: Node Affinity
================
Node Affinity is a more expressive way to specify rules about the placement of pods relative to nodes' labels. It allows you to specify rules that apply only if certain conditions are met. Same as Node Selector but this has flexible way, if matches do it, if not schedule pod on any another node.

Two types
----------
1.Preferred during scheduling Ignore during execution (soft rules) : good if happen
2.Required during scheduling Ignore during execution (hard rules) : Must happen  : Same as NodeSelector

The node affinity syntax supports the following operators: In, NotIn, Exists, DoesNotExist, Gt, Lt, etc.


Preferred:
---------

vi preferred.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      affinity:
       nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
           matchExpressions:
           - key: node-name
             operator: In
             values:
             - mb-node
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80


weight: 1: Indicates the preference weight. Higher values signify stronger preferences.​



we haven't set the label for another node as node-name: mb-node

-- kubectl apply -f preferred.yml

-- kubectl get pods  --> 1 pod running. Why ? even i mention node-name: mb-node, No node has this label yet, but as we used preferred configuration, scheduler has scheduled on other nodes.

-- kubectl get pods -o wide   --> note down node-id , it might show on same node, but scheduler has choosen

-- kubectl delete deploy ib-deployment

-- vi preffered.yml

   change node-name: ib-node

-- kubectl apply -f preferred.yml

-- kubectl get pods -o wide  --- it should schedule on actual labeled node

-- kubectl delete -f preferred.yml

Required
========

vi required.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      affinity:
       nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
         nodeSelectorTerms:
         - matchExpressions:
           - key: node-name
             operator: In
             values:
             - mb-node
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

-- kubectl create -f required.yml

-- kubectl get pods -o wide   [Pod are not running, it is in pending state because, required is mb-node, but no node has mb-node]

-- kubectl delete -f required.yml

TAINTS and TOLERANCE
====================

In Kubernetes, taints and tolerations work together to control the scheduling of Pods onto Nodes. Taints are applied to Nodes to prevent certain Pods from being scheduled on them, while tolerations are applied to Pods to allow them to be scheduled on Nodes with matching taints. ​

Example Scenario: Dedicated Nodes for Specific Workloads
---------------------------------------------------------
Suppose you have a Kubernetes cluster with Nodes equipped with specialized hardware, such as GPUs, intended exclusively for machine learning workloads. To ensure that only Pods requiring GPU resources are scheduled on these Nodes, you can use taints and tolerations.


-- kubectl edit node i-0e6b67bf1b00383be  [Edit control plane, this master node has taint no schedule, thats the reason master node will not have pods ]

Apply a Taint to GPU Nodes
--------------------------

First, taint the GPU Nodes to repel/force Pods that do not require GPU resources:

-- kubectl get nodes

-- kubectl taint nodes i-0333b24c25bf4868b hardware=gpu:NoSchedule

This command adds a taint with key hardware, value gpu, and effect NoSchedule to the specified Node. As a result, Pods without a matching toleration will not be scheduled on this Node.


Lets test:
-------

vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
        - name: cont1
          image: reyadocker/internetbankingrepo:latest


-- kubectl create -f deploy.yml

-- kubectl get pods -o wide  [all pods are scheudled on another node]

Add a Toleration to GPU-Requiring Pods
--------------------------------------

Next, add a toleration to the Pods that require GPU resources, allowing them to be scheduled on the tainted Nodes:

vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ib-deployment
  labels:
    app: bank
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bank
  template:
    metadata:
      labels:
        app: bank
    spec:
      containers:
        - name: cont1
          image: reyadocker/internetbankingrepo:latest
      tolerations:
        - key: "hardware"
          operator: "Equal"
          value: "gpu"
          effect: "NoSchedule"


In this Pod specification, the toleration matches the taint applied to the GPU Nodes, permitting the Pod to be scheduled on those Nodes.


Key Points:
----------
Taints are applied to Nodes to repel certain Pods. They consist of a key, value, and effect (NoSchedule, PreferNoSchedule, or NoExecute). ​

Tolerations are applied to Pods to allow them to be scheduled on Nodes with matching taints. They must match the key, value, and effect of the taint to be effective.


Pod Affinity
=============
Pod affinity allows users to specify which pods a pod should or should not be scheduled with based on labels. For example, you can use pod affinity to specify that a pod should be scheduled on the same node as other pods with a specific label, such as app=database.

SUMMARY
=======

Taint should be used when you want to mark a node as unavailable for certain pods. For example, you can use taint to mark a node as "maintenance" and prevent pods from being scheduled on the node while it is undergoing maintenance.

Node selector is a simpler and more primitive mechanism compared to node affinity and is sufficient for many use cases.

Node affinity should be used when you want to specify which nodes a pod should or should not be scheduled on based on node labels. Node affinity provides more fine-grained control over pod scheduling compared to node selector and allows you to specify complex rules for pod scheduling based on multiple node labels.

Pod affinity allows us to set priorities for which nodes to place our pods based off the attributes of other pods running on those nodes. This works well for grouping pods together in the same node.

Pod anti-affinity allows us to accomplish the opposite, ensuring certain pods don’t run on the same node as other pods. We are going to use this to make sure our pods that run the same application are spread among multiple nodes. To do this, we will tell the scheduler to not place a pod with a particular label onto a node that contains a pod with the same label


https://blog.devops.dev/taints-and-tollerations-vs-node-affinity-42ec5305e11a



==================================================================================================================

HELM:

In K8S Helm is a package manager to install packages
in Redhat: yum & Ubuntu: apt & K8s: helm

it is used to install applications on clusters.
we can install and deploy applications by using helm
it manages k8s resources packages through charts
chart is a collection of files organized on a directory structure.
chart is collection of manifest files.
a running instance of a chart with a specific config is called a release.
The Helm client and library is written in the Go programming language.
The library uses the Kubernetes client library to communicate with Kubernetes.


ARCHITECURE:
1. HEML REPOSITORY: IT HAS AL HELM REPOS WHICH IS PUBLICALLY AVAILABLE
2. HEML CLIENT: DOWNLOADS HELM CHARTS FORM HELM REPOS.
3. API SERVER: DOWNLOADED HELM CHARTS WILL BE EXECUTED ON CLUSTER WITH API SERVER.

===========
ARGOCD - to do deployment in K8S Cluster
=========

GitOps continuous delivery tool for Kubernetes

It automates the deployment of applications to Kubernetes clusters,

Pipeline is not the best method for K8s, use ARGOCD




INTRO:
ArgoCD is a declarative continuous delivery tool for Kubernetes. ArgoCD is the core component of Argo Project.
It helps to automate the deployment and management of applications in a K8s cluster. It uses GitOps methodology to manage the application lifecycle and provides a simple and intuitive UI to monitor the application state, rollout changes, and rollbacks.
With ArgoCD, you can define the desired state of your Kubernetes applications as YAML manifests and version control them in a Git repository.
ArgoCD will continuously monitor the Git repository for changes and automatically apply them to the Kubernetes cluster.
ArgoCD also provides advanced features like application health monitoring, automated drift detection, and support for multiple environments such as production, staging, and development.
It is a popular tool among DevOps teams who want to streamline their Kubernetes application deployment process and ensure consistency and reliability in their infrastructure.


Setup KOPS first and with the helm we will setup ARGOCD

Setup KOPS

Install HELM
---------------
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

Install ARGOCD using HELM
---------------------------
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

kubectl create namespace argocd
helm install argocd argo/argo-cd --namespace argocd
kubectl get all -n argocd

EXPOSE ARGOCD SERVER:
--------------------
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

yum install jq -y

//export ARGOCD_SERVER='kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname''

//echo $ARGOCD_SERVER

kubectl get svc argocd-server -n argocd -o json | jq --raw-output .status.loadBalancer.ingress[0].hostname

The above command will provide load balancer URL to access ARGO CD


TO GET ARGO CD PASSWORD:
------------------------
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d



export ARGO_PWD='kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d'

echo $ARGO_PWD

The above command to provide password to access argo cd


Open ArgoCD load balancer

username: admin and password from above command

create app --> Application Name --> bankapp --> Project Name --> default --> Sync Policy --> Automatic --> Repository --> https://github.com/ReyazShaik/ar-deploy.git --> Path --> ./ --> CLuster URL --> NameSpace --> default

kubectl get po  --> it created automatically from argocd

kubectl get svc  --> copy paste the elb on browser

now modify replica as 5 in GitHub deploy.yml , automatically argocd will deploy

History and RollBack
------

Click on history and roll back --> three dots --> rollback
======================================================================================================================================================================================================================
                                                                                         **PROMETHOUS AND GRAFANA**
What is Observability?
======================
Here we will see the concept of Observability which is a very important aspect for all DevOps Engineers. In DevOps, observability is referred to the software tools and methodologies that help Dev and Ops teams to log, collect, correlate, and analyze massive amounts of performance data from a distributed application and glean real-time insights. Observability is crucial for maintaining the reliability and availability of software systems. It can be divided into three main components namely :

· Monitor

· Logs

· Alerts


What is Grafana ?
====================
Grafana was built on the principle that data should be accessible to everyone in your organization, not just the single Ops person.

By democratizing data, Grafana helps to facilitate a culture where data can easily be used and accessed by the people that need it, helping to break down data silos and empower teams.

With Grafana, you can take any of your existing data- be it from your Kubernetes cluster, raspberry pi, different cloud services, or even Google Sheets- and visualize it however you want, all from a single dashboard.

Grafana = Visualization Purpose

What is Prometheus?
===================

Prometheus is an open-source tool for monitoring and alerting systems. It collects and stores metrics over time, and can be used to monitor applications, infrastructure, and more

What is Grafana Loki?
=====================
Loki is a horizontally scalable, highly available, multi-tenant log aggregation solution. It’s designed to be both affordable and simple to use. Rather than indexing the contents of the logs, it uses a set of labels for each log stream.

Loki is Simply Log Aggregation tool
Prometheus is a metrics monitoring tool.
Loki’s design is inspired by Prometheus but for logs.

What is Promtail?
================

Promtail is an agent that ships the logs from the application to Loki. In Simple words, Loki acts as a data source and provides logs to Grafana whereas Promtail captures logs from various sources.


Monitoring
==========

We need monitoring server to monitor all servers and their metrics

There are many like

nagio, kibana, elk, jabbics, Grafana, new-relic, Prometheus , Splunk


PROMETHEUS:
============
Prometheus is an open-source monitoring system that is especially well-suited for cloud-native environments, like Kubernetes.
It can monitor the performance of your applications and services.
it will sends an alert you if there are any issues.
It has a powerful query language that allows you to analyze the data.
It pulls the real-time metrics, compresses and stores in a time-series database. Prometheus is a standalone system, but it can also be used in conjunction with other tools like Alertmanager to send alerts based on the data it collects.
it can be integration with tools like PagerDuty, Teams, Slack, Emails to send alerts to the appropriate on-call personnel.
it collects, and it also has a rich set of integrations with other tools and systems. 

For example, you can use Prometheus to monitor the health of your Kubernetes cluster, and use its integration with Grafana to visualize the data it collects.


SYNOPSIS:
==========

PROMETEUS:
its a free & opensource monitoring tool it collects metrics of nodes
it store metrics on time series database we use PromQL language
we can integrate promethus with tools like pagerduty, slack and email to send notifications PORT: 9090

GRAFANA:
its a visualization tool used to create dashboard.
Datasource is main component (from where you are getting data) Prometheus will show data but cant create dashboards
Dashboards: create, Import
Prometheus will show data but cannot create dashboards
we can integrate Grafana with tools like
pagerduty, slack and email to send notifications
PORT: 3000

username: admin, password: admin

Launch Amazonlinux2 instance call as Promotheus&Grafana Server, from this server we will create K8S CLuster and monitor all those

Setup KOPS cluster first , amazon Linux 2

vim .bashrc
export PATH=$PATH:/usr/local/bin/
source .bashrc

vi kops.sh

#vim .bashrc
#export PATH=$PATH:/usr/local/bin/
#source .bashrc


#! /bin/bash
aws configure
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

aws s3api create-bucket --bucket reyaz-kops-testbkt123.k8s.local --region ap-south-1 --create-bucket-configuration LocationConstraint=ap-south-1
aws s3api put-bucket-versioning --bucket reyaz-kops-testbkt123.k8s.local --region ap-south-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://reyaz-kops-testbkt123.k8s.local
kops create cluster --name reyaz.k8s.local --zones ap-south-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name reyaz.k8s.local --yes --admin


run the below commands

export KOPS_STATE_STORE=s3://reyaz-kops-testbkt123.k8s.local
kops validate cluster --wait 10m
kops update cluster --name reyaz.k8s.local --yes --admin
kops rolling-update cluster



if you want to delete :  kops delete cluster --name reyaz.k8s.local --yes


HELM:
=====

In K8S Helm is a package manager to install packages
In RedHat: yum 
In Ubuntu: apt
In K8S: helm

It is used to install packages on cluster
we can install and deploy applications by using helm
it manages k8s resources packages through charts
chart is a collection of files organized in a directory structure
chat is a collection of manifest files
a Running instance of a chart with a specific config is called Release

Usually in K8S we use manifest file to deploy, but here we will convert manifest to helm chart and deploy

Install HELM
============

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

Install Metric Server
==============
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml

Steps to Install Prometheus
=========================

First add repositories 
-----------------------

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 
helm repo add grafana https://grafana.github.io/helm-charts

UPDATE HELM CHART REPOS:
=======================
helm repo update
helm repo list

CREATE PROMETHEUS NAMESPACE:
============================
kubectl get ns
kubectl create namespace prometheus
kubectl get ns

INSTALL PROMETHEUS:
----------------
helm install prometheus prometheus-community/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2" --set server.persistentVolume.storageClass="gp2"

kubectl get pods -n prometheus
kubectl get all -n prometheus


Create Namespace Grafana
-=====================

kubectl create namespace grafana


Install Grafana
==============

helm install grafana grafana/grafana --namespace grafana --set persistence.storageClassName="gp2" --set persistence.enabled=true --set adminPassword='Root123456' --set service.type=LoadBalancer

kubectl get pods -n grafana
kubectl get service -n grafana

Copy the Instance-IP and paste in browser 
username:admin, password = Root123456

Check SG , nodes and ELB


Go to Grafana Dashboard --> Add the DataSource --> Select Prometheus and below URL, this URL is same in K8s for all
http://prometheus-server.prometheus.svc.cluster.local/


https://grafana.com/grafana/dashboards/   --- all dashboard id's are here

Import Grafana dashboard --> New --> Import --> 6417 --> load --> select Prometheus --> import 

New --> Import --> 315 --> load --> select Prometheus --> import 

Add 1860 port to monitor Nodes individually

11454 --> pv and pvc
747 --> pod metrics
14623 --> k8s overview -- use this
10907 --> monitor api-server

systemctl start grafana-server.service ---  to start grafana service or see in script






NODEXPORTER: will help to collect the data from workernodes

install NodeExporter
============

vi nodeexp.sh

#NODEEXPORTER
wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz
tar -xf node_exporter-1.5.0.linux-amd64.tar.gz
sudo mv node_exporter-1.5.0.linux-amd64/node_exporter  /usr/local/bin
rm -rv node_exporter-1.5.0.linux-amd64*
sudo useradd -rs /bin/false node_exporter

sudo cat <<EOF | sudo tee /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
EOF

sudo cat /etc/systemd/system/node_exporter.service
sudo systemctl daemon-reload  && sudo systemctl enable node_exporter
sudo systemctl start node_exporter.service && sudo systemctl status node_exporter.service --no-pager


sh nodeexp.sh

http://ip:9100 -- to see if nodeexporter is working
Monitoring EC2 instances with Promotheus and Grafana
=====================================================


Grafana: For visualizing metrics and logs.
Loki: For aggregating and storing logs.
Promtail: agent for logs
Prometheus: For collecting metrics.
Logstash: For log processing and forwarding.


Launch 3 EC2 instance Amazon Linux 2

Monitoring Server
Worker-Node1
Worker-Node2


Prometheus = 9090
graphana = 3000
node exporter = 9100 
loki = 3100/metrics




Install Prometheus and grafana in main monitoring server
=========================================

The below script will install Promo, Grafana and NodeExporter

vi promo.sh  and sh promo.sh

#prometheus
wget https://github.com/prometheus/prometheus/releases/download/v2.43.0/prometheus-2.43.0.linux-amd64.tar.gz
tar -xf prometheus-2.43.0.linux-amd64.tar.gz
sudo mv prometheus-2.43.0.linux-amd64/prometheus prometheus-2.43.0.linux-amd64/promtool /usr/local/bin

Now, We need to Create directories for configuration files and other prometheus data.
sudo mkdir /etc/prometheus /var/lib/prometheus
sudo mv prometheus-2.43.0.linux-amd64/console_libraries /etc/prometheus
ls /etc/prometheus
sudo rm -rvf prometheus-2.43.0.linux-amd64*

#sudo vim /etc/hosts
#3.101.56.72  worker-1
#54.193.223.22 worker-2

sudo cat <<EOF | sudo tee /etc/prometheus/prometheus.yml
global:
  scrape_interval: 10s

scrape_configs:
  - job_name: 'prometheus_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'node_exporter_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9100','worker-1:9100','worker-2:9100']
EOF


sudo useradd -rs /bin/false prometheus
sudo chown -R prometheus: /etc/prometheus /var/lib/prometheus

 sudo ls -l /etc/prometheus/
sudo cat <<EOF | tee /etc/systemd/system/prometheus.service
[Unit]
Description=Prometheus
After=network.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries

[Install]
WantedBy=multi-user.target
EOF

sudo ls -l /etc/systemd/system/prometheus.service
sudo systemctl daemon-reload && sudo systemctl enable prometheus
sudo systemctl start prometheus && sudo systemctl status prometheus --no-pager

#GRAFANA
wget -q -O gpg.key https://rpm.grafana.com/gpg.key
sudo rpm --import gpg.key
sudo cat <<EOF | tee /etc/yum.repos.d/grafana.repo
[grafana]
name=grafana
baseurl=https://rpm.grafana.com
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://rpm.grafana.com/gpg.key
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
EOF

exclude=*beta*
yum install grafana -y
systemctl start grafana-server.service
systemctl status grafana-server.service

#NODEEXPORTER
wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz
tar -xf node_exporter-1.5.0.linux-amd64.tar.gz
sudo mv node_exporter-1.5.0.linux-amd64/node_exporter  /usr/local/bin
rm -rv node_exporter-1.5.0.linux-amd64*
sudo useradd -rs /bin/false node_exporter

sudo cat <<EOF | sudo tee /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
EOF

sudo cat /etc/systemd/system/node_exporter.service
sudo systemctl daemon-reload  && sudo systemctl enable node_exporter
sudo systemctl start node_exporter.service && sudo systemctl status node_exporter.service --no-pager
==================================

http://13.233.215.35:9100 --> node exporter is working
http://13.233.215.35:9090 --> Promo is working

http://Ip:3000 --> Grafana, username: admin, password: admin

Go to Grafana Dashboard --> Add the DataSource --> Select Prometheus and below URL, this URL is same in K8s for all
http://13.233.215.35:9090 --> monitoring server Ip, where promo is installed


Import Grafana dashboard --> New --> Import --> 1860 --> load --> select Prometheus --> import 


Add worker-nodes now
===================

Install node-exporter, this is like agent, to collect metrics

vi node.sh

#NODEEXPORTER
wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz
tar -xf node_exporter-1.5.0.linux-amd64.tar.gz
sudo mv node_exporter-1.5.0.linux-amd64/node_exporter  /usr/local/bin
rm -rv node_exporter-1.5.0.linux-amd64*
sudo useradd -rs /bin/false node_exporter

sudo cat <<EOF | sudo tee /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
EOF

sudo cat /etc/systemd/system/node_exporter.service
sudo systemctl daemon-reload  && sudo systemctl enable node_exporter
sudo systemctl start node_exporter.service && sudo systemctl status node_exporter.service --no-pager


Go to http://13.233.215.35:9090/ --> promo and see the targets --> top --> Status --> Targets , worker nodes are down

go to monitoring server 

cat /etc/prometheus/prometheus.yml  --- this contains, workernode names, to add , go to /etc/hosts and add workernode IP's


vi /etc/hosts

13.127.130.98 worker-1
3.6.41.159 worker-2


http://13.233.215.35:9090/targets?search= ---> refresh and nodes are up

now monitor these through Grafana

http://3.233.215.35:3000/ --> Make sure dashboard is there --> top --> Host --> DropDown -- see workernodes


LOG Monitoring with Loki and Promtail
=====================================

Setup Loki and Promtail on Docker to save money ;-)

Install docker and start
------------------------
yum install -y docker
systemctl start docker


Create a directory for Grafana configs
----------------------------------
sudo mkdir grafana_configs
cd grafana_configs


# Download Loki Config
-----------------------
sudo wget https://raw.githubusercontent.com/grafana/loki/v2.8.0/cmd/loki/loki-local-config.yaml -O loki-config.yaml

# Download Promtail Config
----------------------------
sudo wget https://raw.githubusercontent.com/grafana/loki/v2.8.0/clients/cmd/promtail/promtail-docker-config.yaml -O promtail-config.yaml


# Run Loki with Docker
-----------------------
docker run -d --name loki -v $(pwd):/mnt/config -p 3100:3100 grafana/loki:2.8.0 --config.file=/mnt/config/loki-config.yaml

Check the status using http://your-ec2-instance-ip:3100/metrics

Check the status using http://your-ec2-instance-ip:3100/ready


#RUN Run Promtail with docker
-----------------------------

docker run -d --name promtail -v $(pwd):/mnt/config -v /var/log:/var/log --link loki grafana/promtail:2.8.0 --config.file=/mnt/config/promtail-config.yaml


Link Loki to Grafana
----------------------
· In Grafana, navigate to Configuration > Data Sources.

· Add a new data source and select Loki.

· Enter the Loki URL (http://ec2-public:3100) and save the configuration.

Now, our datasource is connected.

Link Loki to Grafana

· In Grafana, navigate to Configuration > Data Sources.

· Add a new data source and select Loki.

· Enter the Loki URL (http://localhost:3100) and save the configuration.

Now, our datasource is connected



Step 9 — Explore and filter logs
--------------------------------

· Use the Explore tab in Grafana to view logs.

· Select the Loki data source , label filters = jobs, varlogs --> run query

Show Grafana logs
------------------

· Add the Grafana logs path (/var/logs/grafana) to the Promtail config file

vi promtail-config.yaml



  - targets:
      - localhost
    labels:
       job: grafanalogs
     __path__: /var/log/grafana/*log


-- docker ps
-- sudo docker restart promtail-container-id

Go to Grafana --> Dashboard --> New Dashboard --> Add Visualization --> DataSOurce = Grafana --> click Query inspector --> Refresh
Select Loki as datasource --> 


ALERTS
=====

browser -- > my google account > security -- > two setp verification --> App passwords copy password

vi /etc/grafana/grafana.ini line 892/1034
[smtp]
enabled = true
host = smtp.gmail.com:587
user = trainerreyaz@gmail.com
# If the password contains #or; you have to wrap it with triple quotes. Ex """#pa 897 password=xhfa drlb zgwm ogey
cert_file =
;key_file =
skip_verify = true
from_address = trainerreyaz@gmail.com 
from_name = Grafana



systemctl restart grafana-server.service
systemctl status grafana-server.service



Grafana --> Alert Rule --> contact point -- > Create --> name -- > email -- > test --> check gmail

Alerting rules --> new -- > set -- > name: Grafana -- > Metric: node_cpu_seconds_total --> instace > localhost:9100-->

Input: A & Function: Last & Mode: Stric
Input: B & isabove: 0.7 (70%)
Folder: cpu
Evaluation group: cpu
Pending period: 30s


amazon-linux-extras install epel -y
yum install -y stress
run stress command
===================================================================================================================================================================================================================
                                                                                                      ** TERRAFORM**
Terraform
=========

History:
========

In AWS till now, we have created infrastructure manually , created EC2, ELB's, ASG, S3, RDS, VPC etc
If we create infra manually
1. Time consume
2. Mistakes
3. Tracking 

But if you want to repeat these creating infra multiple times? you need to automate creating infrastructure

AWS introduced a Service called CloudFormation in 2011. Its a very cool service to automate infrastructure using JSON and YAML.

A person who is very passionate about IT Infrastructure got impressed on the CF and started exploring and wrote a blog

On the way of exploring this CF, this person got a basic doubt/question . 

AWS has CloudFormation
Azure has Azure Resource Manager (ARM)
Google has Deployment Manager 

But do we have any COMMON tool to automate infrastructure for all cloud providers? (Cloud agnostic). So he raised this question in his post in a site called tumblr , but no one answered. He waited for many days to get answer

And as no one answered , this guy came up with the tool called Terraform. His name is Mitchell Hashimoto, the creator of HashiCorp Terraform.

In July of 2014 , Terraform 0.1 got released 

Terraform
---------
Terraform is an open-source Infrastructure as Code (IaC) tool developed by HashiCorp.

It allows users to define and provision infrastructure resources in a consistent, repeatable manner using a high-level configuration language known as HashiCorp Configuration Language (HCL)

It was a free and opensource too1 but now it is not opensource (check in google, opentofu is a forked version of terraform)

Year: 2014
Developed in: GO lang
Who: Mitchel Hashimoto
Owned: Hashicorp

Alternatives
-----------

PULUMI
ANSIBLE  --- is mainly used for configuration management, TF will create infra
CHEF
PUPPET
OpenTofu


TF VS Ansible
=============

TF will create infra and these servers will be orchestrated/configured by Ansible

Terraform Structure
-------------------

Mainly most of the people use 3 files in Terraform

main.tf:    contains all providers, resources and data sources
variables.tf: contains all defined variables
output.tf:    contains all output resources


The issue with this structure is that most logic is stored in the single main.tf file which therefore becomes pretty complex and long. 

Terraform, however, does not mandate this structure, it only requires a directory of Terraform files. 
Filenames do not matter to Terraform . To make it simple we prefer the following structure 


projectname/
    |
    |-- provider.tf  - Plugin to connect to Cloud , search in google, TF providers --> AWS --> Use provider
    |-- version.tf - Sets required Terraform and provider versions.
    |-- backend.tf - this file is used to configure the backend, which determines how and where Terraform stores its state data.
    |-- main.tf  - Contains the core resource definitions
    |-- variables.tf - Declares input variables.​
    |-- terraform.tfvars - Assigns values to variables
    |-- outputs.tf


provider.tf: contains the terraform block and provider block (AWS, Azure etc)
data.tf: contains all data sources
variables.tf: contains all defined variables
locals.tf: contains all local variables
output.tf: contains all output resources



Installing Terraform on Amazon Linux 2
======================================

Launch Amazon  Linux 2 instance and attach a role with admin permissions

sudo yum update -y
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum install -y terraform
terraform version


Installing Terraform on Ubuntu
==============================

apt update -y
apt install awscli -y
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform
terraform -v

-- mkdir terraform

-- cd terraform


---------- #####Terraform Commands ### --------

terraform –version	Shows terraform version installed

### Initialize infrastructure ###

terraform init	                :Initialize a working directory, it will download the providers plugins
terraform plan	                :Creates an execution plan (dry run)
terraform apply	                :Executes changes to the actual environment
terraform apply –auto-approve	:Apply changes without being prompted to enter ”yes”
terraform destroy –auto-approve :Destroy/cleanup without being prompted to enter ”yes”


All tf code should be written in tf files, extension is .tf , also called as configuration files

Main things in TF file
---------------------
Blocks
Labels
argument


A tf file has blocks, for ex: provider is a block, "aws" is a label, for single block we can have multiple labels 0, or 1 or 2 etc, optional

what ever we write in {} is called arguments, arguments can called as input for the block, argument need to have key and value

For provider we have one label aws, but for resource block has 2 labels, it can be multiple labels
_ is called identifier, if you want to combine 2 words use _ 

we cannot use aws instance (space between) so use aws_instance , instance_type 

First TF Example:
----------------

vi main.tf

provider "aws" {
region = "ap-south-1"
}

provider is block
aws is label
{} inside is arguments

-- terraform fmt    [used to format your configuration files into a canonical format and style]

terraform fmt -recursive  -- for all files 

-- terraform init

Whenever you have a new or existing Terraform directory (containing your Terraform configuration files), you need to run terraform init to prepare that directory for other Terraform commands.

Provider Plugins: Terraform uses plugins to interface with cloud providers (like AWS, Azure, Google Cloud, etc.). The init command checks the configuration files to see which providers you're using and fetches the required provider plugins.

Provider Versions: If you’ve specified a particular version of a provider in your configuration, terraform init will download that version. If not, it'll get the latest compatible version.


-- ls -al
 
-- cd	.terraform  ---> Navigate and see aws plugin with version

-- cd 


vi main.tf

provider "aws" {
region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
ami = "ami-02ddb77f8f93ca4ca"
instance_type = "t2.micro"
}

No need to do again terraform init, because provider already downloaded and we didn't changed in that block

-- terraform validate
-- terraform plan    ------ attach a IAM role to EC2
-- terraform apply
-- terraform apply --auto-approve

+	: Creating
-	: Deleting
~	: Update

I - Init
P - Plan
A - Apply
D - Destroy


.terraform contains lots of information (providers plugins will be stored in this directory)

-- cd .terraform

STATE FILE
==========

###### What is state and why is it important in Terraform? #########

“Terraform must store state about your managed infrastructure and configuration. This state is used by Terraform to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. This state file is extremely important; it maps various resource metadata to actual resource IDs so that Terraform knows what it is managing. This file must be saved and distributed to anyone who might run Terraform.”

Local State and Remote State:
---------------------------

“By default, Terraform stores state locally in a file named terraform.tfstate. When working with Terraform in a team, use of a local file makes Terraform usage complicated because each user must make sure they always have the latest state data before running Terraform and make sure that nobody else runs Terraform at the same time.”

“With remote state, Terraform writes the state data to a remote data store, which can then be shared between all members of a team.” Ex : S3

-- cat terraform.tfstate

-- terraform state list 
     Terraform command used to list all the resources that are currently being tracked in the Terraform state file

-- terraform destroy --auto-approve

.terraform.lock.hcl
===================

When you run terraform init, Terraform downloads the required providers and dependencies and generates the .terraform.lock.hcl file if it doesn't already exist. If the file does exist, Terraform checks the versions specified in the lock file and installs those versions.

The .terraform.lock.hcl file is a lock file used by Terraform to manage the dependencies of your Terraform project. It ensures that the same versions of provider plugins and modules are used every time you run Terraform, making your infrastructure deployments more predictable and consistent. 

Purpose: Dependency Management, Consistency and Security

.terraform.lock.hcl


Current State vs Desired State
=============================

The current state represents the actual state of your infrastructure resources as they exist in the real world (e.g., in your cloud provider). Terraform tracks the current state of your infrastructure in a state file (terraform.tfstate).

The desired state is what you define in your Terraform configuration files. It represents the infrastructure that you want Terraform to create, update, or destroy.You define the desired state using HashiCorp Configuration Language (HCL) in .tf files

Example: Launching 5 EC2 instances using - COUNT Argument
=========================================================

provider "aws" {
region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
count = 5
ami = "ami-0492447090ced6eb5"
instance_type = "t2.micro"
}

terraform apply --auto-approve

terraform state list

==============================================
TARGET =  is used to delete a specific resource
==============================================
Single Target:
-------------

-- terraform destroy --auto-approve -target=aws_instance.myinstance[0]

Multi Target:
------------

-- terraform destroy --auto-approve -target=aws_instance.one[1] -target=aws_instance.one[2]

terraform state list

terraform destroy --auto-approve
VARIABLES
=============
In Terraform, variables are used to make configurations more dynamic and reusable

Terraform variables are a core feature that allows you to parameterize your Terraform configurations.

Its a block in terraform used to define the variables

By using variables, you can make your code more flexible, reusable, and easier to manage.

Variables enable you to customize your Terraform deployments without hardcoding values directly into your configuration files.

Types of Terraform Variables
****************************

Input Variables: These allow you to pass values into Terraform configurations. They are defined using the variable block.
---------------

Output Variables: These are used to return values from your Terraform configurations after they have been applied, often used for
----------------  sharing data between different configurations or modules.

Local Variables: These are used to assign values to an expression or value within a configuration for reuse, improving readability and
---------------- maintainability.


Variable Types
--------------
Terraform supports several types of variables:

string:  A sequence of characters ("hello", "world")
------

number:  Any numeric value (10, 3.14, -5)
------

bool:    A boolean value (true or false).
-----

list(type):  An ordered list of elements (["a", "b", "c"])
----------

map(type):  A key-value pair mapping ({ key1 = "value1", key2 = "value2" })
---------

set(type):  A unique unordered collection of elements (["a", "b", "c"])
---------

object({...}): A structured object with named attributes
-------------

tuple([types]): A fixed sequence of elements with different types
--------------

List Examples:
-----------

length(list): Returns the number of elements in a list.
-------------------------------------------------------


variable "fruits" {
  type    = list(string)
  default = ["apple", "banana", "cherry"]
}

output "list_length" {
  value = length(var.fruits)
}


concat(list1, list2, ...): Combines multiple lists into a single list.
---------------------------------------------------------------------

variable "fruits" {
  type    = list
  default = ["apple", "grapes"]
}

variable "vegies" {
  type    = list
  default = ["onions", "tomatoes"]
}

output "combined_list" {
  value = concat(var.fruits, var.vegies)
}

element(list, index): Returns the element at the specified index in a list.
----------------------------------------------------------------------------

variable "fruits" {
  type    = list
  default = ["apple", "banana", "cherry"]
}

output "selected_element" {
  value = element(var.fruits, 1)
}



zipmap(key, value): Creates a map from a list of keys and a list of values.
-----------------------------------------------------------------------

variable "keys" {
  type    = list(string)
  default = ["name", "age"]
}

variable "values" {
  type    = list(any)
  default = ["Alice", 30]
}

output "my_map" {
  value = zipmap(var.keys, var.values)
}


lookup(map, key): Retrieves the value associated with a specific key in a map.
-----------------------------------------------------------------------------
variable "my_map" {
  type    = map(string)
  default = {"name" = "Venky", "age" = "50"}
}

output "value" {
  value = lookup(var.my_map, "name")
}


join(separator, list): Joins the elements of a list into a single string using the specified separator.
------------------------------------------------------------------------------------------------------

variable "fruits" {
  type    = list
  default = ["apple", "banana", "cherry"]
}

output "joined_string" {
  value = join(", ", var.fruits)
}


When order matters or duplicates are allowed

set Example : TO print unique values
-----------

variable "unique_names" {
  type    = set(string)
  default = ["Alice", "Bob", "Charlie", "Bob"]
}
output "unique_names_list" {
  value = tolist(var.unique_names)
}


When uniqueness is required, and order doesn't matter



MAP Example : key value
------------

variable "instance_tags" {
  type = map(string)
  default = {
    Name = "WebServer"
    Env  = "Production"
  }
}

# Output the entire map
output "instance_tags_output" {
  description = "Displays the instance tags as a map"
  value       = var.instance_tags
}

# Output a specific tag value (e.g., "Name")
output "instance_name" {
  description = "Displays the 'Name' tag from the instance tags"
  value       = var.instance_tags["Name"]
}

# Output all tag keys separately
output "tag_keys" {
  description = "Displays only the keys from the instance tags"
  value       = keys(var.instance_tags)
}

# Output all tag values separately
output "tag_values" {
  description = "Displays only the values from the instance tags"
  value       = values(var.instance_tags)
}


Tuple Example
-------------

tuple allows elements of different types in a fixed order.


variable "mixed_values" {
  type = tuple([string, number, bool])
  default = ["example", 42, true]
}

# Output the entire tuple
output "mixed_values_output" {
  description = "Displays the full tuple"
  value       = var.mixed_values
}

# Output individual elements from the tuple
output "first_element" {
  description = "First element (string)"
  value       = var.mixed_values[0]
}

output "second_element" {
  description = "Second element (number)"
  value       = var.mixed_values[1]
}

output "third_element" {
  description = "Third element (boolean)"
  value       = var.mixed_values[2]
}





Examples
========

Here in below code, we have 3 varibales: instance_count , instance_ami and instance_type , these we can put any name var.anyname
description = "*" and type = number are optional, use or dont use its same. count, ami and instance_type are predefined by Terraform cannot change.

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

variable "instance_count" {
  description = "*"
  type        = number
  default     = 3
}

variable "instance_ami" {

  description = "*"
  type        = string
  default     = "ami-0492447090ced6eb5"
}

variable "instance_type" {

  description = "*"
  type        = string
  default     = "t2.micro"
}

variable "instance_name" {

  description = "*"
  type        = string
  default     = "TF-Server"
}

resource "aws_instance" "myinstance" {
  count         = var.instance_count
  ami           = var.instance_ami
  instance_type = var.instance_type
      tags = {
    Name = var.instance_name
  }

}

-- terraform fmt
-- terraform plan
-- terraform apply --auto-approve

But its difficult to mange variables in single main.tf so now keep them in different configuration file called variables.tf
we no need to call varibles.tf in  main.tf, TF will automatically call the variable.tf file

Note: To delete multiple lines in vi, use ndd Ex:17dd : 17 lines will be deleted

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  count         = var.instance_count
  ami           = var.instance_ami
  instance_type = var.instance_type
  tags = {
    Name = var.instance_name
  }

}


vi variables.tf


variable "instance_count" {
  description = "*"
  type        = number
  default     = 1
}

variable "instance_ami" {

  description = "*"
  type        = string
  default     = "ami-0492447090ced6eb5"
}

variable "instance_type" {

  description = "*"
  type        = string
  default     = "t2.micro"
}
variable "instance_name" {

  description = "*"
  type        = string
  default     = "TF-Server"
}


-- terraform apply --auto-approve
-- terraform state list
-- terraform destroy --auto-approve

==============================
Variables Files .tfvar
============================

TERRAFORM TFVARS:
----------------

This file allows you to separate variable definitions from the main configuration, making it easier to manage different environments and keep your codebase clean and organized.

we use tfvar files when we have multiple configurations like (prod, dev and test)

Each configuration we can write on variable file and attach it while running.

terraform.tfvars the default name for a .tfvars file

You can also create custom-named .tfvars files like dev.tfvars, test.tfvars, prod.tfvars

Example
-------

No change in main.tf

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  count         = var.instance_count
  ami           = var.instance_ami
  instance_type = var.instance_type
  tags = {
    Name = var.instance_name
  }

}

** In variables.tf , remove all hardcoded values(default values) and keep it in separate tfvars file

vi variables.tf

variable "instance_count" {
  description = "*"
  type        = number
}

variable "instance_ami" {

  description = "*"
  type        = string
}

variable "instance_type" {

  description = "*"
  type        = string
}
variable "instance_name" {

  description = "*"
  type        = string
}

** Now create 3 different tfvars files like dev.tfvars, test.tfvars and prod.tfvars

vi dev.tfvars vi test.tfvars and vi prod.tfvars with different instance_name

vi dev.tfvars

instance_count = 1

instance_ami = "ami-0492447090ced6eb5"

instance_type = "t2.micro"

instance_name = "Dev-Server"

---------------

vi prod.tfvars

instance_count = 1

instance_ami = "ami-0492447090ced6eb5"

instance_type = "t2.micro"

instance_name = "Prod-Server"


-- terraform apply --auto-approve  -var-file="dev.tfvars"

-- terraform apply --auto-approve  -var-file="test.tfvars"  
   [ **This command now just rename Dev-Server to Test-Server as we are applying on same workspace(default), What is WorkSpace? will see ** ]
   [terraform workspace list] [we can create a separate workspace for all diff env]

-- terraform destroy --auto-approve  -var-file="test.tfvars"

-- terraform apply --auto-approve  -var-file="prod.tfvars"

-- terraform destroy --auto-approve  -var-file="prod.tfvars"

===========================================================================================
Another type Variable Usage : Command Line Flags: Terraform Command Line and Input Variable
===========================================================================================

If you don't provide any values on configurations file, TF will ask for values on Command Line or provide inputs to Command Line

For Example: Remove all dev.tfvars, prod.tfvars and test.tfvars

First cat Variables.tf - No values defined there in variables.tf

Command Line Flags: Terraform Command Line
------------------------------------------

-- terraform apply --auto-approve  **   [it will ask the values on Command Line]

-- terraform destroy --auto-approve  **   [it will ask the values on Command Line]

Input Variable
--------------

-- terraform apply --auto-approve -var="instance_type=t2.micro" -var="instance_count=1" -var="instance_name=test-server" -var="instance_ami=ami-0492447090ced6eb5"

-- terraform destroy --auto-approve -var="instance_type=t2.micro" -var="instance_count=1" -var="instance_name=test-server" -var="instance_ami=ami-0492447090ced6eb5"

-- In above destroy command, remove one variable and destroy, TF will ask in Command Line but if you don't give also, it will take from statefile and destroy

==============================================================
Environment Variables using EXPORT- we don't use much in real time
================================================================

To make it easy and understandable, lets remove all variables except ami in variables.tf(see variable name should be ami not instance_ami), otherwise need to export all variables

export TF_VAR_ami=ami-0492447090ced6eb5

vi variables.tf

variable "ami" {

  description = "*"
  type        = string
}


** In the main.tf, keep again the hardcoded values for easy purpose to see only AMI from Environment Variable

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  count         = 1
  ami           = var.ami
  instance_type = "t2.micro"
  tags = {
    Name = "reya-server"
  }

}

-- terraform apply --auto-approve
-- terraform destroy --auto-approve

===========================
TERRAFORM OUTPUT VARIABLE
===========================

These are used to return values from your Terraform configurations after they have been applied, often used for sharing data between different configurations or modules.

First remove variables.tf , just to make it simple

This block is used to print the resource outputs. In the output section value = aws_instance.myinstance.public_ip , aws_instance and myinstance is a label for resource

Example
-------

vi main.tf    

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "outputvarexample-server"
  }
}

output "instance-information" {
  value = [aws_instance.myinstance.public_ip, aws_instance.myinstance.private_ip, aws_instance.myinstance.public_dns]

}

-- terraform apply --auto-approve
   [** see the output of the above command on command line]

*** If you need all information about the EC2 instance in output  - value = aws_instance.myinstance
---------------------------------------------------------------------------------------------------

vi main.tf    

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "outputvarexample-server"
  }
}

output "instance-information" {
  value = aws_instance.myinstance

}

-- terraform apply --auto-approve
   [** see the output of the above command on command line]

-- terraform destroy --auto-approve
=========================================================
Another Examples of Using Variables  - Creating S3 Bucket
=========================================================

Create a S3 bucket from TF using complete variable structure

├── main.tf
├── provider.tf
├── terraform.tfvars
├── variables.tf
└── terraform.tfstate

Lets create main.tf (main code, calling values from variables.tf), provider.tf (providers information), terraform.tfvars(values) and
variables.tf (calling values from tfvars)

vi terraform.tfvars

mybucket = "tf-example-reyaz-s3-bkt"

--------------------------

vi provider.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.63.1"
    }
  }
}

provider "aws" {
  region = "ap-south-1"
}

---------------------------

vi variables.tf

variable "mybucket" {
  type        = string
  description = "This is my DevOps test bucket from TF"
  default     = ""

}

---------------------------

vi main.tf

resource "aws_s3_bucket" "example" {
  bucket = var.mybucket

}

-- terraform apply --auto-approve
-- terraform destroy --auto-approve











===========================================
Another Examples of Using Variables : Launching EC2 instance
===========================================
Launching EC2 instance from TF using complete variable structure

.
├── main.tf
├── output.tf
├── provider.tf
├── terraform.tfvars
└── variables.tf


---------------------------------

vi terraform.tfvars

 instance_ami = "ami-0492447090ced6eb5"
 instance_type ="t2.micro"
 key_name = "MyKey"
 name = "MyInstance"


-----------------------------------


vi variables.tf

variable "instance_ami" {
    type = string
    default = ""
 
}
variable "instance_type" {
    type = string
    default = ""
 
}
variable "key_name" {
    type = string
    default = ""
 
}
variable "name" {
  description = "The name of the EC2 instance."
  default = ""
}

-----------------------------------------------------

vi provider.tf

provider "aws" {
region = "ap-south-1"
}


----------------------------------------------------

vi main.tf

resource "aws_instance" "myinstance" {
    ami = var.instance_ami
    instance_type = var.instance_type
    key_name = var.key_name
    tags = {
      Name = var.name
    }
}


------------------------------------------------------

vi output.tf

output "instance_public_ip" {
    value = aws_instance.myinstance.public_ip
    sensitive = true
}

output "instance_id"{
    value = aws_instance.myinstance.id
}
output "instance_public_dns" {
    value = aws_instance.myinstance.public_dns
 
}
output "instance_arn" {
    value = aws_instance.myinstance.arn
 
}


-- terraform apply --auto-approve
   [** see the output of the above command on command line]

-- terraform destroy --auto-approve


------------------------------------


=================================================
TAINT: it is used to recreate specific resources in infrastructure.
================================================

Terraform taint command is used to manually mark a specific resource for recreation.
When you mark a resource as "tainted," it indicates to Terraform that the resource is in a bad or inconsistent state and should be destroyed and recreated during the next terraform apply operation.

When to Use : Failed Deployments, Manual Changes and Resource Corruption

Example:
------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "taint-server-example"
  }
}

resource "aws_s3_bucket" "mys3bucket" {
  bucket = "test-bkt-dkkfg-reya"
}

-- terraform apply --auto-approve
-- terraform state list   [This will show you the resources handles by statefile]

-- terraform taint aws_s3_bucket.mys3bucket

-- terraform apply --auto-approve    [This will now delete only S3 bucket and recreate it not EC2 as S3 bucket has marked as tainted]

Tainted Another Example
----------------------

-- terraform state list
-- terraform taint aws_instance.myinstance
-- terraform apply --auto-approve    [This will terminate the EC2 instance only and re-launch it as it is marked as tainted]


TO UNTAINT: terraform untaint aws_instance.myinstance

TERRAFORM REPLACE: can also use Replace
terraform apply --auto-approve  -replace="aws_instance.myinstance[0]"


=================
TERRAFORM LOCALS:
=================

In Terraform, locals are used to define and assign values to variables that are meant to be used within a module or a configuration block.

Unlike input variables, which allow values to be passed in from the outside, local values are set within the configuration itself and are used to simplify complex expressions, avoid repetition, and improve the readability of your Terraform code.

Example: Using Local Values:
--------
vi  main.tf

locals {
  project_name   = "My-Awesome-DevOps"
  environment    = "Students"
  instance_count = 2
  tags = {
    Name        = "${local.project_name}-${local.environment}"
    Environment = local.environment
  }
}
resource "aws_instance" "myinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  count         = local.instance_count
  tags          = local.tags
}


Example 2: Setup VPC, Subnet and EC2
---------

*** In the below example, it will create a VPC with name Prod-VPC, Subnet with Prod-Subnet, EC2 instance with Prod-EC2. Instead having Prod in every block, lets put it as a variable

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_vpc" "myvpc" {
  cidr_block = "192.168.0.0/16&quot;
  tags = {
    Name = "Prod-VPC"
  }
}

resource "aws_subnet" "subnet1" {
  vpc_id            = aws_vpc.myvpc.id
  cidr_block        = "192.168.1.0/24&quot;
  availability_zone = "ap-south-1a"
  tags = {
    Name = "Prod-Subnet"
  }
}

resource "aws_instance" "myinstance" {
  subnet_id     = aws_subnet.subnet1.id
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "Prod-Server"
  }
}


-- terraform apply --auto-approve
-- terraform destroy --auto-approve


Instead using PROD in every block lets make it as a local variable
-----------------------------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

locals {
env = "Prod"
}

resource "aws_vpc" "myvpc" {
  cidr_block = "192.168.0.0/16&quot;
  tags = {
    Name = "${local.env}-VPC"
  }
}

resource "aws_subnet" "subnet1" {
  vpc_id            = aws_vpc.myvpc.id
  cidr_block        = "192.168.1.0/24&quot;
  availability_zone = "ap-south-1a"
  tags = {
    Name = "${local.env}-Subnet"
  }
}

resource "aws_instance" "myinstance" {
  subnet_id     = aws_subnet.subnet1.id
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "${local.env}-Server"
  }
}

-- terraform apply --auto-approve
-- terraform destroy --auto-approve


=======================
TERRAFORM WORKSPACE:
======================


In Terraform, a workspace is an isolated environment where a separate state file is maintained.

This feature allows you to manage different environments (like development, staging, production) within the same Terraform configuration.

Each workspace has its own state, enabling you to deploy the same infrastructure to multiple environments without needing to duplicate the configuration files.

Key Concepts of Terraform Workspaces:
-------------------------------------

Isolation: Each workspace has its own state file. This means the resources managed by Terraform in one workspace are isolated from those in
---------  another workspace.

Use Cases: Workspaces are typically used for managing multiple environments (e.g., dev, staging, prod) within a single Terraform configuration.
----------

Default Workspace: When you first initialize a Terraform directory, it starts with a default workspace named default. You can switch to
-----------------  other workspaces or create new ones as needed.

Setting up environments using workspaces
Switching between environments
Using different configurations in each environment
Using different backends in each environment

NOTE:
-----
WE CANT DELETE CURRENT WORKSPACE.
BEFORE DELETING WORKSPACE WE NEED TO DELETE RESOURCES ON IT.
WE CANT DELETE DEFAULT WORKSPACE

All workspace statefiles are under directory terraform.tfstate.d

terraform workspace list    : to show list of workspace
terraform workspace new     : to Create and switch to workspace "dev"
terraform workspace show    : to show current workspace
terraform workspace select     : to switch blw workspaces
terraform workspace delete     : to delete the workspaces

Examples
--------

-- terraform workspace list  [It give workspace name as "default"]

-- terraform workspace new dev  [This will create a new Workspace called dev]

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

locals {
  instance_types = {
    dev   = "t2.micro"
    test  = "t2.small"
    prod  = "t2.medium"
  }
}
resource "aws_instance" "workspace-example" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = local.instance_types[terraform.workspace]
  tags = {
    Name = "${terraform.workspace}-server"
  }
}

-- terraform plan  [you should see t2.micro getting launched]

-- terraform workspace new test  [This will create a new Workspace called test]

-- terraform plan  [you should see t2.small getting launched]

-- terraform workspace new prod  [This will create a new Workspace called prod]

-- terraform plan  [you should see t2.medium getting launched]

-- terraform workspace select dev

-- terraform apply --auto-approve

-- terraform workspace select test

-- terraform apply --auto-approve

-- cd terraform.tfstate.d

-- ls

-- come back to main directory

-- terraform workspace list

-- terraform workspace delete test   [Workspace "test" is your active workspace, You cannot delete the currently active workspace. Please switch
                                      to another workspace and try again.]
-- terraform workspace select dev

-- terraform workspace delete test   [Error: Workspace is not empty, first delete the resources in workspace and then delete workspace]

-- terraform workspace select test   [Again go back to test workspace, destory the infra]

-- terraform destroy --auto-approve

-- terraform workspace dev  [Switch to another workspace to delete the test workspace]

-- terraform workspace delete test

-- terraform workspace select dev

-- terraform destroy --auto-approve

-- terraform workspace select default

-- terraform workspace delete dev

-- terraform workspace list

Another WorkSpace Example
==========================

create dev, test and prod workspaces and terraform apply and destroy the workspace, switch to another workspace and delete the workspaces

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

locals {
  env = terraform.workspace
}

resource "aws_vpc" "one" {
  cidr_block = "192.168.0.0/16&quot;
  tags = {
    Name = "${local.env}-vpc"
  }
}

resource "aws_subnet" "two" {
  vpc_id            = aws_vpc.one.id
  cidr_block        = "192.168.1.0/24&quot;
  availability_zone = "ap-south-1a"
  tags = {
    Name = "${local.env}-subnet"
  }
}

resource "aws_instance" "three" {
  subnet_id     = aws_subnet.two.id
  ami           = "ami-0cb441cf7bb9cba22"
  instance_type = "t2.micro"
  tags = {
    Name = "${local.env}-server"
  }
}


Note: We cannot delete the default workspace
=================================================
Another Variable concept: Dynamic Local Variable
================================================

-- terraform workspace list   [keep default]


Dynamically assigns the EC2 instance type based on the workspace

vi main.tf

resource "aws_instance" "dynamiclocalvarinstance" {

  ami           = "ami-08ee1453725d19cdb"
  instance_type = terraform.workspace == "prod" ? "m4.large" : "t2.small"
  tags = {
    Name = "Dynamic-local-var-example-server-${terraform.workspace}"
  }
}

output "active_workspace" {
  description = "Current Terraform workspace"
  value       = terraform.workspace
}    

output "selected_instance_type" {
  description = "Instance type selected for the current workspace"
  value       = aws_instance.dynamiclocalvarinstance.instance_type
}

-- terraform apply --auto-approve  

[Explanation: The above code has a variable terraform.workspace which is taking the value dynamicly from which workspace we are using, in this         case we are using default workspace, that default workspace will be passed to instace_type = default, if prod m4.large will launch, if not t2.small]

-- terraform destroy --auto-approve

=======================
TERRAFORM BACKEND SETUP:
=======================

Remote State:
--------------

“By default, Terraform stores state locally in a file named terraform.tfstate. When working with Terraform in a team, use of a local file makes Terraform usage complicated because each user must make sure they always have the latest state data before running Terraform and make sure that nobody else runs Terraform at the same time.”

“With remote state, Terraform writes the state data to a remote data store(S3), which can then be shared between all members of a team.”

Generally we have the statefile in local, if you lost the machine, statefile is also lost. So that reason we keep statefile in S3
And also, all DevOps Engineers can share the statefile if required.


First, Create a S3 Private bucket with Versioning

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

terraform {
  backend "s3" {
    bucket = "terraform-statefile-reyaz"
    key    = "prod/terraform.tfstate"
    region = "ap-south-1"
  }
}
resource "aws_instance" "myfirstinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "backend-example1"
  }
}

resource "aws_instance" "mysecondinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "backend-example2"
  }
}

output "instance_ids" {
  description = "List of EC2 instance IDs"
  value       = [aws_instance.myfirstinstance.id, aws_instance.mysecondinstance.id]
}

output "instance_names" {
  description = "List of EC2 instance names"
  value       = [aws_instance.myfirstinstance.tags.Name, aws_instance.mysecondinstance.tags.Name]
}


-- terraform init

-- terraform apply --auto-approve

Note: Now see the terraform.tfstate in S3 bucket

General Note: If you modify myfirstinstance or mysecondinstance, it will destroy the existing server and create a new one , but if you want to modify tag, just change the tag to something else and terraform apply, server will not destroy only tag will change.

-- terraform state list

-- terraform destroy --auto-approve -target="aws_instance.myfirstinstance"    [This will destroy only one instance and update in S3 statefile]

-- cat terraform.tfstate [Currently no resources are there]

-- cat terraform.tfstate.backup [This is the backup of the previous state]


======================================
Bring Back State File from S3 to Local
======================================

If you don't want to use S3 and want statefile to be in local again , modify main.tf and remove the backend code

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myfirstinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "backend-example1"
  }
}

resource "aws_instance" "mysecondinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "backend-example2"
  }
}


-- terraform init -migrate-state

-- terraform init -reconfigure   [ To bring statefile to local instead of S3, if you want to manage it local]

-- terraform apply --auto-approve

-- terraform state list   [Now you have the statefile in local]

==================================================================
Untrack a Terraform Resource from Statefile and StateFile Commands
==================================================================

-- terraform state list

-- terraform state rm aws_instance.mysecondinstance     [It show the details of mysecondinstance only in statefile]

-- terraform state rm aws_instance.myfirstinstance

-- terraform state list

-- cat terraform.tfstate   [it shows null, but still our EC2 instance is alive in AWS Console]

-- terraform destroy --auto-approve   [Nothing to destroy]

-- If you want to import it back

terraform import aws_instance.mysecondinstance i-0b1c2d3e4f5g67891


Extras
======

-- terraform state mv aws_instance.mysecondinstance aws_instance.mythirdinstance    

   [It moves the block name to mythird instance, but no change in infra]

-- terraform state pull  [If you have a statefile in remote and want to pull local] - No Practical

=====================================================
Securing statefile in backend with State Lock option
=====================================================


State Lock:
-----------

State locking is a mechanism that prevents multiple Terraform processes from simultaneously attempting to modify the same state file. Without state locking, concurrent Terraform operations could corrupt the state file, leading to unpredictable behavior and infrastructure issues.

“State locking happens automatically on all operations that could write state. You won’t see any message that it is happening. If state locking fails, Terraform will not continue. You can disable state locking for most commands with the -lock flag but it is not recommended.”



First in S3, bucket delete the objects if any but not bucket. Because we can use the same bucket and backend

For state locking, we use DynamoDB to store the statelocking mechanism , for that lets create a dynamodb table first

Tablename: dynamodb-terraform-state-lock
Column: LockID

Note: you can create a dynamodb table using TF code also

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

terraform {
  backend "s3" {

    bucket         = "terraform-statefile-reyaz"
    key            = "prod/terraform.tfstate"
    encrypt        = true
    dynamodb_table = "dynamodb-terraform-state-lock"
    region         = "ap-south-1"
  }
}
resource "aws_instance" "myfirstinstance" {
  ami           = "ami-0492447090ced6eb5"
  instance_type = "t2.micro"
  tags = {
    Name = "backend-example1"
  }
}

-- terraform init

-- terraform plan

-- terraform apply --auto-approve

[See the statefile in S3 and see the table in dynamoDB]

===========================================
Mini Project - With actual real time File Structure
==========================================

├── backend.tf   -- contains S3 statefile with statelock in dynamodb
├── main.tf      -- actual code
├── output.tf    -- instance details as output
├── terraform.tfvars  -- variables
└── variables.tf   -- calling variables from tfvars


vi backend.tf

terraform {
  backend "s3" {

    bucket         = "terraform-statefile-reyaz"
    key            = "prod/terraform.tfstate"
    encrypt        = true
    dynamodb_table = "dynamodb-terraform-state-lock"
    region         = "ap-south-1"
  }
}

--------------------------------

vi terraform.tfvars

ami           = "ami-08ee1453725d19cdb"
instance_type = "t2.micro"
key_name      = "MyKey"


----------------------------------

vi variables.tf

variable "ami" {
  description = "passing ami value"
  type        = string
  default     = ""

}
variable "instance_type" {
  type    = string
  default = ""

}
variable "key_name" {
  type    = string
  default = ""

}

--------------------------------------

vi output.tf

output "instance_public_ip" {
  value     = aws_instance.myinstance.public_ip
  sensitive = true
}

output "instance_id" {
  value = aws_instance.myinstance.id
}
output "instance_public_dns" {
  value = aws_instance.myinstance.public_dns

}
output "instance_arn" {
  value = aws_instance.myinstance.arn

}

----------------------------------------

vi main.tf

resource "aws_instance" "myinstance" {
  ami           = var.ami
  instance_type = var.instance_type
  key_name      = var.key_name
  tags = {
    Name = "my-ec2"
  }
}


---------------------------------------

-- terraform init

-- terraform plan

-- terraform apply --auto-approve

Note: see the resources, S3 and DynamoDB

-- terraform destroy --auto-approve


Multi-Region Resource Creation -
==============================

provider "aws" {
  alias = "ap-south-1"
  region = "ap-south-1"
}

provider "aws" {
  alias = "eu-west-1"
  region = "eu-west-1"
}

resource "aws_instance" "example" {
  ami = "ami-0123456789abcdef0"
  instance_type = "t2.micro"
  provider = "aws.ap-south-1"
}

resource "aws_instance" "example2" {
  ami = "ami-0123456789abcdef0"
  instance_type = "t2.micro"
  provider = "aws.eu-west-1"
}


PROJECT
=======

VPC and 2 webservers with userdata.

yum install -y git

git clone https://github.com/ReyazShaik/terraform.git

terraform init
terraform plan
terraform apply --auto-approve
terraform destroy --auto-approve
===============
Meta arguments
================

In Terraform, meta-arguments are special arguments that you can use with resources, modules, and other blocks to control how they behave.

The most commonly used meta-arguments in Terraform include

-- count     : The "count" meta-argument allows you to specify the number of instances of a resource or module to create.

-- for_each    : The "for_each" meta-argument allows you to create multiple instances of a resource or module based on the elements of a set                      
                  It provides more control and flexibility than "count"

-- depends_on   : The depends_on meta-argument explicitly defines dependencies between resources. This ensures that one resource is
                   created or updated only after another resource has been successfully created or updated.

-- provider     : The provider meta-argument allows you to specify which provider configuration to use for a particular resource or
                  module. This is useful when you have multiple configurations for the same provider, such as when managing resources
                  in multiple regions.

-- lifecycle    : The lifecycle meta-argument allows you to control the lifecycle of a resource. It provides options to prevent the
                  destruction of resources, create resources before destroying existing ones, or ignore changes to specific attributes.
               
                  -- create_before_destroy
                  -- prevent_destroy
                  -- ignore_changes

-- ignore_changes : This meta-argument is used within the lifecycle block to instruct Terraform to ignore changes to specific
                    attributes of a resource. This is particularly useful when an attribute is managed outside of Terraform, or if you
                    want to prevent Terraform from trying to update a resource when certain attributes change.




DEPENDS-ON
===========

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
tags = {
    Name = "DependsOn-Example"
  }
}

resource "aws_eip" "myinstance_eip" {
  instance   = aws_instance.myinstance.id
  depends_on = [aws_instance.myinstance]
}

output "elastic_ip" {
  description = "Elastic IP of the instance"
  value       = aws_eip.myinstance_eip.public_ip
}

output "instance_id" {
  description = "EC2 instance ID"
  value       = aws_instance.myinstance.id
}

-- terraform apply --auto-approve

-- terraform destroy --auto-approve





COUNT
====

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  count         = 2
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  tags = {
    Name = "WebServer-${count.index}"
  }
}

output "instance_ids" {
  description = "List of EC2 instance IDs"
  value       = aws_instance.myinstance[*].id
}
output "instance_names" {
  description = "List of EC2 instance names"
  value       = aws_instance.myinstance[*].tags.Name
}

-- terraform init

-- terraform apply --auto-approve

-- terraform destroy --auto-approve

Above code will launch EC2 instance with identical objects that meaning  with same nam WebServer-0, WebServer-1 etc
But if you need different names?

Bad Example - The below code has 3 resource blocks of aws_instance, which will create 3 different servers with different names and
              instance types But this code will be very long. instead use Length and Count concept

vi main.tf

provider "aws" {
region = "ap-south-1"
}

resource "aws_instance" "one" {
count = "3"
ami = "ami-08ee1453725d19cdb"
instance_type = "t2.micro"
tags = {
Name = "test-server"
}
}
resource "aws_instance" "two" {
count = "3"
ami = "ami-08ee1453725d19cdb"
instance_type = "t2.small"
tags = {
Name = "dev-server"
}
}

resource "aws_instance" "three" {
count = "3"
ami = "ami-08ee1453725d19cdb"
instance_type = "t2.medium"
tags = {
Name = "prod-server"
}
}

Good Example: With Length and Count
-----------------------------------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

variable "instance_type" {

  default = ["t2.micro", "t2.small", "t2.medium"]
}

variable "instance_name" {

  default = ["dev-server", "test-server", "prod-server"]
}

resource "aws_instance" "myinstance" {
  count         = length(var.instance_type)
  ami           = "ami-08ee1453725d19cdb"
  instance_type = var.instance_type[count.index]
  tags = {
    Name = var.instance_name[count.index]
  }
}


output "instance_ids" {
  description = "List of EC2 instance IDs"
  value       = aws_instance.myinstance[*].id
}
output "instance_names" {
  description = "List of EC2 instance names"
  value       = aws_instance.myinstance[*].tags.Name
}


-- terraform apply --auto-approve

-- terraform destroy --auto-approve

========
FOR_EACH
========

count vs for_each : count will create identical resources, for_each will create different resources

same above example can be achieved using for_each

Example 1:
----------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  for_each      = toset(["dev-server", "test-server", "prod-server"])
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  tags = {
    Name = "${each.key}"
  }
}

output "instance_ids" {
  description = "List of EC2 instance IDs"
  value       = { for k, v in aws_instance.myinstance : k => v.id }
}

output "instance_names" {
  description = "Instance names"
  value       = { for k, v in aws_instance.myinstance : k => v.tags.Name }
}


toset() is a function to create multiple EC2 instances from a list of names:

Terraform will generate a map of instances with keys as "dev-server", "test-server", and "prod-server".

The for expression iterates over aws_instance.myinstance
k represents the instance key (dev-server, test-server, etc.)
v.id retrieves the instance ID
The result is a map of key => instance_id

In for_each , we can play with key and value like .key and .value


-- terraform plan

-- terraform apply --auto-approve

-- terraform destroy --auto-approve

Another Optional Example:
------------------------

Note: The below code will launch 4 EC2 instances with different instance type and names

vi main.tf

variable "instances" {
  type = map(string)
  default = {
    "web-server-1" = "t2.micro"
    "web-server-2" = "t2.small"
    "app-server-1" = "t3.medium"
    "db-server-1"  = "m5.large"
  }
}

resource "aws_instance" "myinstance" {
  for_each = var.instances

  ami           = "ami-08ee1453725d19cdb"
  instance_type = each.value

  tags = {
    Name = each.key
  }
}

output "instance_ids" {
  description = "List of EC2 instance IDs"
  value       = { for k, v in aws_instance.myinstance : k => v.id }
}

output "instance_names" {
  description = "Instance names"
  value       = { for k, v in aws_instance.myinstance : k => v.tags.Name }
}

output "instance_names" {
  description = "List of instances with their Name key"
  value       = { for k, v in aws_instance.myinstance : k => { "Name" = v.tags.Name } }
}



k represents the instance key (dev-server, test-server, etc.)
v.id retrieves the instance ID
k => v.tags.Name retrives Instance Name, k => v.tags.Name (key and value both are same (app-server-1="app-server-1") like below output)
"Name" = v.tags.Name --> Print Key as Name, Value as instance name (+ Name = "app-server-1")


The last 2 outputs are like this: output "instance_names" {
------------------------------------------------

instance_names           = {
      + app-server-1 = "app-server-1"
      + db-server-1  = "db-server-1"
      + web-server-1 = "web-server-1"
      + web-server-2 = "web-server-2"
    }

If you need Output as Name: Value: output "instance_names_with_Name" {
---------------------------------------------------------------

instance_names_with_Name = {
      + app-server-1 = {
          + Name = "app-server-1"
        }
      + db-server-1  = {
          + Name = "db-server-1"
        }
      + web-server-1 = {
          + Name = "web-server-1"
        }
      + web-server-2 = {
          + Name = "web-server-2"
        }
    }


-- terraform plan

Another Optional Example:
-------------------------

vi main.tf

variable "instances" {
  type = map(object({
    instance_type = string
    availability_zone = string
  }))
  default = {
    "web-server-1" = { instance_type = "t2.micro", availability_zone = "ap-south-1a" }
    "web-server-2" = { instance_type = "t2.small", availability_zone = "ap-south-1b" }
    "app-server-1" = { instance_type = "t3.medium", availability_zone = "ap-south-1c" }
    "db-server-1"  = { instance_type = "m5.large", availability_zone = "ap-south-1a" }
  }
}

resource "aws_instance" "servers" {
  for_each = var.instances

  ami               = "ami-08ee1453725d19cdb"
  instance_type     = each.value.instance_type
  availability_zone = each.value.availability_zone

  tags = {
    Name = each.key
  }
}

-- terraform plan


IGNORE CHANGES
==============

If anyone modified the resources in AWS console which is created by TF, It will ignore that changes, it will not bring back to desired state. Actual State is AWS Console, Desired State is Statefile

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "myinstance" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  tags = {
    Name = "reyaz-server"
  }
  lifecycle {
    ignore_changes = all
  }
}

output "instance_id" {
  description = "EC2 instance ID"
  value       = aws_instance.myinstance.id
}
output "instance_name" {
  description = "Instance Name"
  value       = aws_instance.myinstance.tags.Name
}


-- terraform apply --auto-approve

-- cat terraform.tfstate | grep reyaz  [This show reyaz-server]

Now, modify instance_type or tags or ami in configuration block , TF will ignore the changes while apply
example: modify instance_type = "t2.nano" and reyaz-server to hello-server

-- terraform plan  [it shows no changes]

-- terraform apply --auto-approve   [no change here as we ignored changes]

-- terraform destroy --auto-approve

Example 2:
----------


vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "web_server" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  tags = {
    Name = "test-server"
  }
  lifecycle {
    ignore_changes = [
      instance_type,
      tags
    ]
  }
}

-- terraform plan

-- terraform apply --auto-approve

As you have ignored instance_type and tags, if you do any changes in those block in configuration file, terraform will ignore will apply

-- terraform destroy --auto-approve

===============
Prevent_Destory  -- resources will not delete if you give destroy command
===============

vi main.tf

provider "aws" {
region = "ap-south-1"
}

resource "aws_instance" "one" {
ami = "ami-08ee1453725d19cdb"
instance_type = "t2.micro"
tags = {
Name = "reyaz-server"
}
lifecycle{
prevent_destroy = true
}
}

-- terraform apply --auto-approve

-- terraform destroy --auto-approve    [It will not destroy as prevent_destroy is true , make it false and then destroy]


=======================
Create_before_destory
======================

If you change the instance type or instance name , security groups etc , It will change immediately ,
instance will not delete but if you want to change the image id of the EC2 instance , instance will delete first and then create a new instance with new ami-id

vi main.tf

provider "aws" {
region = "ap-south-1"
}

resource "aws_instance" "one" {
ami = "ami-08ee1453725d19cdb"
instance_type = "t2.micro"
tags = {
Name = "reyaz-server"
}
lifecycle{
create_before_destroy = true
}
}

-- terraform apply --auto-approve    

Now change the image id to ami-022ce6f32988af5fa

-- terraform apply --auto-approve    [First instance will be created and then old instance will be deleted]

Note: if you remove the code in main.tf and terraform apply , resources will be deleted

Give a try
----------
vi  main.tf

keep only provider and remove all code and give destroy

-- terraform destory --auto-approve
=====================================================
PROVIDERS
======================================================

Terraform has 3 main providers

1. Official : maintained by Terraform (AWS, Azure, GCP)
2. Partner : Maintained by terraform and organizations (Oracle, Alibaba)
3. Community : Maintained by individual


Lets use another provider, we were using AWS , lets now use GitHub


Example 1:
---------

Create a token first in GitHub --> Settings --> Developer Settings --> Personal access tokens (classic) --> Generate new token(classic)

vi main.tf

provider "github" {
  token = "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"
}

resource "github_repository" "example" {
  name        = "tf-github-repo"
  description = "created repo from tf"

  visibility = "public"

}


-- terraform init

-- terraform apply --auto-approve

Note: to destroy, in GitHub token select delete-repo option also

-- terraform destroy --auto-approve

==============
Local Provider
=============

vi main.tf

provider "local" {
}

resource "local_file" "one" {
  filename        = "test.txt"
  content = "this is from test data from tf using local provider"
}

-- terraform apply --auto-approve
-- ls
it will create a new file locally

-- terraform destroy --auto-approve


see in chatgpt to create docker and k8s if required

=======================
Terraform with Docker
======================

yum install docker -y

systemctl start docker

vi main.tf

terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 2.0"
    }
  }
}
provider "docker" {
  host = "unix:///var/run/docker.sock"  # For Linux/macOS
}

resource "docker_image" "nginx" {
  name         = "nginx:latest"
  keep_locally = false  # Removes the image when the container is deleted
}

resource "docker_container" "nginx" {
  name  = "nginx-container"
  image = docker_image.nginx.image_id

  ports {
    internal = 80  # Inside the container
    external = 8080  # Exposed on the host machine
  }
}

output "container_name" {
  value = docker_container.nginx.name
}

output "container_id" {
  value = docker_container.nginx.id
}

output "nginx_url" {
  value = "http://13.201.46.206:8080"
}


-- terraform init -upgrade

-- terraform plan

-- terraform apply --auto-approve

http://13.201.46.206:8080

================================================================

setup cluster first - minikube or KOPS

vi main.tf

terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }
  }
}

provider "kubernetes" {
  config_path = "~/.kube/config"  # Adjust if using a different kubeconfig
}
resource "kubernetes_deployment" "nginx" {
  metadata {
    name = "nginx-deployment"
    labels = {
      app = "nginx"
    }
  }

  spec {
    replicas = 2

    selector {
      match_labels = {
        app = "nginx"
      }
    }

    template {
      metadata {
        labels = {
          app = "nginx"
        }
      }

      spec {
        container {
          image = "nginx:latest"
          name  = "nginx"

          port {
            container_port = 80
          }
        }
      }
    }
  }
}

resource "kubernetes_service" "nginx" {
  metadata {
    name = "nginx-service"
  }

  spec {
    selector = {
      app = "nginx"
    }

    port {
      protocol    = "TCP"
      port        = 80
      target_port = 80
    }

    type = "LoadBalancer"
  }
}

================================

==============
Version Constraints
==============

Terraform version constraints allow you to specify which versions of Terraform or provider plugins your configuration is compatible with. This ensures that your infrastructure code is used with the correct version of Terraform or providers, avoiding unexpected behavior due to version mismatches.

We can change the versions of hashicorp provider plugins

For Example:

Whenever we have new changes on AWS console the old code might not work so if you want to work with the new console or new features in AWS, we need to download the new provider plugins

https://registry.terraform.io/browse/providers  -- select AWS and show versions

when we do terraform init, It will download always latest, if you are using old version just use below code to update version

all version plugins will be under .terraform --> go inside

depend upon version, code will also change

vi main.tf


provider "aws" {

}

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "<5.56.1"   -- upgrade or downgrade, use > < >= <= anything is fine based on requriement

    }
  }
}

-- terraform init  [see it is downloading 5.56.1]

Now modify to latest <5.64.0

-- terraform init [it uses the old version only, if you want to upgrade use below command]

-- terraform init -upgrade   [see now latest plugins are downloaded]


========================
Upgrade local provider - check in chatgpt
========================

No Practise

vi main.tf

terraform {
  required_providers {
    myprovider = {
      source  = "local/myprovider"
      version = ">= 1.2.0"
    }
  }
}


Here, you're specifying that any version greater than or equal to 1.2.0 can be used.

-- terraform init -upgrade


=========================
TERRAFORM REFRESH - DANGER - Dont do this in Real time
==========================


In Terraform, the terraform refresh command is used to update the Terraform state file to reflect the current state of the real-world infrastructure.

This command queries the actual state of the resources defined in your Terraform configuration and compares it to the state recorded in the .tfstate file. If any discrepancies are found, the state file is updated to match the actual infrastructure.

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "one" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  tags = {
    Name = "reyaz-server"
  }
}

-- terraform apply --auto-approve

-- cat terraform.tfstate   [This show reyaz-server]

Now change the value in AWS Console instance name to student-server

-- terraform apply --auto-approve  [TF will check the statefile first, in statefile it has reyaz-server, so TF will update AWS console
                                    to reyaz-server as there is difference between desired(tf) and actual state(AWS)]

See in AWS Console - changed to reyaz-server

Now change the value in AWS Console instance name to student-server

-- cat terraform.tfstate [This show reyaz-server]

-- terraform refresh

-- cat terraform.tfstate [This show student-server]

-- terraform destroy --auto-approve

===================
Terraform IMPORT  - track in TF which was not created by TF
===================

The terraform import command is used to bring existing infrastructure resources under Terraform management.

This is particularly useful when you have resources that were created manually or by another tool, and you now want to manage them using Terraform without recreating them.

Example 1:
---------

-- cat terraform.tfvars

Step 1: First launch a Linux or any EC2 instance manually on AWS Console

vi main.tf


provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "example" {
  ami           = "ami-08ee1453725d19cdb"  -- give the same ami of the EC2 to import
  instance_type = "t2.micro"
}

-- terraform import aws_instance.example i-0ab2056f11dfa5a6e

-- cat terraform.tfstate

-- terraform destroy --auto-approve


Example 2:
----------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

resource "aws_s3_bucket" "example" {
  bucket = "lala-ola-lelaa"
}

-- cat terraform.tfstate

-- terraform import aws_s3_bucket.example lala-ola-lelaa

-- cat terraform.tfstate

Example 3: - Multi resources import
-----------------------------------

-- terraform destroy --auto-approve

Again re-create resources in AWS Manually

vi main.tf


provider "aws" {
  region = "ap-south-1"
}

resource "aws_instance" "example" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
}
resource "aws_s3_bucket" "example" {
  bucket = "lala-ola-lelaa"
}


-- terraform import aws_instance.example i-0ab2056f11dfa5a6e

-- terraform import aws_s3_bucket.example lala-ola-lelaa

-- terraform destroy --auto-approve



Terraform does not support importing multiple resources in a single command or operation.

Each resource must be imported individually using the terraform import command. However, you can streamline the process by scripting the import commands, especially if you have a large number of resources to import

Terraform's import command works one resource at a time, so multiple resources need to be imported individually.

For Multiple resources import use another tool called TERRAFORMER

TERRAFORMER
-----------

vi terraformer.sh

sudo yum update -y

sudo yum install -y wget unzip

wget https://github.com/GoogleCloudPlatform/terraformer/releases/download/0.8.24/terraformer-aws-linux-amd64 -O terraformer

chmod +x terraformer
sudo mv terraformer /usr/local/bin/

echo 'export PATH=$PATH:/usr/local/bin' >> ~/.bashrc
source ~/.bashrc

terraformer --version

run the below command
----------------------
source ~/.bashrc


launch 3 ec2 instances manually

terraformer import aws --resources=ec2_instance --regions=ap-south-1

can import entire vpc, subnet also

terraformer import aws --resources=instance,vpc,subnet --regions=ap-south-1



TERRAFORM MODULES:
===================


Terraform modules are a fundamental feature that helps in organizing and reusing Terraform configurations.

A module is a container for multiple resources that are used together.

Modules allow you to encapsulate and manage resources as a single unit, making your Terraform configurations more modular, readable, and maintainable.

Root Module:
-----------

    -- The root module is the main configuration where Terraform starts its execution.
    -- It is usually defined in the main configuration directory where terraform init and terraform apply are run.
    -- The root module can call other modules, referred to as child modules.

Child Modules:
--------------

    -- Child modules are modules that are called from within other modules (including the root module).
    -- They help in organizing resources and reusing configurations.
    -- Each child module can be stored in a separate directory and can be called using a module block in the root module or another parent module.

PROJECT:
--------

Create modules directory with instance, bucket, vpc.

├── main.tf
├── modules
│   ├── ec2-instances
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── s3-bucket
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   └── vpc
│       ├── main.tf
│       ├── outputs.tf
│       └── variables.tf
└── terraform.tfstate


-- yum install -y tree

-- mkdir -p modules/ec2-instances/

-- mkdir -p modules/s3-bucket/

-- mkdir -p modules/vpc/




-- cd modules/ec2-instances/
 
vi main.tf
vi outputs.tf
vi variables.tf

-- cd modules/s3-bucket/
 
vi main.tf
vi outputs.tf
vi variables.tf

-- cd modules/vpc/
 
vi main.tf
vi outputs.tf
vi variables.tf


Root Module main.tf

vi main.tf

git clone https://github.com/ReyazShaik/terraform.git

cd modulesproject


-- tree

-- terraform init

-- terraform apply --auto-approve

-- terraform destroy --auto-approve
Install TF
-- mkdir terraform
-- cd terraform

=====================================================
Dynamic Block, Terraform Provisioners, HCP
==========================================


Provisioners
-----------
   -- Terraform provisioners are used to perform actions on a local or remote machine after a resource is created or updated.

   -- They are typically used for tasks such as configuring or installing software on a machine, which Terraform itself does not handle directly.

Types of Provisioners
---------------------

Terraform supports several types of provisioners:

local-exec:
**********

Executes a command locally on the machine where Terraform is run.
Useful for running scripts or commands that need to be executed locally.

remote-exec:
***********

Executes commands on a remote resource, such as an EC2 instance, after it has been created.
Useful for configuring instances or applying configurations remotely.

file:
****

Uploads files from the local machine to a remote resource.
Useful for transferring configuration files or scripts to a remote machine.

Examples:
--------

local-exec:  The "local-exec" provisioner runs commands on the local machine where Terraform is executed.
**********

In this example, the local-exec provisioner writes the instance ID to a file instance_id.txt on the local machine after the EC2 instance is created.

vi main.tf

resource "aws_instance" "example" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"

  provisioner "local-exec" {
    command = "echo 'Instance ID: ${self.id}' > instance_id.txt"
  }
}

output "instance_id" {
  value = aws_instance.example.id
}


self.id will be available after the instance is created
The provisioner runs on the local machine, saving the instance ID to instance_id.txt.


-- terraform apply --auto-approve

-- ls  [you can see the txt got created locally]

-- terraform destroy  --auto-approve


Remote-Exec - The remote-exec provisioner runs commands on a remote resource. It typically requires a connection configuration.
**********

Example:
-------

From TF machine , we will connect remotely to another machine, for this we need to have pem file under ~/.ssh/id_rsa


-- vi ~/.ssh/id_rsa
copy paste pem file data


vi main.tf

resource "aws_instance" "example" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  key_name      = "MyKey"
  tags = {
    Name = "ec2-instance"
  }
  provisioner "remote-exec" {
    inline = [
      "sudo yum update -y",
      "sudo yum install -y httpd",
      "sudo systemctl start httpd"
    ]

    connection {
      type        = "ssh"
      user        = "ec2-user"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }
}

-- terraform apply --auto-approve

-- terraform destroy  --auto-approve


File Provisioner - The file provisioner uploads files from the local machine to the remote resource.
***************

-- cd /tmp

-- vi remote_script.sh

#!/bin/bash

# Example commands to run on the remote instance
echo "Running remote script"

# Update the system
sudo yum update -y

# Install Apache HTTP Server
sudo yum install -y httpd

# Start and enable the Apache service
sudo systemctl start httpd
sudo systemctl enable httpd

# Write content to /var/www/html/index.html with sudo
echo "<html><h1>Welcome to Naresh IT ! AWS Infra created using Terraform in Mumbai Region!</h1></html>" | sudo tee /var/www/html/index.html > /dev/null


-------------------------------------------------------

vi main.tf

resource "aws_instance" "apache" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  key_name      = "MyKey"
  tags = {
    Name = "ec2-apache"
  }

  provisioner "file" {
    source      = "remote_script.sh"
    destination = "/tmp/remote_script.sh"

    connection {
      type        = "ssh"
      user        = "ec2-user"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }

provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/remote_script.sh",
      "/tmp/remote_script.sh"
    ]

    connection {
      type        = "ssh"
      user        = "ec2-user"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }
}

output "instance_ip" {
  value = aws_instance.apache.public_ip
}

-- terraform apply --auto-approve

-- terraform destroy  --auto-approve


http://IP


Example 2:
-----------

vi userdata.sh

#!/bin/bash
sudo yum update -y
sudo yum install -y httpd
sudo service httpd start  
sudo systemctl enable httpd
echo "<h1>Welcome to Reyaz DevOps ! AWS Infra created using Terraform in Mumbai Region</h1>" > /var/www/html/index.html


vi main.tf

provider "aws" {
region = "ap-south-1"  
}
resource "aws_instance" "test" {
    ami = "ami-08ee1453725d19cdb"
    instance_type = "t2.micro"
    user_data= file("userdata.sh")
}

output "instance_ip" {
  value = aws_instance.test.public_ip
}


-- terraform apply --auto-approve

-- terraform destroy  --auto-approve

=================
Dynamic Block  - it is used to reduce the length of the block
=================

Example: Launch a new EC2 instance with security group allowed protocols 22 and 80, in this example we should create multiple ingress rules for multiple protocols

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

# Security Group
resource "aws_security_group" "example_sg" {
  name        = "example-sg"
  description = "Security group for example EC2 instance"

  # Inbound rules
  ingress {
    description = "Allow SSH"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    description = "Allow HTTP"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Outbound rules
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "example-sg"
  }
}

# EC2 Instance
resource "aws_instance" "example" {
  ami           = "ami-08ee1453725d19cdb"  # Replace with your desired AMI ID
  instance_type = "t2.micro"
  key_name      = "MyKey"          # Replace with your SSH key name

  # Associate the security group with the EC2 instance
  vpc_security_group_ids = [aws_security_group.example_sg.id]

  tags = {
    Name = "ExampleInstance"
  }
}

# Output the public IP of the instance
output "instance_ip" {
  value = aws_instance.example.public_ip
}



-- terraform apply --auto-approve

-- terraform destroy  --auto-approve

---------------------------------------------------------

But instead writing multiple ingress rules, we can use dynamic block

Example 2:
-----------

vi main.tf


provider "aws" {
  region = "ap-south-1"
}

locals {
  ingress_rules = [{
    port        = 443
    description = "Ingress rules for port 443"
    },
    {
      port        = 80
      description = "Ingress rules for port 80"
    },
    {
      port        = 8080
      description = "Ingress rules for port 8080"

  }]
}

resource "aws_instance" "ec2_example" {
  ami                    = "ami-08ee1453725d19cdb"
  instance_type          = "t2.micro"
  vpc_security_group_ids = [aws_security_group.main.id]
  tags = {
    Name = "Terraform EC2"
  }
}

resource "aws_security_group" "main" {

  egress = [
    {
      cidr_blocks      = ["0.0.0.0/0"]
      description      = "*"
      from_port        = 0
      ipv6_cidr_blocks = []
      prefix_list_ids  = []
      protocol         = "-1"
      security_groups  = []
      self             = false
      to_port          = 0
  }]

  dynamic "ingress" {
    for_each = local.ingress_rules

    content {
      description = "*"
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  tags = {
    Name = "terra sg"
  }
}


-- terraform apply --auto-approve

-- terraform destroy  --auto-approve


===============
Terraform MAP - Giving tags for resources, key and value
================


It  is a variable type used to pass key and value pair for resource.


vi main.tf

provider "aws" {
region = "ap-south-1"
}

resource "aws_instance" "three" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  availability_zone = "ap-south-1a"
  tags = var.ec2_tags
}

variable "ec2_tags" {
description = ""
type = map(any)
default = {
Env = "prod"
Client = "Infy"
Name = "Taggingserver"
}
}


-- terraform apply --auto-approve

-- terraform destroy  --auto-approve


=============================
DATA-Source
==========================

In Terraform, a data source allows you to fetch information from existing resources or services that are external to your Terraform configuration.

Fetching Information About Existing Resources:
       
If you need to retrieve information about an existing resource that wasn't created by your Terraform configuration (e.g., an existing AWS VPC or EC2 AMI).

Example 1:  Below example is used to fetch the latest ami and assign to the EC2 instance
----------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

# Data source to fetch the latest Amazon Linux 2 AMI
data "aws_ami" "amazon_linux" {
  most_recent = true

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  owners = ["137112412989"]  # Amazon's official AWS account ID for Amazon Linux
}

# EC2 instance using the fetched AMI
resource "aws_instance" "example" {
  ami           = data.aws_ami.amazon_linux.id
  instance_type = "t2.micro"
  key_name      = "MyKey"  # Replace with your SSH key name

  tags = {
    Name = "ExampleInstance"
  }
}

output "instance_ip" {
  value = aws_instance.example.public_ip
}


Example 2: To get the existing vpc and bucket details
---------------------------------------------------------

vi main.tf

provider "aws" {
  region = "ap-south-1"
}

data "aws_vpc" "default" {
  default = true
}

resource "aws_instance" "example" {
  ami           = "ami-08ee1453725d19cdb"
  instance_type = "t2.micro"
  subnet_id     = data.aws_vpc.default.id
}


data "aws_s3_bucket" "example_bucket" {
  bucket = "test-lala-ola-la"
}

output "bucket_arn" {
  value = data.aws_s3_bucket.example_bucket.arn
}


-- terraform apply --auto-approve

-- terraform destroy  --auto-approve



CONDITIONS
===========

vi main.tf

variable "aws_region" {
  description = "The region in which to create the infrastructure"
  type        = string
  nullable    = false
  default     = "change me" #here we need to define either us-west-1 or eu-west-2 if i give other region will get error
  validation {
    condition = var.aws_region == "ap-south-1" || var.aws_region == "eu-west-1"
    error_message = "The variable 'aws_region' must be one of the following regions: ap-south-1, eu-west-1"
  }
}

provider "aws" {
  region = "ap-south-1"
 
   
 }

 resource "aws_s3_bucket" "dev" {
    bucket = "statefile-configuresss"
   
 
}


=================================================================

If you accidently deleted main.tf and have only state file, we can recover the code

terraform show -no-color terraform.tfstate > recovered.tf


or use terraformer

====================================================================



TERRAFORM CLOUD: used to create resourec form gui.

1. create account
2. create organization
3. create workspace
4. add vsc -- > GitHub -- > username & password -- > select repo
5. start new plan
7. variables -- > add var -- > env vars -- >

-----------------------------------------
\How to Fix Drift?

✅ Reconcile via Terraform
Run terraform apply to bring infrastructure back in sync.

✅ Manually Adjust Terraform Code
If manual changes were intentional, update main.tf to match the current infrastructure.

✅ Use terraform import for External Changes
If new resources were manually created, import them into Terraform:

terraform import <resource_type>.<resource_name> <resource_id>
✅ Destroy and Recreate If necessary, destroy and recreate the resource:


terraform taint <resource_name>

terraform apply

Preventing Drift

🔹 Use Terraform Cloud or Remote State Locking
🔹 Enable Sentinel Policies to Block Manual Changes
🔹 Implement IaC Best Practices (Avoid Direct Manual Changes)

---------------------------------

Terraform Vault: no practise
===============

HashiCorp Vault is a tool designed to securely store and manage sensitive information such as secrets, passwords, certificates, and API keys.

Terraform can be integrated with Vault to dynamically retrieve and manage secrets as part of your infrastructure provisioning process.

sudo yum install -y yum-utils

sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo

sudo yum -y install vault

systemctl status vault

systemctl start vault

vault server -dev

vault secrets enable -path=secret kv

vault kv put secret/mysecret password="supersecretpassword"
=====================================================================================================================================================================================================================
